{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38e670e7-6fdc-4f86-888b-69ffbe02ca11",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "import backtrader as bt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pytz\n",
    "import time\n",
    "import os\n",
    "from xbbg import blp\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import interact, Dropdown, HBox, VBox, Button, Output, Text, widgets\n",
    "import sympy as sp\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "from IPython import get_ipython\n",
    "import matplotlib.dates as mdates\n",
    "from pydataquery import DataQuery\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "from scipy.optimize import minimize\n",
    "import scipy.stats as stats\n",
    "import itertools\n",
    "import warnings\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import yfinance as yf\n",
    "import csv\n",
    "import uuid\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import warnings\n",
    "from multiprocess import Pool\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f98a4-75c4-44ad-ba75-7e6b2b9162eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-30 20:20:35.554187+0000 UTC [INFO] pydataquery: DataQuery initialized with client_id='jbAIMF2Tkp0JO3sc'\n",
      "2025-09-30 20:20:35.555317+0000 UTC [INFO] pydataquery: Set preferred_timezone='UTC'. current_time='2025-09-30 20:20:35.555317+0000 UTC'\n",
      "2025-09-30 20:20:35.558435+0000 UTC [INFO] pydataquery: DataQuery default attribute `start_date` set to '2010-10-04'\n",
      "2025-09-30 20:20:35.558435+0000 UTC [WARNING] pydataquery: Potential long process time due to date range exceeding two years: 15.0 years.\n",
      "2025-09-30 20:20:35.558435+0000 UTC [INFO] pydataquery: DataQuery default attribute `start_date` set to '2010-10-04' (client_id='jbAIMF2Tkp0JO3sc')\n",
      "2025-09-30 20:20:35.558435+0000 UTC [INFO] pydataquery: Execution started\n",
      "2025-09-30 20:20:35.558435+0000 UTC [INFO] pydataquery: Prepared 32 API requests for 67 expressions of 14 year(s) 364 day(s): start_date=20101004, end_date=20250929\n",
      "2025-09-30 20:20:35.558435+0000 UTC [INFO] pydataquery: Sending 1/32 API request\n",
      "2025-09-30 20:20:35.558435+0000 UTC [INFO] pydataquery: Generating a new OAuth access token for client_id=jbAIMF2Tkp0JO3sc\n",
      "2025-09-30 20:20:35.890349+0000 UTC [INFO] pydataquery: Generated an OAuth access token for client_id=jbAIMF2Tkp0JO3sc in 0.33 seconds. The token expires_at=2025-09-30 22:20:34+0000 UTC\n"
     ]
    }
   ],
   "source": [
    "isin_list = [\"US105756BX78\", \"US105756BZ27\", \"US105756CA66\", \"US105756CC23\", \"US105756CG37\", \"US105756CE88\", \"US105756CK49\", \"US105756CF53\", \"US105756CH10\", \"US105756BW95\", \"US105756BY51\", \"US105756CB40\", \"US105756CJ75\", \"US168863DX33\", \"US168863CF36\", \"US168863EB04\", \"US168863DP09\", \"US168863DN50\", \"US168863DT21\", \"US168863DV76\", \"US168863DZ80\", \"US168863DS48\", \"US168863DY16\", \"US168863CE60\", \"US168863DL94\", \"US168863DW59\", \"XS2327851874\", \"US168863EA21\", \"US168863DQ81\", \"US168863DU93\", \"US195325CX13\", \"US195325DL65\", \"US195325DP79\", \"US195325DR36\", \"US195325DS19\", \"US195325DZ51\", \"US195325EF88\", \"US195325EG61\", \"US195325EL56\", \"US195325BK01\", \"US195325BM66\", \"US195325EA91\", \"US195325BR53\", \"US195325CU73\", \"US195325DQ52\", \"US195325DT91\", \"US195325EM30\", \"US195325DX04\", \"US91086QBG29\", \"US91087BAC46\", \"US91087BAE02\", \"US91087BAU44\", \"US91087BAF76\", \"US91087BAY65\", \"US91087BAH33\", \"US91087BAM28\", \"US91087BAK61\", \"US91087BAT70\", \"US91087BAR15\", \"US91087BAV27\", \"US91087BAZ31\", \"US91086QAV05\", \"US91087BAQ32\", \"US91086QBB32\", \"US91086QBE70\", \"US91086QBF46\", \"US91087BAB62\", \"US91087BAD29\", \"US91087BAG59\", \"US91087BAL45\", \"US91087BAS97\", \"US91087BAX82\", \"US91087BBA70\", \"US91087BAN01\", \"XS2280637039\", \"US91086QAZ19\", \"USP90603AN40\", \"US698299BF03\", \"US698299BK97\", \"US698299BX19\", \"USP90603AP97\", \"US698299BN37\", \"US698299BR41\", \"US698299BT07\", \"US698299AW45\", \"US698299BW36\", \"US698299BY91\", \"US698299BG85\", \"US698299BH68\", \"US698299BB98\", \"US698299BV52\", \"US698299BM53\", \"US698299BL70\", \"US698299BS24\", \"US715638DE95\", \"US715638BU55\", \"US715638DA73\", \"US715638DF60\", \"US715638DP43\", \"US715638DU38\", \"US715638EB48\", \"US715638DS81\", \"US715638BM30\", \"US715638DT64\", \"US715638EC21\", \"US715638DQ26\", \"US715638DW93\", \"US715638DR09\", \"US836205AT15\", \"US836205AW44\", \"US836205AU87\", \"US836205BA15\", \"US836205AY00\", \"US836205BC70\", \"US836205AS32\", \"US836205AV60\", \"US836205AX27\", \"US836205BB97\", \"US836205BE37\", \"US900123DB31\", \"US900123CJ75\", \"US900123CK49\", \"US900123CL22\", \"USM88269US88\", \"US900123DF45\", \"US900123CP36\", \"US900123CQ19\", \"US900123DH01\", \"US900123CT57\", \"US900123CY43\", \"US900123DJ66\", \"US900123DA57\", \"US900123DC14\", \"US900123DL13\", \"US900123DG28\", \"US900123DD96\", \"US900123AT75\", \"US900123DK30\", \"US900123DN78\", \"US900123AY60\", \"US900123BB58\", \"US900123BG46\", \"US900123BJ84\", \"US900123CB40\", \"US900123CG37\", \"US900123CM05\", \"USP3579EBK21\", \"USP3579EBV85\", \"USP3579ECB13\", \"USP3579ECP09\", \"USP3579ECF27\", \"USP3579ECR64\", \"USP3579ECH82\", \"USP3579ECN50\", \"USP3579ECU93\", \"USP3579ECJ49\", \"USP3579EAY34\", \"USP3579EBE60\", \"USP3579EBY25\", \"USP3579ECE51\", \"USP3579ECG00\", \"US71654QDH20\", \"US71654QBW15\", \"US71654QCB68\", \"US71654QDB59\", \"US71654QCG55\", \"US71654QCK67\", \"US71654QCP54\", \"US71654QDL32\", \"US71654QDC33\", \"US71654QDE98\", \"US71643VAB18\", \"US71654QDP46\", \"US706451BG56\", \"US71654QAZ54\", \"US71654QBR20\", \"US71654QCC42\", \"US71654QCL41\", \"US71654QDD16\", \"US71654QDF63\", \"US71656MAF68\", \"US71647NAS80\", \"US71647NAY58\", \"US71647NBH17\", \"US71647NBK46\", \"US71647NBL29\", \"US71645WAQ42\", \"US71645WAS08\", \"US71647NAK54\", \"US71647NBJ72\", \"US71647NAN93\",\n",
    "             \"XS2214237807\", \"XS2214239506\", \"XS2214238441\", \"XS2214239175\", \"US040114HX11\", \"US040114HS26\", \"US040114HT09\", \"US040114HU71\", \"US040114HV54\", \"US040114HW38\", \"XS1910826996\", \"XS1717011982\", \"XS2384698994\", \"XS2445169985\", \"XS1777972511\", \"XS1910827887\", \"XS2948511949\", \"XS1566179039\", \"XS2384701020\", \"XS2948512913\", \"XS1777972941\", \"XS1717013095\", \"XS1910828182\", \"XS2384704800\", \"XS1843435840\", \"XS1781710543\", \"XS2764839945\", \"XS1843435766\", \"XS2354781614\", \"XS1781710626\", \"XS1318576086\", \"XS1819680288\", \"XS2083302419\", \"XS2446175577\", \"XS1819680528\", \"XS2083302500\", \"XS1790104530\", \"XS1619155564\", \"XS2333676133\", \"XS1790134362\", \"XS2297220423\", \"XS1903488572\", \"XS1558078736\", \"XS2391394348\", \"XS1775618439\", \"XS1504948776\", \"XS1953057061\", \"XS1903489463\", \"XS2297226545\", \"XS2079842642\", \"XS2176897754\", \"XS2391395154\", \"XS1558078496\", \"XS1775617464\", \"XS1953057491\", \"XS2176899701\", \"XS2391398174\", \"USP01012BX31\", \"USP01012CF16\", \"XS0146173371\", \"USP01012AN67\", \"USP01012AR71\", \"USP01012CA29\", \"USP01012CC84\", \"USP01012CH71\", \n",
    "             \"USP7808BAA54\", \"USP7808BAB38\", \"XS0505478684\", \"XS2079846635\", \"XS2297221405\", \n",
    "             \"USG2583XAB76\", \"USL21779AL44\", \"USL21779AJ97\", \"USL21779AK60\", \n",
    "             \"XS2322319398\", \"XS1729875598\", \"XS2419405274\", \"XS2322319638\", \"XS2322321964\",\n",
    "             \"USN15516AB83\", \"USN15516AD40\", \"USN15516AH53\",\"USN15516AG70\",\"USN15516AJ10\",\"USU1065PAA94\",\n",
    "             \"USN15516AE23\"\n",
    "            ]\n",
    "isin_list += ['XS2965710598','US105756CL22','US105756CM05','US168863EE43','US195325ER27','US195325EQ44','US195325EP60','US195325ES00','US25714PFB94','US25714PFC77','XS2990500766','XS2989586941','XS3010561762','US91087BBB53','US91087BBF67','US91087BBE92','US91087BBC37','US91087BBD10','US715638FD94','US715638FC12','US836205BF02','XS2917537875','US900123DQ00']\n",
    "isin_list += [\"US60367QAC78\", \"US16955EAC49\", \"US455780CB07\", \"US455780BM70\", \"US455780BY19\", \"US455780BJ42\", \"US455780BU96\", \"US455780BW52\", \"US455780BR67\", \"US455780AX45\", \"US455780AZ92\", \"US455780AT33\", \"US455780CV60\", \"US455780CY00\", \"US455780CQ75\", \"US455780CR58\", \"US455780CW44\", \"US455780DG84\", \"US455780CX27\", \"US455780DJ24\", \"US455780CN45\", \"US455780CS32\", \"US455780DX18\", \"US455780CT15\", \"US455780DK96\", \"US455780CE46\", \"US455780CU87\", \"US455780DN36\", \"US455780DU78\", \"US455780DR40\", \"US455780DZ65\", \"US455780DV51\", \"US455780DW35\", \"US455780CJ33\", \"US455780EA06\", \"US455780DS23\"]\n",
    "isin_list += [\"XS2290957146\", \"US05675M2G83\", \"XS2408003064\", \"US05675M2J23\", \"XS2058948451\", \"US05675M2A14\", \"XS2226916216\", \"US05675M2D52\", \"XS1675862012\", \"US05674RAJ32\", \"XS2764424813\", \"US05675M2L78\", \"XS2611617700\", \"US05675M2K95\", \"XS2172965282\", \"US05675M2C79\", \"US50064FAS39\", \"XS2294322818\", \"US80413TAV17\", \"XS2109766472\", \"US80413TAQ22\", \"XS2159975700\", \"US80413TAT60\", \"XS1791939066\", \"US80413TAH23\", \"XS2747598444\", \"US80413TBF57\", \"XS2577135127\", \"US80413TBD00\", \"XS2747599095\", \"US80413TBG31\", \"XS2548892020\", \"US80413TBA60\", \"XS2974968161\", \"US80413TBK43\", \"XS2974969482\", \"US80413TBL26\", \"XS2288905370\", \"US68205LAS25\", \"XS2234859283\", \"US68205LAE39\", \"US718286CK14\", \"US718286CN52\", \"US718286CJ41\", \"US718286CR66\", \"US718286DC88\", \"US718286BW60\", \"US718286CW51\", \"US718286DG92\", \"US718286DA23\", \"US718286CT23\", \"US718286BD89\", \"US718286BG11\", \"US718286BB24\", \"US718286AY36\", \"XS2155352664\", \"US74727PBD24\", \"XS2822506833\", \"US74738PTP35\", \"XS2918565198\", \"US74738PUJ55\", \"XS0113419690\", \"US74727PAE16\"]\n",
    "\n",
    "names_list = [\"BRAZIL 6 04/07/26\", \"BRAZIL 4 5/8 01/13/28\", \"BRAZIL 4 1/2 05/30/29\", \"BRAZIL 3 7/8 06/12/30\", \"BRAZIL 6 1/4 03/18/31\", \"BRAZIL 3 3/4 09/12/31\", \"BRAZIL 6 1/8 01/22/32\", \"BRAZIL 6 10/20/33\", \"BRAZIL 6 1/8 03/15/34\", \"BRAZIL 5 01/27/45\", \"BRAZIL 5 5/8 02/21/47\", \"BRAZIL 4 3/4 01/14/50\", \"BRAZIL 7 1/8 05/13/54\", \"CHILE 2 3/4 01/31/27\", \"CHILE 3.24 02/06/28\", \"CHILE 4.85 01/22/29\", \"CHILE 2.45 01/31/31\", \"CHILE 2.55 01/27/32\", \"CHILE 2.55 07/27/33\", \"CHILE 3 1/2 01/31/34\", \"CHILE 4.95 01/05/36\", \"CHILE 3.1 05/07/41\", \"CHILE 4.34 03/07/42\", \"CHILE 3.86 06/21/47\", \"CHILE 3 1/2 01/25/50\", \"CHILE 4 01/31/52\", \"CHILE 3 1/2 04/15/53\", \"CHILE 5.33 01/05/54\", \"CHILE 3.1 01/22/61\", \"CHILE 3 1/4 09/21/2071\", \"COLOM 4 1/2 01/28/26\", \"COLOM 3 7/8 04/25/27\", \"COLOM 4 1/2 03/15/29\", \"COLOM 3 01/30/30\", \"COLOM 3 1/8 04/15/31\", \"COLOM 3 1/4 04/22/32\", \"COLOM 8 04/20/33\", \"COLOM 7 1/2 02/02/34\", \"COLOM 8 11/14/35\", \"COLOM 7 3/8 09/18/37\", \"COLOM 6 1/8 01/18/41\", \"COLOM 4 1/8 02/22/42\", \"COLOM 5 5/8 02/26/44\", \"COLOM 5 06/15/45\", \"COLOM 5.2 05/15/49\", \"COLOM 4 1/8 05/15/51\", \"COLOM 8 3/4 11/14/53\", \"COLOM 3 7/8 02/15/61\", \"MEX 4 1/8 01/21/26\", \"MEX 4.15 03/28/27\", \"MEX 3 3/4 01/11/28\", \"MEX 5.4 02/09/28\", \"MEX 4 1/2 04/22/29\", \"MEX 5 05/07/29\", \"MEX 3 1/4 04/16/30\", \"MEX 2.659 05/24/31\", \"MEX 4 3/4 04/27/32\", \"MEX 4 7/8 05/19/33\", \"MEX 3 1/2 02/12/34\", \"MEX 6.35 02/09/35\", \"MEX 6 05/07/36\", \"MEX 6.05 01/11/40\", \"MEX 4.28 08/14/41\", \"MEX 4 3/4 03/08/44\", \"MEX 5.55 01/21/45\", \"MEX 4.6 01/23/46\", \"MEX 4.35 01/15/47\", \"MEX 4.6 02/10/48\", \"MEX 4 1/2 01/31/50\", \"MEX 5 04/27/51\", \"MEX 4.4 02/12/52\", \"MEX 6.338 05/04/53\", \"MEX 6.4 05/07/54\", \"MEX 3.771 05/24/61\", \"MEX 3 3/4 04/19/2071\", \"MEX 5 3/4 10/12/2110\", \"PANAMA 3 3/4 04/17/26\", \"PANAMA 3 7/8 03/17/28\", \"PANAMA 3.16 01/23/30\", \"PANAMA 7 1/2 03/01/31\", \"PANAMA 3.362 06/30/31\", \"PANAMA 2.252 09/29/32\", \"PANAMA 3.298 01/19/33\", \"PANAMA 6.4 02/14/35\", \"PANAMA 6.7 01/26/36\", \"PANAMA 6 7/8 01/31/36\", \"PANAMA 8 03/01/38\", \"PANAMA 4 1/2 05/15/47\", \"PANAMA 4 1/2 04/16/50\", \"PANAMA 4.3 04/29/53\", \"PANAMA 6.853 03/28/54\", \"PANAMA 4 1/2 04/01/56\", \"PANAMA 3.87 07/23/60\", \"PANAMA 4 1/2 01/19/63\", \"PERU 2.392 01/23/26\", \"PERU 4 1/8 08/25/27\", \"PERU 2.844 06/20/30\", \"PERU 2.783 01/23/31\", \"PERU 1.862 12/01/32\", \"PERU 3 01/15/34\", \"PERU 5 3/8 02/08/35\", \"PERU 3.3 03/11/41\", \"PERU 5 5/8 11/18/50\", \"PERU 3.55 03/10/51\", \"PERU 5 7/8 08/08/54\", \"PERU 2.78 12/01/60\", \"PERU 3.6 01/15/2072\", \"PERU 3.23 07/28/2121\", \"SOAF 4 7/8 04/14/26\", \"SOAF 4.85 09/27/27\", \"SOAF 4.3 10/12/28\", \"SOAF 4.85 09/30/29\", \"SOAF 5 7/8 06/22/30\", \"SOAF 5 7/8 04/20/32\", \"SOAF 5 3/8 07/24/44\", \"SOAF 5 10/12/46\", \"SOAF 5.65 09/27/47\", \"SOAF 5 3/4 09/30/49\", \"SOAF 7.3 04/20/52\", \"TURKEY 4 3/4 01/26/26\", \"TURKEY 4 1/4 04/14/26\", \"TURKEY 4 7/8 10/09/26\", \"TURKEY 6 03/25/27\", \"TURKEY 8.6 09/24/27\", \"TURKEY 9 7/8 01/15/28\", \"TURKEY 5 1/8 02/17/28\", \"TURKEY 6 1/8 10/24/28\", \"TURKEY 9 3/8 03/14/29\", \"TURKEY 7 5/8 04/26/29\", \"TURKEY 5 1/4 03/13/30\", \"TURKEY 9 1/8 07/13/30\", \"TURKEY 5.95 01/15/31\", \"TURKEY 5 7/8 06/26/31\", \"TURKEY 7 1/8 07/17/32\", \"TURKEY 9 3/8 01/19/33\", \"TURKEY 6 1/2 09/20/33\", \"TURKEY 8 02/14/34\", \"TURKEY 7 5/8 05/15/34\", \"TURKEY 6 1/2 01/03/35\", \"TURKEY 6 7/8 03/17/36\", \"TURKEY 7 1/4 03/05/38\", \"TURKEY 6 3/4 05/30/40\", \"TURKEY 6 01/14/41\", \"TURKEY 4 7/8 04/16/43\", \"TURKEY 6 5/8 02/17/45\", \"TURKEY 5 3/4 05/11/47\", \"DOMREP 6 7/8 01/29/26\", \"DOMREP 5.95 01/25/27\", \"DOMREP 6 07/19/28\", \"DOMREP 5 1/2 02/22/29\", \"DOMREP 4 1/2 01/30/30\", \"DOMREP 7.05 02/03/31\", \"DOMREP 4 7/8 09/23/32\", \"DOMREP 6 02/22/33\", \"DOMREP 6.6 06/01/36\", \"DOMREP 5.3 01/21/41\", \"DOMREP 7.45 04/30/44\", \"DOMREP 6.85 01/27/45\", \"DOMREP 6 1/2 02/15/48\", \"DOMREP 6.4 06/05/49\", \"DOMREP 5 7/8 01/30/60\", \"PEMEX 6 7/8 10/16/25\", \"PEMEX 4 1/2 01/23/26\", \"PEMEX 6 7/8 08/04/26\", \"PEMEX 6.49 01/23/27\", \"PEMEX 6 1/2 03/13/27\", \"PEMEX 5.35 02/12/28\", \"PEMEX 6 1/2 01/23/29\", \"PEMEX 8 3/4 06/02/29\", \"PEMEX 6.84 01/23/30\", \"PEMEX 5.95 01/28/31\", \"PEMEX 6.7 02/16/32\", \"PEMEX 10 02/07/33\", \"PEMEX 6 5/8 06/15/35\", \"PEMEX 6 1/2 06/02/41\", \"PEMEX 6 3/8 01/23/45\", \"PEMEX 6 3/4 09/21/47\", \"PEMEX 6.35 02/12/48\", \"PEMEX 7.69 01/23/50\", \"PEMEX 6.95 01/28/60\", \"PEMEX 6 5/8 PERP\", \"PETBRA 7 3/8 01/17/27\", \"PETBRA 5.999 01/27/28\", \"PETBRA 5.6 01/03/31\", \"PETBRA 6 1/2 07/03/33\", \"PETBRA 6 01/13/35\", \"PETBRA 6 7/8 01/20/40\", \"PETBRA 6 3/4 01/27/41\", \"PETBRA 7 1/4 03/17/44\", \"PETBRA 5 1/2 06/10/51\", \"PETBRA 6.85 06/05/2115\",\n",
    "             \"ECUA 6.9 07/31/30\", \"ECUA 0 07/31/30\", \"ECUA 6.9 07/31/35\", \"ECUA 5 07/31/40\", \"ARGENT 1 07/09/29\", \"ARGENT 0 3/4 07/09/30\", \"ARGENT 4 1/8 07/09/35\", \"ARGENT 5 01/09/38\", \"ARGENT 3 1/2 07/09/41\", \"ARGENT 4 1/8 07/09/46\", \"NGERIA 7 5/8 11/21/25\", \"NGERIA 6 1/2 11/28/27\", \"NGERIA 6 1/8 09/28/28\", \"NGERIA 8 3/8 03/24/29\", \"NGERIA 7.143 02/23/30\", \"NGERIA 8.747 01/21/31\", \"NGERIA 9 5/8 06/09/31\", \"NGERIA 7 7/8 02/16/32\", \"NGERIA 7 3/8 09/28/33\", \"NGERIA 10 3/8 12/09/34\", \"NGERIA 7.696 02/23/38\", \"NGERIA 7 5/8 11/28/47\", \"NGERIA 9.248 01/21/49\", \"NGERIA 8 1/4 09/28/51\", \"KENINT 7 05/22/27\", \"KENINT 7 1/4 02/28/28\", \"KENINT 9 3/4 02/16/31\", \"KENINT 8 05/22/32\", \"KENINT 6.3 01/23/34\", \"KENINT 8 1/4 02/28/48\", \"ANGOL 9 1/2 11/12/25\", \"ANGOL 8 1/4 05/09/28\", \"ANGOL 8 11/26/29\", \"ANGOL 8 3/4 04/14/32\", \"ANGOL 9 3/8 05/08/48\", \"ANGOL 9 1/8 11/26/49\", \"SENEGL 4 3/4 03/13/28\", \"SENEGL 6 1/4 05/23/33\", \"SENEGL 5 3/8 06/08/37\", \"SENEGL 6 3/4 03/13/48\", \"EGYPT 3 7/8 02/16/26\", \"EGYPT 7 1/8 11/10/26\", \"EGYPT 7 1/2 01/31/27\", \"EGYPT 5.8 09/30/27\", \"EGYPT 6.588 02/21/28\", \"EGYPT 7 11/10/28\", \"EGYPT 7.6003 03/01/29\", \"EGYPT 7 5/8 11/10/30\", \"EGYPT 5 7/8 02/16/31\", \"EGYPT 7.0529 01/15/32\", \"EGYPT 7 5/8 05/29/32\", \"EGYPT 7.3 09/30/33\", \"EGYPT 8 1/2 01/31/47\", \"EGYPT 7.903 02/21/48\", \"EGYPT 8.7002 03/01/49\", \"EGYPT 8 7/8 05/29/50\", \"EGYPT 8 3/4 09/30/51\", \"ELSALV 8 5/8 02/28/29\", \"ELSALV 9 1/4 04/17/30\", \"ELSALV 8 1/4 04/10/32\", \"ELSALV 7.65 06/15/35\", \"ELSALV 7 5/8 02/01/41\", \"ELSALV 7.1246 01/20/50\", \"ELSALV 9 1/2 07/15/52\", \"ELSALV 9.65 11/21/54\", \n",
    "             \"PETRPE 4 3/4 06/19/32\", \"PETRPE 5 5/8 06/19/47\", \"EGYPT 6 7/8 04/30/40\", \"EGYPT 8.15 11/20/59\", \"EGYPT 7 1/2 02/16/61\", \n",
    "             \"CSNABZ 6 3/4 01/28/28\", \"CSNABZ 8 7/8 12/05/30\", \"CSNABZ 4 5/8 06/10/31\", \"CSNABZ 5 7/8 04/08/32\",\n",
    "             \"PKSTAN 6 04/08/26\", \"PKSTAN 6 7/8 12/05/27\", \"PKSTAN 7.95 01/31/29\", \"PKSTAN 7 3/8 04/08/31\", \"PKSTAN 8 7/8 04/08/51\",\n",
    "             \"BRASKM 4.5 01/10/28\", \"BRASKM 4.5 01/31/30\",\"BRASKM 8.5 01/12/31\",\"BRASKM 7.25 02/13/33\",\"BRASKM 8 10/15/34\",\"BRASKM 7.125 07/22/41\", \"BRASKM 5.875 01/31/50\",\n",
    "             ]\n",
    "names_list += ['ANGOL 10.95 12/27/30', 'BRAZIL 6 5/8 03/15/35', 'BRAZIL 5 1/2 11/06/30', 'CHILE 5.65 01/13/37', 'COLOM 7 3/8 04/25/30', 'COLOM 8 3/8 11/07/54', 'COLOM 7 3/4 11/07/36', 'COLOM 8 1/2 04/25/35', 'DOMREP 6.95 03/15/37', 'DOMREP 7.15 02/24/55', 'EGYPT 9.45 02/04/33', 'EGYPT 8 5/8 02/04/30', 'KENINT 9 1/2 03/05/36', 'MEX 6 05/13/30', 'MEX 6 5/8 01/29/38', 'MEX 5.85 07/02/32', 'MEX 6 7/8 05/13/37', 'MEX 7 3/8 05/13/55', 'PERU 6.2 06/30/55', 'PERU 5 1/2 03/30/36', 'SOAF 7.1 11/19/36', 'SOAF 7.95 11/19/54', 'TURKEY 7 1/4 05/29/32']\n",
    "names_list += [\"CHINA 1 3/4 10/26/31\", \"CHINA 1.2 10/21/30\", \"INDON 4 3/4 07/18/47\", \"INDON 4 5/8 04/15/43\", \"INDON 5 1/4 01/08/47\", \"INDON 5 1/4 01/17/42\", \"INDON 5 1/8 01/15/45\", \"INDON 5.95 01/08/46\", \"INDON 6 3/4 01/15/44\", \"INDON 6 5/8 02/17/37\", \"INDON 7 3/4 01/17/38\", \"INDON 8 1/2 10/12/35\", \"INDON 1.85 03/12/31\", \"INDON 2.15 07/28/31\", \"INDON 2.85 02/14/30\", \"INDON 3 1/2 02/14/50\", \"INDON 3.05 03/12/51\", \"INDON 3.2 09/23/61\", \"INDON 3.35 03/12/2071\", \"INDON 3.55 03/31/32\", \"INDON 3.7 10/30/49\", \"INDON 3.85 10/15/30\", \"INDON 4 3/4 09/10/34\", \"INDON 4.2 10/15/50\", \"INDON 4.3 03/31/52\", \"INDON 4.35 01/11/48\", \"INDON 4.45 04/15/70\", \"INDON 4.65 09/20/32\", \"INDON 4.7 02/10/34\", \"INDON 4.85 01/11/33\", \"INDON 5 1/4 01/15/30\", \"INDON 5.1 02/10/54\", \"INDON 5.15 09/10/54\", \"INDON 5.35 02/11/49\", \"INDON 5.6 01/15/35\", \"INDON 5.65 01/11/53\"]\n",
    "names_list += [\"BHRAIN 5 1/4 01/25/33\", \"BHRAIN 5 1/4 01/25/33\", \"BHRAIN 5 5/8 05/18/34\", \"BHRAIN 5 5/8 05/18/34\", \"BHRAIN 5 5/8 09/30/31\", \"BHRAIN 5 5/8 09/30/31\", \"BHRAIN 5.45 09/16/32\", \"BHRAIN 5.45 09/16/32\", \"BHRAIN 6 3/4 09/20/29\", \"BHRAIN 6 3/4 09/20/29\", \"BHRAIN 7 1/2 02/12/36\", \"BHRAIN 7 1/2 02/12/36\", \"BHRAIN 7 3/4 04/18/35\", \"BHRAIN 7 3/4 04/18/35\", \"BHRAIN 7 3/8 05/14/30\", \"BHRAIN 7 3/8 05/14/30\", \"KOREA 1 09/16/30\", \"KSA 2 1/4 02/02/33\", \"KSA 2 1/4 02/02/33\", \"KSA 2 3/4 02/03/32\", \"KSA 2 3/4 02/03/32\", \"KSA 3 1/4 10/22/30\", \"KSA 3 1/4 10/22/30\", \"KSA 4 1/2 04/17/30\", \"KSA 4 1/2 04/17/30\", \"KSA 4 3/4 01/16/30\", \"KSA 4 3/4 01/16/30\", \"KSA 4 7/8 07/18/33\", \"KSA 4 7/8 07/18/33\", \"KSA 5 01/16/34\", \"KSA 5 01/16/34\", \"KSA 5 1/2 10/25/32\", \"KSA 5 1/2 10/25/32\", \"KSA 5 3/8 01/13/31\", \"KSA 5 3/8 01/13/31\", \"KSA 5 5/8 01/13/35\", \"KSA 5 5/8 01/13/35\", \"OMAN 6 1/4 01/25/31\", \"OMAN 6 1/4 01/25/31\", \"OMAN 7 3/8 10/28/32\", \"OMAN 7 3/8 10/28/32\", \"PHILIP 1.648 06/10/31\", \"PHILIP 1.95 01/06/32\", \"PHILIP 2.457 05/05/30\", \"PHILIP 3.556 09/29/32\", \"PHILIP 4 3/4 03/05/35\", \"PHILIP 5 01/13/37\", \"PHILIP 5 07/17/33\", \"PHILIP 5 1/2 02/04/35\", \"PHILIP 5 1/4 05/14/34\", \"PHILIP 5.609 04/13/33\", \"PHILIP 6 3/8 01/15/32\", \"PHILIP 6 3/8 10/23/34\", \"PHILIP 7 3/4 01/14/31\", \"PHILIP 9 1/2 02/02/30\", \"QATAR 3 3/4 04/16/30\", \"QATAR 3 3/4 04/16/30\", \"QATAR 4 3/4 05/29/34\", \"QATAR 4 3/4 05/29/34\", \"QATAR 4 7/8 02/27/35\", \"QATAR 4 7/8 02/27/35\", \"QATAR 9 3/4 06/15/30\", \"QATAR 9 3/4 06/15/30\"]\n",
    "\n",
    "new_dict = dict(zip(names_list, isin_list))\n",
    "all_dict = {'ECUA 6.9 07/31/30': 'XS2214237807', 'ECUA 0 07/31/30': 'XS2214239506', 'ECUA 6.9 07/31/35': 'XS2214238441', 'ECUA 5 07/31/40': 'XS2214239175', 'ARGENT 1 07/09/29': 'US040114HX11', 'ARGENT 0 3/4 07/09/30': 'US040114HS26', 'ARGENT 4 1/8 07/09/35': 'US040114HT09', 'ARGENT 5 01/09/38': 'US040114HU71', 'ARGENT 3 1/2 07/09/41': 'US040114HV54', 'ARGENT 4 1/8 07/09/46': 'US040114HW38', 'NGERIA 7 5/8 11/21/25': 'XS1910826996', 'NGERIA 6 1/2 11/28/27': 'XS1717011982', 'NGERIA 6 1/8 09/28/28': 'XS2384698994', 'NGERIA 8 3/8 03/24/29': 'XS2445169985', 'NGERIA 7.143 02/23/30': 'XS1777972511', 'NGERIA 8.747 01/21/31': 'XS1910827887', 'NGERIA 9 5/8 06/09/31': 'XS2948511949', 'NGERIA 7 7/8 02/16/32': 'XS1566179039', 'NGERIA 7 3/8 09/28/33': 'XS2384701020', 'NGERIA 10 3/8 12/09/34': 'XS2948512913', 'NGERIA 7.696 02/23/38': 'XS1777972941', 'NGERIA 7 5/8 11/28/47': 'XS1717013095', 'NGERIA 9.248 01/21/49': 'XS1910828182', 'NGERIA 8 1/4 09/28/51': 'XS2384704800', 'KENINT 7 05/22/27': 'XS1843435840', 'KENINT 7 1/4 02/28/28': 'XS1781710543', 'KENINT 9 3/4 02/16/31': 'XS2764839945', 'KENINT 8 05/22/32': 'XS1843435766', 'KENINT 6.3 01/23/34': 'XS2354781614', 'KENINT 8 1/4 02/28/48': 'XS1781710626', 'ANGOL 9 1/2 11/12/25': 'XS1318576086', 'ANGOL 8 1/4 05/09/28': 'XS1819680288', 'ANGOL 8 11/26/29': 'XS2083302419', 'ANGOL 8 3/4 04/14/32': 'XS2446175577', 'ANGOL 9 3/8 05/08/48': 'XS1819680528', 'ANGOL 9 1/8 11/26/49': 'XS2083302500', 'SENEGL 4 3/4 03/13/28': 'XS1790104530', 'SENEGL 6 1/4 05/23/33': 'XS1619155564', 'SENEGL 5 3/8 06/08/37': 'XS2333676133', 'SENEGL 6 3/4 03/13/48': 'XS1790134362', 'ELSALV 8 5/8 02/28/29': 'USP01012BX31', 'ELSALV 9 1/4 04/17/30': 'USP01012CF16', 'ELSALV 8 1/4 04/10/32': 'XS0146173371', 'ELSALV 7.65 06/15/35': 'USP01012AN67', 'ELSALV 7 5/8 02/01/41': 'USP01012AR71', 'ELSALV 7.1246 01/20/50': 'USP01012CA29', 'ELSALV 9 1/2 07/15/52': 'USP01012CC84', 'ELSALV 9.65 11/21/54': 'USP01012CH71', 'EGYPT 3 7/8 02/16/26': 'XS2297220423', 'EGYPT 7 1/8 11/10/26': 'XS1903488572', 'EGYPT 7 1/2 01/31/27': 'XS1558078736', 'EGYPT 5.8 09/30/27': 'XS2391394348', 'EGYPT 6.588 02/21/28': 'XS1775618439', 'EGYPT 7 11/10/28': 'XS1504948776', 'EGYPT 7.6003 03/01/29': 'XS1953057061', 'EGYPT 7 5/8 11/10/30': 'XS1903489463', 'EGYPT 5 7/8 02/16/31': 'XS2297226545', 'EGYPT 7.0529 01/15/32': 'XS2079842642', 'EGYPT 7 5/8 05/29/32': 'XS2176897754', 'EGYPT 7.3 09/30/33': 'XS2391395154', 'EGYPT 6 7/8 04/30/40': 'XS0505478684', 'EGYPT 8 1/2 01/31/47': 'XS1558078496', 'EGYPT 7.903 02/21/48': 'XS1775617464', 'EGYPT 8.7002 03/01/49': 'XS1953057491', 'EGYPT 8 7/8 05/29/50': 'XS2176899701', 'EGYPT 8 3/4 09/30/51': 'XS2391398174', 'EGYPT 8.15 11/20/59': 'XS2079846635', 'EGYPT 7 1/2 02/16/61': 'XS2297221405', 'PEMEX 6 7/8 10/16/25': 'US71654QDH20', 'PEMEX 4 1/2 01/23/26': 'US71654QBW15', 'PEMEX 6 7/8 08/04/26': 'US71654QCB68', 'PEMEX 6.49 01/23/27': 'US71654QDB59', 'PEMEX 6 1/2 03/13/27': 'US71654QCG55', 'PEMEX 5.35 02/12/28': 'US71654QCK67', 'PEMEX 6 1/2 01/23/29': 'US71654QCP54', 'PEMEX 8 3/4 06/02/29': 'US71654QDL32', 'PEMEX 6.84 01/23/30': 'US71654QDC33', 'PEMEX 5.95 01/28/31': 'US71654QDE98', 'PEMEX 6.7 02/16/32': 'US71643VAB18', 'PEMEX 10 02/07/33': 'US71654QDP46', 'PEMEX 6 5/8 06/15/35': 'US706451BG56', 'PEMEX 6 1/2 06/02/41': 'US71654QAZ54', 'PEMEX 6 3/8 01/23/45': 'US71654QBR20', 'PEMEX 6 3/4 09/21/47': 'US71654QCC42', 'PEMEX 6.35 02/12/48': 'US71654QCL41', 'PEMEX 7.69 01/23/50': 'US71654QDD16', 'PEMEX 6.95 01/28/60': 'US71654QDF63', 'PEMEX 6 5/8 PERP': 'US71656MAF68', 'PETRPE 4 3/4 06/19/32': 'USP7808BAA54', 'PETRPE 5 5/8 06/19/47': 'USP7808BAB38', 'CSNABZ 6 3/4 01/28/28': 'USG2583XAB76', 'CSNABZ 8 7/8 12/05/30': 'USL21779AL44', 'CSNABZ 4 5/8 06/10/31': 'USL21779AJ97', 'CSNABZ 5 7/8 04/08/32': 'USL21779AK60', 'PKSTAN 6 04/08/26': 'XS2322319398', 'PKSTAN 6 7/8 12/05/27': 'XS1729875598', 'PKSTAN 7.95 01/31/29': 'XS2419405274', 'PKSTAN 7 3/8 04/08/31': 'XS2322319638', 'PKSTAN 8 7/8 04/08/51': 'XS2322321964',\n",
    "'ANGOL 10.95 12/27/30': 'XS2214237807','EGYPT 9.45 02/04/33': 'XS2214239506','EGYPT 8 5/8 02/04/30': 'XS2214238441','ELSALV 9.65 11/21/54': 'XS2214239175','KENINT 9 1/2 03/05/36': 'US040114HX11','NGERIA 10 3/8 12/09/34': 'US040114HS26','NGERIA 9 5/8 06/09/31': 'US040114HT09','BRASKM 4.5 01/10/28': 'USN15516AB83','BRASKM 4.5 01/31/30': 'USN15516AD40','BRASKM 8.5 01/12/31': 'USN15516AH53','BRASKM 7.25 02/13/33': 'USN15516AG70','BRASKM 8 10/15/34': 'USN15516AJ10','BRASKM 7.125 07/22/41': 'USU1065PAA94','BRASKM 5.875 01/31/50': 'USN15516AE23',\n",
    "'PETBRA 7 3/8 01/17/27': 'US71647NAS80', 'PETBRA 5.999 01/27/28': 'US71647NAY58', 'PETBRA 5.6 01/03/31': 'US71647NBH17', 'PETBRA 6 1/2 07/03/33': 'US71647NBK46', 'PETBRA 6 01/13/35': 'US71647NBL29', 'PETBRA 6 7/8 01/20/40': 'US71645WAQ42', 'PETBRA 6 3/4 01/27/41': 'US71645WAS08', 'PETBRA 7 1/4 03/17/44': 'US71647NAK54', 'PETBRA 5 1/2 06/10/51': 'US71647NBJ72', 'PETBRA 6.85 06/05/2115': 'US71647NAN93', }\n",
    "\n",
    "#Making Manual Adj. for Perp Bond of PEMEX\n",
    "# names_list[names_list.index(\"PEMEX 6 5/8 PERP\")] = \"PEMEX 6 5/8 01/01/2999\"\n",
    "\n",
    "all_dict = {**all_dict, **new_dict}\n",
    "names_list = list(all_dict.keys())\n",
    "names_list = [\"PEMEX 6 5/8 01/01/2199\" if item == \"PEMEX 6 5/8 PERP\" else item for item in names_list]\n",
    "isin_list = list(all_dict.values())\n",
    "\n",
    "all_dict = dict(zip(names_list, isin_list))\n",
    "\n",
    "##################  Change here only\n",
    "mat_date = [item.split(\" \")[-1] for item in names_list]\n",
    "names_list = [item.split(' ',1)[0] + ' ' + item.rsplit(' ',1)[0].split(' ',1)[1] + '% due ' + item.rsplit('/',1)[-1] for item in names_list]\n",
    "mat_dict = dict(zip(names_list, mat_date))\n",
    "\n",
    "fields = [\n",
    "          # [\"DB(SAGE,INSTRUMENT,id_isin,\", \",StatDurSprdEffWrst)\"],\n",
    "          # [\"DB(SAGE,INSTRUMENT,id_isin,\", \",StatCdsPESpreadAsk)\"],\n",
    "          # [\"DB(SAGE,INSTRUMENT,id_isin,\", \",StatCdsPESpreadBid)\"],\n",
    "          [\"DB(SAGE,INSTRUMENT,id_isin,\", \",StatMidPrice)\"],         \n",
    "          [\"DB(SAGE,INSTRUMENT,id_isin,\", \",AM_IDX_TOT)\"],\n",
    "          # [\"DB(SAGE,INSTRUMENT,id_isin,\", \",StatRtAskStripSolMid)*100\"],\n",
    "          [\"DB(SAGE,INSTRUMENT,id_isin,\", \",StatCdsBasis)\"],\n",
    "          # [\"DB(SAGE,INSTRUMENT,id_isin,\", \",StatDurSprdEffWrst)\"],\n",
    "          # [\"DB(SAGE,INSTRUMENT,id_isin,\", \",CMStatCdsPESpreadAskIsrDistance)\"],\n",
    "         ]\n",
    "          \n",
    "field_names = [#'Duration', \n",
    "               #'PECS Ask', 'PECS Bid', \n",
    "               'Price','TR','Basis']# 'TR', 'ZSprd', 'Basis',]#'Rich/Cheap Model Sprd to Curve',]\n",
    "\n",
    "dq_list = []\n",
    "full_names = []\n",
    "\n",
    "for i in range(len(isin_list)):\n",
    "    for j in range(len(fields)):\n",
    "        dq_list = dq_list + [fields[j][0] + isin_list[i] + fields[j][1]]\n",
    "        full_names = full_names + [names_list[i] + ' ' + field_names[j]]\n",
    "\n",
    "labels = dict(zip(full_names, dq_list))\n",
    "\n",
    "################################################# PA Data Only\n",
    "new_labels = {}\n",
    "for key, val in labels.items():\n",
    "    if key.split(\" \")[0] == \"PANAMA\":\n",
    "        new_labels[key] = val\n",
    "\n",
    "cds_dict = {\"PA Par 5Y CDS Sprd\": \"DB(JPMR-EMCREDIT,CDS-Sovereign,PA/CDS/USD/5y/ParSpread)\",\n",
    "\"PA Dur\": \"DB(JPMR-EMCREDIT,CDS-Sovereign,PA/CDS/USD/5y/Duration100)\",\n",
    "\"PA Uf 100\": \"DB(JPMR-EMCREDIT,CDS-Sovereign,PA/CDS/USD/5y/Upfront100)\",}\n",
    "\n",
    "d = {\"Hot Run | 5 year note | Current | Mid Price\" : \"DB(FHR,N5,MIDPRC)\", \"Hot Run | 5 year note | Current | Coupon\" : \"DB(FHR,N5,COUPON)\", \"Hot Run | 7 year note | Current | Mid Price\" : \"DB(FHR,N7,MIDPRC)\", \"Hot Run | 7 year note | Current | Coupon\" : \"DB(FHR,N7,COUPON)\", \"Hot Run | 10 year note | Current | Mid Price\" : \"DB(FHR,N10,MIDPRC)\", \"Hot Run | 10 year note | Current | Coupon\" : \"DB(FHR,N10,COUPON)\", \"Hot Run | 20 year bond | Current | Mid Price\" : \"DB(FHR,B20,MIDPRC)\", \"Hot Run | 20 year bond | Current | Coupon\" : \"DB(FHR,B20,COUPON)\", \"Hot Run | 30 year bond | Current | Mid Price\" : \"DB(FHR,B30,MIDPRC)\", \"Hot Run | 30 year bond | Current | Coupon\" : \"DB(FHR,B30,COUPON)\"\n",
    "    }\n",
    "new_d = {}\n",
    "for key, val in d.items():\n",
    "    new_d[\"CT\" + key.split(\" | \")[1].replace(\" year note\",\"\").replace(\" year bond\",\"\") + \"_\" + key.split(\" | \")[-1].\\\n",
    "        replace(\"Mid Price\",\"PX\")] = val\n",
    "\n",
    "################################################# PA Data Only\n",
    "labels = {**new_labels, **cds_dict, **new_d}\n",
    "\n",
    "all_start_date = str((datetime.now()-timedelta(days=365*15)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "# dq = DataQuery(\n",
    "# client_id='jbAIMF2Tkp0JO3sc',\n",
    "# client_secret='d7qfzgt55pddjs352sgxosFyI4t2eja07k7opbi6wg9oqjc1OjkdAksn1btmnugeMjchcx2vwTsJupw',\n",
    "# )\n",
    "\n",
    "# job = dq.create_job(expressions = list(labels.values()))\n",
    "# dq.start_date = all_start_date\n",
    "# var = job.execute()\n",
    "# df = job.to_pivot_table()\n",
    "# df = df.T\n",
    "# df.index = pd.to_datetime(df.index, format='%Y%m%d').date\n",
    "# df.index.name = 'Date'\n",
    "\n",
    "# df.rename(columns={v:k for k, v in labels.items()},inplace=True)\n",
    "# df.columns.name = None\n",
    "# clear_output(wait=False)\n",
    "# df = df.dropna(how='all')\n",
    "# df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# dq = df.copy()\n",
    "# dq.to_parquet(\"PA - DQ Data.parquet\")\n",
    "dq = pd.read_parquet(\"PA - DQ Data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4081d1b-fe8e-454e-b128-e20d2a6e5c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CT5_PX': 'DB(FHR,N5,MIDPRC)',\n",
       " 'CT5_Coupon': 'DB(FHR,N5,COUPON)',\n",
       " 'CT7_PX': 'DB(FHR,N7,MIDPRC)',\n",
       " 'CT7_Coupon': 'DB(FHR,N7,COUPON)',\n",
       " 'CT10_PX': 'DB(FHR,N10,MIDPRC)',\n",
       " 'CT10_Coupon': 'DB(FHR,N10,COUPON)',\n",
       " 'CT20_PX': 'DB(FHR,B20,MIDPRC)',\n",
       " 'CT20_Coupon': 'DB(FHR,B20,COUPON)',\n",
       " 'CT30_PX': 'DB(FHR,B30,MIDPRC)',\n",
       " 'CT30_Coupon': 'DB(FHR,B30,COUPON)'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dq.iloc[:,3:].dropna(how=\"all\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8914b13-8382-4b2a-bf60-db21749755a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd01825-4a65-4c1a-bbda-2fd1db6b88c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d53043-20fc-468b-8eb6-9fc1e0c64fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c17909-dc6c-4e2e-92e9-5c534399f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "bonds=pd.DataFrame({\"Bonds\":list(all_dict.keys()), \"ISIN\":list(all_dict.values())})\n",
    "bonds[\"Country\"] = bonds[\"Bonds\"].apply(lambda x: x.split(\" \")[0])\n",
    "bonds=bonds.drop_duplicates(subset=\"Country\",keep=\"first\")\n",
    "isin_list = list(bonds[\"ISIN\"])\n",
    "\n",
    "bbg_tickers = []\n",
    "for item in isin_list:\n",
    "    bbg_tickers += [f\"/isin/{item}@BGN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e3920ac-e639-4586-bf12-2ec72c34b9ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ECUA 6.9% due 30 Price': ['16:41:00', '16:30:00', '16:29:00'],\n",
       " 'ARGENT 1% due 29 Price': ['16:17:00', '16:20:00', '16:25:00'],\n",
       " 'NGERIA 7 5/8% due 25 Price': ['15:00:00', '15:03:00', '15:02:00'],\n",
       " 'KENINT 7% due 27 Price': ['11:47:00', '11:53:00', '11:51:00'],\n",
       " 'ANGOL 9 1/2% due 25 Price': ['09:07:00', '09:09:00', '10:24:00'],\n",
       " 'SENEGL 4 3/4% due 28 Price': ['11:55:00', '11:44:00', '11:43:00'],\n",
       " 'ELSALV 8 5/8% due 29 Price': ['16:01:00', '16:00:00', '16:02:00'],\n",
       " 'EGYPT 3 7/8% due 26 Price': ['16:01:00', '16:00:00', '16:25:00'],\n",
       " 'PEMEX 6 7/8% due 25 Price': ['16:16:00', '16:20:00', '16:18:00'],\n",
       " 'PETRPE 4 3/4% due 32 Price': ['16:46:00', '16:45:00', '16:44:00'],\n",
       " 'CSNABZ 6 3/4% due 28 Price': ['16:22:00', '16:21:00', '16:30:00'],\n",
       " 'PKSTAN 6% due 26 Price': ['21:19:00', '21:20:00', '21:18:00'],\n",
       " 'BRASKM 4.5% due 28 Price': ['11:48:00', '11:59:00', '11:47:00'],\n",
       " 'PETBRA 7 3/8% due 27 Price': ['11:59:00', '11:58:00', '11:57:00'],\n",
       " 'BRAZIL 6% due 26 Price': ['15:04:00', '15:05:00', '15:22:00'],\n",
       " 'CHILE 2 3/4% due 27 Price': ['11:19:00', '12:22:00', '12:16:00'],\n",
       " 'COLOM 4 1/2% due 26 Price': ['16:32:00', '13:03:00', '16:10:00'],\n",
       " 'MEX 4 1/8% due 26 Price': ['16:33:00', '16:59:00', '16:23:00'],\n",
       " 'PANAMA 3 3/4% due 26 Price': ['03:19:00', '03:31:00', '03:33:00'],\n",
       " 'PERU 2.392% due 26 Price': ['03:00:00', '03:01:00', '03:02:00'],\n",
       " 'SOAF 4 7/8% due 26 Price': ['03:02:00', '03:01:00', '03:00:00'],\n",
       " 'TURKEY 4 3/4% due 26 Price': ['03:03:00', '03:00:00', '03:01:00'],\n",
       " 'DOMREP 6 7/8% due 26 Price': ['08:58:00', '08:59:00', '08:57:00'],\n",
       " 'CHINA 1 3/4% due 31 Price': ['22:49:00', '23:36:00', '23:39:00'],\n",
       " 'INDON 4 3/4% due 47 Price': ['19:06:00', '19:07:00', '19:08:00'],\n",
       " 'BHRAIN 5 1/4% due 33 Price': ['16:49:00', '16:51:00', '16:55:00'],\n",
       " 'KOREA 1% due 30 Price': ['19:00:00', '22:26:00', '22:27:00'],\n",
       " 'KSA 2 1/4% due 33 Price': ['13:01:00', '13:00:00', '11:26:00'],\n",
       " 'OMAN 6 1/4% due 31 Price': ['16:51:00', '16:57:00', '16:45:00'],\n",
       " 'PHILIP 1.648% due 31 Price': ['23:56:00', '23:53:00', '23:52:00'],\n",
       " 'QATAR 3 3/4% due 30 Price': ['10:21:00', '10:53:00', '11:07:00']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_time = {}\n",
    "for item in bbg_tickers:\n",
    "    bbg_tickers1 = [item]\n",
    "    all_dates = None\n",
    "    i=0\n",
    "    for date in pd.date_range(\"2025-7-1\",\"2025-9-29\",freq=\"B\"):\n",
    "        all_x = None\n",
    "        for item in bbg_tickers1:\n",
    "            try:\n",
    "                x1 = blp.bdib(ticker = item, flds=\"PX_LAST\", dt = str(date.date()), ref='IndexYieldCurve') \n",
    "                x1 = x1.iloc[:,[3]]\n",
    "                all_x = pd.concat([all_x, x1],axis=1)\n",
    "            except:\n",
    "                continue\n",
    "            # display(all_x.tail(2))\n",
    "        try:\n",
    "            all_dates = pd.concat([all_x, all_dates])\n",
    "        except:\n",
    "            h=1\n",
    "        # display(all_dates)\n",
    "    all_dates.to_parquet(\"All Dates.parquet\")\n",
    "    \n",
    "    # all_x.to_parquet(\"All X.parquet\")\n",
    "    reverse_labels = dict(zip(labels.values(),labels.keys()))\n",
    "    # reverse_labels\n",
    "    \n",
    "    # df = pd.read_parquet(\"All X.parquet\")\n",
    "    df = pd.read_parquet(\"All Dates.parquet\")\n",
    "    df.columns = [item[0] for item in df.columns]\n",
    "    df.index = df.index.tz_localize(None)\n",
    "    df.columns = [reverse_labels[\"DB(SAGE,INSTRUMENT,id_isin,\"+\\\n",
    "        item.replace(\"/isin/\",\"\").replace(\"@BGN\",\"\")+\",StatMidPrice)\"] for item in df.columns]\n",
    "    # df = df[df.index.time>pd.to_datetime(\"08:00\").time()]\n",
    "    # df = df[df.index.date == pd.to_datetime(\"2025-09-29\").date()]\n",
    "    dq1 = dq.iloc[[-1],:].copy()\n",
    "    # dq1 = dq1[dq1.columns.isin(df.columns)]\n",
    "    # dq1 = dq1.loc[:, dq1.columns.intersection(df.columns)]\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    ###################################################################\n",
    "    \n",
    "    ecua = df.copy()\n",
    "    ecua[\"Time\"] = ecua.index.time\n",
    "    ecua[\"Date\"] = ecua.index.date\n",
    "    ecua = pd.pivot_table(columns='Time',index='Date',values=ecua.columns[0],data=ecua)\n",
    "    dt = pd.concat([dq[[df.columns[0]]], ecua],axis=1).dropna()\n",
    "    \n",
    "    first_col = dt.columns[0]\n",
    "    \n",
    "    std_devs = {}\n",
    "    for col in dt.columns[1:]:\n",
    "        std = ((dt[first_col] - dt[col])**2).std()\n",
    "        std_devs[col] = std\n",
    "    \n",
    "    sorted_std_devs = dict(sorted(std_devs.items(), key=lambda item: item[1]))\n",
    "    \n",
    "    best_time[df.columns[0]] = [str(list(sorted_std_devs.keys())[0]), str(list(sorted_std_devs.keys())[1]), str(list(sorted_std_devs.keys())[2])]\n",
    "    display(best_time)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d2e0eb-83e5-497a-bb94-ee3e5bf25a84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "old_best_time = {'ECUA 6.9% due 30 Price': ['16:36:00', '16:35:00', '16:34:00'],\n",
    " 'ARGENT 1% due 29 Price': ['16:25:00', '16:19:00', '16:20:00'],\n",
    " 'NGERIA 7 5/8% due 25 Price': ['15:04:00', '15:02:00', '15:03:00'],\n",
    " 'KENINT 7% due 27 Price': ['09:38:00', '09:36:00', '09:35:00'],\n",
    " 'ANGOL 9 1/2% due 25 Price': ['03:22:00', '03:21:00', '03:23:00'],\n",
    " 'SENEGL 4 3/4% due 28 Price': ['05:57:00', '06:01:00', '05:59:00'],\n",
    " 'ELSALV 8 5/8% due 29 Price': ['16:27:00', '16:25:00', '16:28:00'],\n",
    " 'EGYPT 3 7/8% due 26 Price': ['07:50:00', '07:56:00', '07:51:00'],\n",
    " 'PEMEX 6 7/8% due 25 Price': ['06:48:00', '06:46:00', '06:59:00'],\n",
    " 'PETRPE 4 3/4% due 32 Price': ['16:32:00', '16:31:00', '16:30:00'],\n",
    " 'CSNABZ 6 3/4% due 28 Price': ['16:59:00', '16:47:00', '16:54:00'],\n",
    " 'PKSTAN 6% due 26 Price': ['05:49:00', '05:48:00', '05:46:00'],\n",
    " 'BRASKM 4.5% due 28 Price': ['11:59:00', '11:57:00', '11:58:00'],\n",
    " 'PETBRA 7 3/8% due 27 Price': ['11:38:00', '11:40:00', '11:39:00'],\n",
    " 'BRAZIL 6% due 26 Price': ['10:58:00', '10:53:00', '08:56:00'],\n",
    " 'CHILE 2 3/4% due 27 Price': ['11:18:00', '11:19:00', '11:20:00'],\n",
    " 'COLOM 4 1/2% due 26 Price': ['11:27:00', '11:39:00', '11:36:00'],\n",
    " 'MEX 4 1/8% due 26 Price': ['07:08:00', '07:09:00', '07:10:00'],\n",
    " 'PANAMA 3 3/4% due 26 Price': ['03:19:00', '03:31:00', '03:33:00'],\n",
    " 'PERU 2.392% due 26 Price': ['03:00:00', '03:01:00', '03:02:00'],\n",
    " 'SOAF 4 7/8% due 26 Price': ['09:25:00', '09:24:00', '09:16:00'],\n",
    " 'TURKEY 4 3/4% due 26 Price': ['11:00:00', '10:50:00', '10:49:00'],\n",
    " 'DOMREP 6 7/8% due 26 Price': ['08:20:00', '08:21:00', '08:22:00'],\n",
    " 'CHINA 1 3/4% due 31 Price': ['22:48:00', '22:52:00', '22:53:00'],\n",
    " 'INDON 4 3/4% due 47 Price': ['20:14:00', '20:15:00', '20:12:00'],\n",
    " 'BHRAIN 5 1/4% due 33 Price': ['12:42:00', '12:41:00', '12:40:00'],\n",
    " 'KOREA 1% due 30 Price': ['20:51:00', '20:52:00', '20:45:00'],\n",
    " 'KSA 2 1/4% due 33 Price': ['13:00:00', '12:17:00', '12:16:00'],\n",
    " 'OMAN 6 1/4% due 31 Price': ['16:51:00', '16:59:00', '16:45:00'],\n",
    " 'PHILIP 1.648% due 31 Price': ['21:43:00', '21:44:00', '21:35:00'],\n",
    " 'QATAR 3 3/4% due 30 Price': ['11:03:00', '11:07:00', '11:19:00']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee114ee9-74a8-4325-8714-09e5e1a1e54d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1M Best Times</th>\n",
       "      <th>3M Best Times</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANGOL 9 1/2% due 25 Price</th>\n",
       "      <td>[03:22, 03:21, 03:23]</td>\n",
       "      <td>[09:07, 09:09, 10:24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARGENT 1% due 29 Price</th>\n",
       "      <td>[16:25, 16:19, 16:20]</td>\n",
       "      <td>[16:17, 16:20, 16:25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHRAIN 5 1/4% due 33 Price</th>\n",
       "      <td>[12:42, 12:41, 12:40]</td>\n",
       "      <td>[16:49, 16:51, 16:55]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRASKM 4.5% due 28 Price</th>\n",
       "      <td>[11:59, 11:57, 11:58]</td>\n",
       "      <td>[11:48, 11:59, 11:47]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRAZIL 6% due 26 Price</th>\n",
       "      <td>[10:58, 10:53, 08:56]</td>\n",
       "      <td>[15:04, 15:05, 15:22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHILE 2 3/4% due 27 Price</th>\n",
       "      <td>[11:18, 11:19, 11:20]</td>\n",
       "      <td>[11:19, 12:22, 12:16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHINA 1 3/4% due 31 Price</th>\n",
       "      <td>[22:48, 22:52, 22:53]</td>\n",
       "      <td>[22:49, 23:36, 23:39]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COLOM 4 1/2% due 26 Price</th>\n",
       "      <td>[11:27, 11:39, 11:36]</td>\n",
       "      <td>[16:32, 13:03, 16:10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSNABZ 6 3/4% due 28 Price</th>\n",
       "      <td>[16:59, 16:47, 16:54]</td>\n",
       "      <td>[16:22, 16:21, 16:30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOMREP 6 7/8% due 26 Price</th>\n",
       "      <td>[08:20, 08:21, 08:22]</td>\n",
       "      <td>[08:58, 08:59, 08:57]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ECUA 6.9% due 30 Price</th>\n",
       "      <td>[16:36, 16:35, 16:34]</td>\n",
       "      <td>[16:41, 16:30, 16:29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EGYPT 3 7/8% due 26 Price</th>\n",
       "      <td>[07:50, 07:56, 07:51]</td>\n",
       "      <td>[16:01, 16:00, 16:25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELSALV 8 5/8% due 29 Price</th>\n",
       "      <td>[16:27, 16:25, 16:28]</td>\n",
       "      <td>[16:01, 16:00, 16:02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDON 4 3/4% due 47 Price</th>\n",
       "      <td>[20:14, 20:15, 20:12]</td>\n",
       "      <td>[19:06, 19:07, 19:08]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KENINT 7% due 27 Price</th>\n",
       "      <td>[09:38, 09:36, 09:35]</td>\n",
       "      <td>[11:47, 11:53, 11:51]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KOREA 1% due 30 Price</th>\n",
       "      <td>[20:51, 20:52, 20:45]</td>\n",
       "      <td>[19:00, 22:26, 22:27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KSA 2 1/4% due 33 Price</th>\n",
       "      <td>[13:00, 12:17, 12:16]</td>\n",
       "      <td>[13:01, 13:00, 11:26]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEX 4 1/8% due 26 Price</th>\n",
       "      <td>[07:08, 07:09, 07:10]</td>\n",
       "      <td>[16:33, 16:59, 16:23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NGERIA 7 5/8% due 25 Price</th>\n",
       "      <td>[15:04, 15:02, 15:03]</td>\n",
       "      <td>[15:00, 15:03, 15:02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OMAN 6 1/4% due 31 Price</th>\n",
       "      <td>[16:51, 16:59, 16:45]</td>\n",
       "      <td>[16:51, 16:57, 16:45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PANAMA 3 3/4% due 26 Price</th>\n",
       "      <td>[03:19, 03:31, 03:33]</td>\n",
       "      <td>[03:19, 03:31, 03:33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PEMEX 6 7/8% due 25 Price</th>\n",
       "      <td>[06:48, 06:46, 06:59]</td>\n",
       "      <td>[16:16, 16:20, 16:18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERU 2.392% due 26 Price</th>\n",
       "      <td>[03:00, 03:01, 03:02]</td>\n",
       "      <td>[03:00, 03:01, 03:02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PETBRA 7 3/8% due 27 Price</th>\n",
       "      <td>[11:38, 11:40, 11:39]</td>\n",
       "      <td>[11:59, 11:58, 11:57]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PETRPE 4 3/4% due 32 Price</th>\n",
       "      <td>[16:32, 16:31, 16:30]</td>\n",
       "      <td>[16:46, 16:45, 16:44]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHILIP 1.648% due 31 Price</th>\n",
       "      <td>[21:43, 21:44, 21:35]</td>\n",
       "      <td>[23:56, 23:53, 23:52]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PKSTAN 6% due 26 Price</th>\n",
       "      <td>[05:49, 05:48, 05:46]</td>\n",
       "      <td>[21:19, 21:20, 21:18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QATAR 3 3/4% due 30 Price</th>\n",
       "      <td>[11:03, 11:07, 11:19]</td>\n",
       "      <td>[10:21, 10:53, 11:07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SENEGL 4 3/4% due 28 Price</th>\n",
       "      <td>[05:57, 06:01, 05:59]</td>\n",
       "      <td>[11:55, 11:44, 11:43]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOAF 4 7/8% due 26 Price</th>\n",
       "      <td>[09:25, 09:24, 09:16]</td>\n",
       "      <td>[03:02, 03:01, 03:00]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TURKEY 4 3/4% due 26 Price</th>\n",
       "      <td>[11:00, 10:50, 10:49]</td>\n",
       "      <td>[03:03, 03:00, 03:01]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    1M Best Times          3M Best Times\n",
       "Name                                                                    \n",
       "ANGOL 9 1/2% due 25 Price   [03:22, 03:21, 03:23]  [09:07, 09:09, 10:24]\n",
       "ARGENT 1% due 29 Price      [16:25, 16:19, 16:20]  [16:17, 16:20, 16:25]\n",
       "BHRAIN 5 1/4% due 33 Price  [12:42, 12:41, 12:40]  [16:49, 16:51, 16:55]\n",
       "BRASKM 4.5% due 28 Price    [11:59, 11:57, 11:58]  [11:48, 11:59, 11:47]\n",
       "BRAZIL 6% due 26 Price      [10:58, 10:53, 08:56]  [15:04, 15:05, 15:22]\n",
       "CHILE 2 3/4% due 27 Price   [11:18, 11:19, 11:20]  [11:19, 12:22, 12:16]\n",
       "CHINA 1 3/4% due 31 Price   [22:48, 22:52, 22:53]  [22:49, 23:36, 23:39]\n",
       "COLOM 4 1/2% due 26 Price   [11:27, 11:39, 11:36]  [16:32, 13:03, 16:10]\n",
       "CSNABZ 6 3/4% due 28 Price  [16:59, 16:47, 16:54]  [16:22, 16:21, 16:30]\n",
       "DOMREP 6 7/8% due 26 Price  [08:20, 08:21, 08:22]  [08:58, 08:59, 08:57]\n",
       "ECUA 6.9% due 30 Price      [16:36, 16:35, 16:34]  [16:41, 16:30, 16:29]\n",
       "EGYPT 3 7/8% due 26 Price   [07:50, 07:56, 07:51]  [16:01, 16:00, 16:25]\n",
       "ELSALV 8 5/8% due 29 Price  [16:27, 16:25, 16:28]  [16:01, 16:00, 16:02]\n",
       "INDON 4 3/4% due 47 Price   [20:14, 20:15, 20:12]  [19:06, 19:07, 19:08]\n",
       "KENINT 7% due 27 Price      [09:38, 09:36, 09:35]  [11:47, 11:53, 11:51]\n",
       "KOREA 1% due 30 Price       [20:51, 20:52, 20:45]  [19:00, 22:26, 22:27]\n",
       "KSA 2 1/4% due 33 Price     [13:00, 12:17, 12:16]  [13:01, 13:00, 11:26]\n",
       "MEX 4 1/8% due 26 Price     [07:08, 07:09, 07:10]  [16:33, 16:59, 16:23]\n",
       "NGERIA 7 5/8% due 25 Price  [15:04, 15:02, 15:03]  [15:00, 15:03, 15:02]\n",
       "OMAN 6 1/4% due 31 Price    [16:51, 16:59, 16:45]  [16:51, 16:57, 16:45]\n",
       "PANAMA 3 3/4% due 26 Price  [03:19, 03:31, 03:33]  [03:19, 03:31, 03:33]\n",
       "PEMEX 6 7/8% due 25 Price   [06:48, 06:46, 06:59]  [16:16, 16:20, 16:18]\n",
       "PERU 2.392% due 26 Price    [03:00, 03:01, 03:02]  [03:00, 03:01, 03:02]\n",
       "PETBRA 7 3/8% due 27 Price  [11:38, 11:40, 11:39]  [11:59, 11:58, 11:57]\n",
       "PETRPE 4 3/4% due 32 Price  [16:32, 16:31, 16:30]  [16:46, 16:45, 16:44]\n",
       "PHILIP 1.648% due 31 Price  [21:43, 21:44, 21:35]  [23:56, 23:53, 23:52]\n",
       "PKSTAN 6% due 26 Price      [05:49, 05:48, 05:46]  [21:19, 21:20, 21:18]\n",
       "QATAR 3 3/4% due 30 Price   [11:03, 11:07, 11:19]  [10:21, 10:53, 11:07]\n",
       "SENEGL 4 3/4% due 28 Price  [05:57, 06:01, 05:59]  [11:55, 11:44, 11:43]\n",
       "SOAF 4 7/8% due 26 Price    [09:25, 09:24, 09:16]  [03:02, 03:01, 03:00]\n",
       "TURKEY 4 3/4% due 26 Price  [11:00, 10:50, 10:49]  [03:03, 03:00, 03:01]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ty = pd.DataFrame({\"Name\":best_time.keys()})\n",
    "ty[\"1M Best Times\"] = ty[\"Name\"].apply(lambda x: [str(item)[:-3] for item in old_best_time[x]])\n",
    "ty[\"3M Best Times\"] = ty[\"Name\"].apply(lambda x: [str(item)[:-3] for item in best_time[x]])\n",
    "ty[\"Country\"] = ty[\"Name\"].apply(lambda x: x.split(\" \")[0])\n",
    "ty = ty.sort_values(by=\"Country\").set_index(\"Name\").drop(\"Country\",axis=1)\n",
    "ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "719bae78-9664-4f35-a959-6b217b89b020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>/isin/US74727PBD24@BGN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-09-29 03:00:00-04:00</th>\n",
       "      <td>99.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-29 03:01:00-04:00</th>\n",
       "      <td>99.407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-29 03:02:00-04:00</th>\n",
       "      <td>99.409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-29 03:03:00-04:00</th>\n",
       "      <td>99.406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-29 03:04:00-04:00</th>\n",
       "      <td>99.408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-01 16:55:00-04:00</th>\n",
       "      <td>98.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-01 16:56:00-04:00</th>\n",
       "      <td>98.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-01 16:57:00-04:00</th>\n",
       "      <td>98.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-01 16:58:00-04:00</th>\n",
       "      <td>98.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-01 16:59:00-04:00</th>\n",
       "      <td>98.023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52901 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          /isin/US74727PBD24@BGN\n",
       "                                           close\n",
       "2025-09-29 03:00:00-04:00                 99.411\n",
       "2025-09-29 03:01:00-04:00                 99.407\n",
       "2025-09-29 03:02:00-04:00                 99.409\n",
       "2025-09-29 03:03:00-04:00                 99.406\n",
       "2025-09-29 03:04:00-04:00                 99.408\n",
       "...                                          ...\n",
       "2025-07-01 16:55:00-04:00                 98.024\n",
       "2025-07-01 16:56:00-04:00                 98.024\n",
       "2025-07-01 16:57:00-04:00                 98.024\n",
       "2025-07-01 16:58:00-04:00                 98.023\n",
       "2025-07-01 16:59:00-04:00                 98.023\n",
       "\n",
       "[52901 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dates   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5a66b0b-6d2f-4dd7-aa0f-768af69d52b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([03:00:00, 03:01:00, 03:02:00, 03:03:00, 03:04:00, 03:05:00, 03:06:00,\n",
       "       03:07:00, 03:08:00, 03:09:00,\n",
       "       ...\n",
       "       16:50:00, 16:51:00, 16:52:00, 16:53:00, 16:54:00, 16:55:00, 16:56:00,\n",
       "       16:57:00, 16:58:00, 16:59:00],\n",
       "      dtype='object', name='Time', length=840)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecua.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6c9d0a3-5656-47da-a3eb-e4bc29c4ef99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Timestamp('2025-09-29 03:00:00'): nan,\n",
       " Timestamp('2025-09-29 03:01:00'): nan,\n",
       " Timestamp('2025-09-29 03:02:00'): nan,\n",
       " Timestamp('2025-09-29 03:03:00'): nan,\n",
       " Timestamp('2025-09-29 03:04:00'): nan,\n",
       " Timestamp('2025-09-29 03:05:00'): nan,\n",
       " Timestamp('2025-09-29 03:06:00'): nan,\n",
       " Timestamp('2025-09-29 03:07:00'): nan,\n",
       " Timestamp('2025-09-29 03:08:00'): nan,\n",
       " Timestamp('2025-09-29 03:09:00'): nan,\n",
       " Timestamp('2025-09-29 03:10:00'): nan,\n",
       " Timestamp('2025-09-29 03:11:00'): nan,\n",
       " Timestamp('2025-09-29 03:12:00'): nan,\n",
       " Timestamp('2025-09-29 03:13:00'): nan,\n",
       " Timestamp('2025-09-29 03:14:00'): nan,\n",
       " Timestamp('2025-09-29 03:15:00'): nan,\n",
       " Timestamp('2025-09-29 03:16:00'): nan,\n",
       " Timestamp('2025-09-29 03:17:00'): nan,\n",
       " Timestamp('2025-09-29 03:18:00'): nan,\n",
       " Timestamp('2025-09-29 03:19:00'): nan,\n",
       " Timestamp('2025-09-29 03:20:00'): nan,\n",
       " Timestamp('2025-09-29 03:21:00'): nan,\n",
       " Timestamp('2025-09-29 03:22:00'): nan,\n",
       " Timestamp('2025-09-29 03:23:00'): nan,\n",
       " Timestamp('2025-09-29 03:24:00'): nan,\n",
       " Timestamp('2025-09-29 03:25:00'): nan,\n",
       " Timestamp('2025-09-29 03:26:00'): nan,\n",
       " Timestamp('2025-09-29 03:27:00'): nan,\n",
       " Timestamp('2025-09-29 03:28:00'): nan,\n",
       " Timestamp('2025-09-29 03:29:00'): nan,\n",
       " Timestamp('2025-09-29 03:30:00'): nan,\n",
       " Timestamp('2025-09-29 03:31:00'): nan,\n",
       " Timestamp('2025-09-29 03:32:00'): nan,\n",
       " Timestamp('2025-09-29 03:33:00'): nan,\n",
       " Timestamp('2025-09-29 03:34:00'): nan,\n",
       " Timestamp('2025-09-29 03:35:00'): nan,\n",
       " Timestamp('2025-09-29 03:36:00'): nan,\n",
       " Timestamp('2025-09-29 03:37:00'): nan,\n",
       " Timestamp('2025-09-29 03:38:00'): nan,\n",
       " Timestamp('2025-09-29 03:39:00'): nan,\n",
       " Timestamp('2025-09-29 03:40:00'): nan,\n",
       " Timestamp('2025-09-29 03:41:00'): nan,\n",
       " Timestamp('2025-09-29 03:42:00'): nan,\n",
       " Timestamp('2025-09-29 03:43:00'): nan,\n",
       " Timestamp('2025-09-29 03:44:00'): nan,\n",
       " Timestamp('2025-09-29 03:45:00'): nan,\n",
       " Timestamp('2025-09-29 03:46:00'): nan,\n",
       " Timestamp('2025-09-29 03:47:00'): nan,\n",
       " Timestamp('2025-09-29 03:48:00'): nan,\n",
       " Timestamp('2025-09-29 03:49:00'): nan,\n",
       " Timestamp('2025-09-29 03:50:00'): nan,\n",
       " Timestamp('2025-09-29 03:51:00'): nan,\n",
       " Timestamp('2025-09-29 03:52:00'): nan,\n",
       " Timestamp('2025-09-29 03:53:00'): nan,\n",
       " Timestamp('2025-09-29 03:54:00'): nan,\n",
       " Timestamp('2025-09-29 03:55:00'): nan,\n",
       " Timestamp('2025-09-29 03:56:00'): nan,\n",
       " Timestamp('2025-09-29 03:57:00'): nan,\n",
       " Timestamp('2025-09-29 03:58:00'): nan,\n",
       " Timestamp('2025-09-29 03:59:00'): nan,\n",
       " Timestamp('2025-09-29 04:00:00'): nan,\n",
       " Timestamp('2025-09-29 04:01:00'): nan,\n",
       " Timestamp('2025-09-29 04:02:00'): nan,\n",
       " Timestamp('2025-09-29 04:03:00'): nan,\n",
       " Timestamp('2025-09-29 04:04:00'): nan,\n",
       " Timestamp('2025-09-29 04:05:00'): nan,\n",
       " Timestamp('2025-09-29 04:06:00'): nan,\n",
       " Timestamp('2025-09-29 04:07:00'): nan,\n",
       " Timestamp('2025-09-29 04:08:00'): nan,\n",
       " Timestamp('2025-09-29 04:09:00'): nan,\n",
       " Timestamp('2025-09-29 04:10:00'): nan,\n",
       " Timestamp('2025-09-29 04:11:00'): nan,\n",
       " Timestamp('2025-09-29 04:12:00'): nan,\n",
       " Timestamp('2025-09-29 04:13:00'): nan,\n",
       " Timestamp('2025-09-29 04:14:00'): nan,\n",
       " Timestamp('2025-09-29 04:15:00'): nan,\n",
       " Timestamp('2025-09-29 04:16:00'): nan,\n",
       " Timestamp('2025-09-29 04:17:00'): nan,\n",
       " Timestamp('2025-09-29 04:18:00'): nan,\n",
       " Timestamp('2025-09-29 04:19:00'): nan,\n",
       " Timestamp('2025-09-29 04:20:00'): nan,\n",
       " Timestamp('2025-09-29 04:21:00'): nan,\n",
       " Timestamp('2025-09-29 04:22:00'): nan,\n",
       " Timestamp('2025-09-29 04:23:00'): nan,\n",
       " Timestamp('2025-09-29 04:24:00'): nan,\n",
       " Timestamp('2025-09-29 04:25:00'): nan,\n",
       " Timestamp('2025-09-29 04:26:00'): nan,\n",
       " Timestamp('2025-09-29 04:27:00'): nan,\n",
       " Timestamp('2025-09-29 04:28:00'): nan,\n",
       " Timestamp('2025-09-29 04:29:00'): nan,\n",
       " Timestamp('2025-09-29 04:30:00'): nan,\n",
       " Timestamp('2025-09-29 04:31:00'): nan,\n",
       " Timestamp('2025-09-29 04:32:00'): nan,\n",
       " Timestamp('2025-09-29 04:33:00'): nan,\n",
       " Timestamp('2025-09-29 04:34:00'): nan,\n",
       " Timestamp('2025-09-29 04:35:00'): nan,\n",
       " Timestamp('2025-09-29 04:36:00'): nan,\n",
       " Timestamp('2025-09-29 04:37:00'): nan,\n",
       " Timestamp('2025-09-29 04:38:00'): nan,\n",
       " Timestamp('2025-09-29 04:39:00'): nan,\n",
       " Timestamp('2025-09-29 04:40:00'): nan,\n",
       " Timestamp('2025-09-29 04:41:00'): nan,\n",
       " Timestamp('2025-09-29 04:42:00'): nan,\n",
       " Timestamp('2025-09-29 04:43:00'): nan,\n",
       " Timestamp('2025-09-29 04:44:00'): nan,\n",
       " Timestamp('2025-09-29 04:45:00'): nan,\n",
       " Timestamp('2025-09-29 04:46:00'): nan,\n",
       " Timestamp('2025-09-29 04:47:00'): nan,\n",
       " Timestamp('2025-09-29 04:48:00'): nan,\n",
       " Timestamp('2025-09-29 04:49:00'): nan,\n",
       " Timestamp('2025-09-29 04:50:00'): nan,\n",
       " Timestamp('2025-09-29 04:51:00'): nan,\n",
       " Timestamp('2025-09-29 04:52:00'): nan,\n",
       " Timestamp('2025-09-29 04:53:00'): nan,\n",
       " Timestamp('2025-09-29 04:54:00'): nan,\n",
       " Timestamp('2025-09-29 04:55:00'): nan,\n",
       " Timestamp('2025-09-29 04:56:00'): nan,\n",
       " Timestamp('2025-09-29 04:57:00'): nan,\n",
       " Timestamp('2025-09-29 04:58:00'): nan,\n",
       " Timestamp('2025-09-29 04:59:00'): nan,\n",
       " Timestamp('2025-09-29 05:00:00'): nan,\n",
       " Timestamp('2025-09-29 05:01:00'): nan,\n",
       " Timestamp('2025-09-29 05:02:00'): nan,\n",
       " Timestamp('2025-09-29 05:03:00'): nan,\n",
       " Timestamp('2025-09-29 05:04:00'): nan,\n",
       " Timestamp('2025-09-29 05:05:00'): nan,\n",
       " Timestamp('2025-09-29 05:06:00'): nan,\n",
       " Timestamp('2025-09-29 05:07:00'): nan,\n",
       " Timestamp('2025-09-29 05:08:00'): nan,\n",
       " Timestamp('2025-09-29 05:09:00'): nan,\n",
       " Timestamp('2025-09-29 05:10:00'): nan,\n",
       " Timestamp('2025-09-29 05:11:00'): nan,\n",
       " Timestamp('2025-09-29 05:12:00'): nan,\n",
       " Timestamp('2025-09-29 05:13:00'): nan,\n",
       " Timestamp('2025-09-29 05:14:00'): nan,\n",
       " Timestamp('2025-09-29 05:15:00'): nan,\n",
       " Timestamp('2025-09-29 05:16:00'): nan,\n",
       " Timestamp('2025-09-29 05:17:00'): nan,\n",
       " Timestamp('2025-09-29 05:18:00'): nan,\n",
       " Timestamp('2025-09-29 05:19:00'): nan,\n",
       " Timestamp('2025-09-29 05:20:00'): nan,\n",
       " Timestamp('2025-09-29 05:21:00'): nan,\n",
       " Timestamp('2025-09-29 05:22:00'): nan,\n",
       " Timestamp('2025-09-29 05:23:00'): nan,\n",
       " Timestamp('2025-09-29 05:24:00'): nan,\n",
       " Timestamp('2025-09-29 05:25:00'): nan,\n",
       " Timestamp('2025-09-29 05:26:00'): nan,\n",
       " Timestamp('2025-09-29 05:27:00'): nan,\n",
       " Timestamp('2025-09-29 05:28:00'): nan,\n",
       " Timestamp('2025-09-29 05:29:00'): nan,\n",
       " Timestamp('2025-09-29 05:30:00'): nan,\n",
       " Timestamp('2025-09-29 05:31:00'): nan,\n",
       " Timestamp('2025-09-29 05:32:00'): nan,\n",
       " Timestamp('2025-09-29 05:33:00'): nan,\n",
       " Timestamp('2025-09-29 05:34:00'): nan,\n",
       " Timestamp('2025-09-29 05:35:00'): nan,\n",
       " Timestamp('2025-09-29 05:36:00'): nan,\n",
       " Timestamp('2025-09-29 05:37:00'): nan,\n",
       " Timestamp('2025-09-29 05:38:00'): nan,\n",
       " Timestamp('2025-09-29 05:39:00'): nan,\n",
       " Timestamp('2025-09-29 05:40:00'): nan,\n",
       " Timestamp('2025-09-29 05:41:00'): nan,\n",
       " Timestamp('2025-09-29 05:42:00'): nan,\n",
       " Timestamp('2025-09-29 05:43:00'): nan,\n",
       " Timestamp('2025-09-29 05:44:00'): nan,\n",
       " Timestamp('2025-09-29 05:45:00'): nan,\n",
       " Timestamp('2025-09-29 05:46:00'): nan,\n",
       " Timestamp('2025-09-29 05:47:00'): nan,\n",
       " Timestamp('2025-09-29 05:48:00'): nan,\n",
       " Timestamp('2025-09-29 05:49:00'): nan,\n",
       " Timestamp('2025-09-29 05:50:00'): nan,\n",
       " Timestamp('2025-09-29 05:51:00'): nan,\n",
       " Timestamp('2025-09-29 05:52:00'): nan,\n",
       " Timestamp('2025-09-29 05:53:00'): nan,\n",
       " Timestamp('2025-09-29 05:54:00'): nan,\n",
       " Timestamp('2025-09-29 05:55:00'): nan,\n",
       " Timestamp('2025-09-29 05:56:00'): nan,\n",
       " Timestamp('2025-09-29 05:57:00'): nan,\n",
       " Timestamp('2025-09-29 05:58:00'): nan,\n",
       " Timestamp('2025-09-29 05:59:00'): nan,\n",
       " Timestamp('2025-09-29 06:00:00'): nan,\n",
       " Timestamp('2025-09-29 06:01:00'): nan,\n",
       " Timestamp('2025-09-29 06:02:00'): nan,\n",
       " Timestamp('2025-09-29 06:03:00'): nan,\n",
       " Timestamp('2025-09-29 06:04:00'): nan,\n",
       " Timestamp('2025-09-29 06:05:00'): nan,\n",
       " Timestamp('2025-09-29 06:06:00'): nan,\n",
       " Timestamp('2025-09-29 06:07:00'): nan,\n",
       " Timestamp('2025-09-29 06:08:00'): nan,\n",
       " Timestamp('2025-09-29 06:09:00'): nan,\n",
       " Timestamp('2025-09-29 06:10:00'): nan,\n",
       " Timestamp('2025-09-29 06:11:00'): nan,\n",
       " Timestamp('2025-09-29 06:12:00'): nan,\n",
       " Timestamp('2025-09-29 06:13:00'): nan,\n",
       " Timestamp('2025-09-29 06:14:00'): nan,\n",
       " Timestamp('2025-09-29 06:15:00'): nan,\n",
       " Timestamp('2025-09-29 06:16:00'): nan,\n",
       " Timestamp('2025-09-29 06:17:00'): nan,\n",
       " Timestamp('2025-09-29 06:18:00'): nan,\n",
       " Timestamp('2025-09-29 06:19:00'): nan,\n",
       " Timestamp('2025-09-29 06:20:00'): nan,\n",
       " Timestamp('2025-09-29 06:21:00'): nan,\n",
       " Timestamp('2025-09-29 06:22:00'): nan,\n",
       " Timestamp('2025-09-29 06:23:00'): nan,\n",
       " Timestamp('2025-09-29 06:24:00'): nan,\n",
       " Timestamp('2025-09-29 06:25:00'): nan,\n",
       " Timestamp('2025-09-29 06:26:00'): nan,\n",
       " Timestamp('2025-09-29 06:27:00'): nan,\n",
       " Timestamp('2025-09-29 06:28:00'): nan,\n",
       " Timestamp('2025-09-29 06:29:00'): nan,\n",
       " Timestamp('2025-09-29 06:30:00'): nan,\n",
       " Timestamp('2025-09-29 06:31:00'): nan,\n",
       " Timestamp('2025-09-29 06:32:00'): nan,\n",
       " Timestamp('2025-09-29 06:33:00'): nan,\n",
       " Timestamp('2025-09-29 06:34:00'): nan,\n",
       " Timestamp('2025-09-29 06:35:00'): nan,\n",
       " Timestamp('2025-09-29 06:36:00'): nan,\n",
       " Timestamp('2025-09-29 06:37:00'): nan,\n",
       " Timestamp('2025-09-29 06:38:00'): nan,\n",
       " Timestamp('2025-09-29 06:39:00'): nan,\n",
       " Timestamp('2025-09-29 06:40:00'): nan,\n",
       " Timestamp('2025-09-29 06:41:00'): nan,\n",
       " Timestamp('2025-09-29 06:42:00'): nan,\n",
       " Timestamp('2025-09-29 06:43:00'): nan,\n",
       " Timestamp('2025-09-29 06:44:00'): nan,\n",
       " Timestamp('2025-09-29 06:45:00'): nan,\n",
       " Timestamp('2025-09-29 06:46:00'): nan,\n",
       " Timestamp('2025-09-29 06:47:00'): nan,\n",
       " Timestamp('2025-09-29 06:48:00'): nan,\n",
       " Timestamp('2025-09-29 06:49:00'): nan,\n",
       " Timestamp('2025-09-29 06:50:00'): nan,\n",
       " Timestamp('2025-09-29 06:51:00'): nan,\n",
       " Timestamp('2025-09-29 06:52:00'): nan,\n",
       " Timestamp('2025-09-29 06:53:00'): nan,\n",
       " Timestamp('2025-09-29 06:54:00'): nan,\n",
       " Timestamp('2025-09-29 06:55:00'): nan,\n",
       " Timestamp('2025-09-29 06:56:00'): nan,\n",
       " Timestamp('2025-09-29 06:57:00'): nan,\n",
       " Timestamp('2025-09-29 06:58:00'): nan,\n",
       " Timestamp('2025-09-29 06:59:00'): nan,\n",
       " Timestamp('2025-09-29 07:00:00'): nan,\n",
       " Timestamp('2025-09-29 07:01:00'): nan,\n",
       " Timestamp('2025-09-29 07:02:00'): nan,\n",
       " Timestamp('2025-09-29 07:03:00'): nan,\n",
       " Timestamp('2025-09-29 07:04:00'): nan,\n",
       " Timestamp('2025-09-29 07:05:00'): nan,\n",
       " Timestamp('2025-09-29 07:06:00'): nan,\n",
       " Timestamp('2025-09-29 07:07:00'): nan,\n",
       " Timestamp('2025-09-29 07:08:00'): nan,\n",
       " Timestamp('2025-09-29 07:09:00'): nan,\n",
       " Timestamp('2025-09-29 07:10:00'): nan,\n",
       " Timestamp('2025-09-29 07:11:00'): nan,\n",
       " Timestamp('2025-09-29 07:12:00'): nan,\n",
       " Timestamp('2025-09-29 07:13:00'): nan,\n",
       " Timestamp('2025-09-29 07:14:00'): nan,\n",
       " Timestamp('2025-09-29 07:15:00'): nan,\n",
       " Timestamp('2025-09-29 07:16:00'): nan,\n",
       " Timestamp('2025-09-29 07:17:00'): nan,\n",
       " Timestamp('2025-09-29 07:18:00'): nan,\n",
       " Timestamp('2025-09-29 07:19:00'): nan,\n",
       " Timestamp('2025-09-29 07:20:00'): nan,\n",
       " Timestamp('2025-09-29 07:21:00'): nan,\n",
       " Timestamp('2025-09-29 07:22:00'): nan,\n",
       " Timestamp('2025-09-29 07:23:00'): nan,\n",
       " Timestamp('2025-09-29 07:24:00'): nan,\n",
       " Timestamp('2025-09-29 07:25:00'): nan,\n",
       " Timestamp('2025-09-29 07:26:00'): nan,\n",
       " Timestamp('2025-09-29 07:27:00'): nan,\n",
       " Timestamp('2025-09-29 07:28:00'): nan,\n",
       " Timestamp('2025-09-29 07:29:00'): nan,\n",
       " Timestamp('2025-09-29 07:30:00'): nan,\n",
       " Timestamp('2025-09-29 07:31:00'): nan,\n",
       " Timestamp('2025-09-29 07:32:00'): nan,\n",
       " Timestamp('2025-09-29 07:33:00'): nan,\n",
       " Timestamp('2025-09-29 07:34:00'): nan,\n",
       " Timestamp('2025-09-29 07:35:00'): nan,\n",
       " Timestamp('2025-09-29 07:36:00'): nan,\n",
       " Timestamp('2025-09-29 07:37:00'): nan,\n",
       " Timestamp('2025-09-29 07:38:00'): nan,\n",
       " Timestamp('2025-09-29 07:39:00'): nan,\n",
       " Timestamp('2025-09-29 07:40:00'): nan,\n",
       " Timestamp('2025-09-29 07:41:00'): nan,\n",
       " Timestamp('2025-09-29 07:42:00'): nan,\n",
       " Timestamp('2025-09-29 07:43:00'): nan,\n",
       " Timestamp('2025-09-29 07:44:00'): nan,\n",
       " Timestamp('2025-09-29 07:45:00'): nan,\n",
       " Timestamp('2025-09-29 07:46:00'): nan,\n",
       " Timestamp('2025-09-29 07:47:00'): nan,\n",
       " Timestamp('2025-09-29 07:48:00'): nan,\n",
       " Timestamp('2025-09-29 07:49:00'): nan,\n",
       " Timestamp('2025-09-29 07:50:00'): nan,\n",
       " Timestamp('2025-09-29 07:51:00'): nan,\n",
       " Timestamp('2025-09-29 07:52:00'): nan,\n",
       " Timestamp('2025-09-29 07:53:00'): nan,\n",
       " Timestamp('2025-09-29 07:54:00'): nan,\n",
       " Timestamp('2025-09-29 07:55:00'): nan,\n",
       " Timestamp('2025-09-29 07:56:00'): nan,\n",
       " Timestamp('2025-09-29 07:57:00'): nan,\n",
       " Timestamp('2025-09-29 07:58:00'): nan,\n",
       " Timestamp('2025-09-29 07:59:00'): nan,\n",
       " Timestamp('2025-09-29 08:00:00'): nan,\n",
       " Timestamp('2025-09-29 08:01:00'): nan,\n",
       " Timestamp('2025-09-29 08:02:00'): nan,\n",
       " Timestamp('2025-09-29 08:03:00'): nan,\n",
       " Timestamp('2025-09-29 08:04:00'): nan,\n",
       " Timestamp('2025-09-29 08:05:00'): nan,\n",
       " Timestamp('2025-09-29 08:06:00'): nan,\n",
       " Timestamp('2025-09-29 08:07:00'): nan,\n",
       " Timestamp('2025-09-29 08:08:00'): nan,\n",
       " Timestamp('2025-09-29 08:09:00'): nan,\n",
       " Timestamp('2025-09-29 08:10:00'): nan,\n",
       " Timestamp('2025-09-29 08:11:00'): nan,\n",
       " Timestamp('2025-09-29 08:12:00'): nan,\n",
       " Timestamp('2025-09-29 08:13:00'): nan,\n",
       " Timestamp('2025-09-29 08:14:00'): nan,\n",
       " Timestamp('2025-09-29 08:15:00'): nan,\n",
       " Timestamp('2025-09-29 08:16:00'): nan,\n",
       " Timestamp('2025-09-29 08:17:00'): nan,\n",
       " Timestamp('2025-09-29 08:18:00'): nan,\n",
       " Timestamp('2025-09-29 08:19:00'): nan,\n",
       " Timestamp('2025-09-29 08:20:00'): nan,\n",
       " Timestamp('2025-09-29 08:21:00'): nan,\n",
       " Timestamp('2025-09-29 08:22:00'): nan,\n",
       " Timestamp('2025-09-29 08:23:00'): nan,\n",
       " Timestamp('2025-09-29 08:24:00'): nan,\n",
       " Timestamp('2025-09-29 08:25:00'): nan,\n",
       " Timestamp('2025-09-29 08:26:00'): nan,\n",
       " Timestamp('2025-09-29 08:27:00'): nan,\n",
       " Timestamp('2025-09-29 08:28:00'): nan,\n",
       " Timestamp('2025-09-29 08:29:00'): nan,\n",
       " Timestamp('2025-09-29 08:30:00'): nan,\n",
       " Timestamp('2025-09-29 08:31:00'): nan,\n",
       " Timestamp('2025-09-29 08:32:00'): nan,\n",
       " Timestamp('2025-09-29 08:33:00'): nan,\n",
       " Timestamp('2025-09-29 08:34:00'): nan,\n",
       " Timestamp('2025-09-29 08:35:00'): nan,\n",
       " Timestamp('2025-09-29 08:36:00'): nan,\n",
       " Timestamp('2025-09-29 08:37:00'): nan,\n",
       " Timestamp('2025-09-29 08:38:00'): nan,\n",
       " Timestamp('2025-09-29 08:39:00'): nan,\n",
       " Timestamp('2025-09-29 08:40:00'): nan,\n",
       " Timestamp('2025-09-29 08:41:00'): nan,\n",
       " Timestamp('2025-09-29 08:42:00'): nan,\n",
       " Timestamp('2025-09-29 08:43:00'): nan,\n",
       " Timestamp('2025-09-29 08:44:00'): nan,\n",
       " Timestamp('2025-09-29 08:45:00'): nan,\n",
       " Timestamp('2025-09-29 08:46:00'): nan,\n",
       " Timestamp('2025-09-29 08:47:00'): nan,\n",
       " Timestamp('2025-09-29 08:48:00'): nan,\n",
       " Timestamp('2025-09-29 08:49:00'): nan,\n",
       " Timestamp('2025-09-29 08:50:00'): nan,\n",
       " Timestamp('2025-09-29 08:51:00'): nan,\n",
       " Timestamp('2025-09-29 08:52:00'): nan,\n",
       " Timestamp('2025-09-29 08:53:00'): nan,\n",
       " Timestamp('2025-09-29 08:54:00'): nan,\n",
       " Timestamp('2025-09-29 08:55:00'): nan,\n",
       " Timestamp('2025-09-29 08:56:00'): nan,\n",
       " Timestamp('2025-09-29 08:57:00'): nan,\n",
       " Timestamp('2025-09-29 08:58:00'): nan,\n",
       " Timestamp('2025-09-29 08:59:00'): nan,\n",
       " Timestamp('2025-09-29 09:00:00'): nan,\n",
       " Timestamp('2025-09-29 09:01:00'): nan,\n",
       " Timestamp('2025-09-29 09:02:00'): nan,\n",
       " Timestamp('2025-09-29 09:03:00'): nan,\n",
       " Timestamp('2025-09-29 09:04:00'): nan,\n",
       " Timestamp('2025-09-29 09:05:00'): nan,\n",
       " Timestamp('2025-09-29 09:06:00'): nan,\n",
       " Timestamp('2025-09-29 09:07:00'): nan,\n",
       " Timestamp('2025-09-29 09:08:00'): nan,\n",
       " Timestamp('2025-09-29 09:09:00'): nan,\n",
       " Timestamp('2025-09-29 09:10:00'): nan,\n",
       " Timestamp('2025-09-29 09:11:00'): nan,\n",
       " Timestamp('2025-09-29 09:12:00'): nan,\n",
       " Timestamp('2025-09-29 09:13:00'): nan,\n",
       " Timestamp('2025-09-29 09:14:00'): nan,\n",
       " Timestamp('2025-09-29 09:15:00'): nan,\n",
       " Timestamp('2025-09-29 09:16:00'): nan,\n",
       " Timestamp('2025-09-29 09:17:00'): nan,\n",
       " Timestamp('2025-09-29 09:18:00'): nan,\n",
       " Timestamp('2025-09-29 09:19:00'): nan,\n",
       " Timestamp('2025-09-29 09:20:00'): nan,\n",
       " Timestamp('2025-09-29 09:21:00'): nan,\n",
       " Timestamp('2025-09-29 09:22:00'): nan,\n",
       " Timestamp('2025-09-29 09:23:00'): nan,\n",
       " Timestamp('2025-09-29 09:24:00'): nan,\n",
       " Timestamp('2025-09-29 09:25:00'): nan,\n",
       " Timestamp('2025-09-29 09:26:00'): nan,\n",
       " Timestamp('2025-09-29 09:27:00'): nan,\n",
       " Timestamp('2025-09-29 09:28:00'): nan,\n",
       " Timestamp('2025-09-29 09:29:00'): nan,\n",
       " Timestamp('2025-09-29 09:30:00'): nan,\n",
       " Timestamp('2025-09-29 09:31:00'): nan,\n",
       " Timestamp('2025-09-29 09:32:00'): nan,\n",
       " Timestamp('2025-09-29 09:33:00'): nan,\n",
       " Timestamp('2025-09-29 09:34:00'): nan,\n",
       " Timestamp('2025-09-29 09:35:00'): nan,\n",
       " Timestamp('2025-09-29 09:36:00'): nan,\n",
       " Timestamp('2025-09-29 09:37:00'): nan,\n",
       " Timestamp('2025-09-29 09:38:00'): nan,\n",
       " Timestamp('2025-09-29 09:39:00'): nan,\n",
       " Timestamp('2025-09-29 09:40:00'): nan,\n",
       " Timestamp('2025-09-29 09:41:00'): nan,\n",
       " Timestamp('2025-09-29 09:42:00'): nan,\n",
       " Timestamp('2025-09-29 09:43:00'): nan,\n",
       " Timestamp('2025-09-29 09:44:00'): nan,\n",
       " Timestamp('2025-09-29 09:45:00'): nan,\n",
       " Timestamp('2025-09-29 09:46:00'): nan,\n",
       " Timestamp('2025-09-29 09:47:00'): nan,\n",
       " Timestamp('2025-09-29 09:48:00'): nan,\n",
       " Timestamp('2025-09-29 09:49:00'): nan,\n",
       " Timestamp('2025-09-29 09:50:00'): nan,\n",
       " Timestamp('2025-09-29 09:51:00'): nan,\n",
       " Timestamp('2025-09-29 09:52:00'): nan,\n",
       " Timestamp('2025-09-29 09:53:00'): nan,\n",
       " Timestamp('2025-09-29 09:54:00'): nan,\n",
       " Timestamp('2025-09-29 09:55:00'): nan,\n",
       " Timestamp('2025-09-29 09:56:00'): nan,\n",
       " Timestamp('2025-09-29 09:57:00'): nan,\n",
       " Timestamp('2025-09-29 09:58:00'): nan,\n",
       " Timestamp('2025-09-29 09:59:00'): nan,\n",
       " Timestamp('2025-09-29 10:00:00'): nan,\n",
       " Timestamp('2025-09-29 10:01:00'): nan,\n",
       " Timestamp('2025-09-29 10:02:00'): nan,\n",
       " Timestamp('2025-09-29 10:03:00'): nan,\n",
       " Timestamp('2025-09-29 10:04:00'): nan,\n",
       " Timestamp('2025-09-29 10:05:00'): nan,\n",
       " Timestamp('2025-09-29 10:06:00'): nan,\n",
       " Timestamp('2025-09-29 10:07:00'): nan,\n",
       " Timestamp('2025-09-29 10:08:00'): nan,\n",
       " Timestamp('2025-09-29 10:09:00'): nan,\n",
       " Timestamp('2025-09-29 10:10:00'): nan,\n",
       " Timestamp('2025-09-29 10:11:00'): nan,\n",
       " Timestamp('2025-09-29 10:12:00'): nan,\n",
       " Timestamp('2025-09-29 10:13:00'): nan,\n",
       " Timestamp('2025-09-29 10:14:00'): nan,\n",
       " Timestamp('2025-09-29 10:15:00'): nan,\n",
       " Timestamp('2025-09-29 10:16:00'): nan,\n",
       " Timestamp('2025-09-29 10:17:00'): nan,\n",
       " Timestamp('2025-09-29 10:18:00'): nan,\n",
       " Timestamp('2025-09-29 10:19:00'): nan,\n",
       " Timestamp('2025-09-29 10:20:00'): nan,\n",
       " Timestamp('2025-09-29 10:21:00'): nan,\n",
       " Timestamp('2025-09-29 10:22:00'): nan,\n",
       " Timestamp('2025-09-29 10:23:00'): nan,\n",
       " Timestamp('2025-09-29 10:24:00'): nan,\n",
       " Timestamp('2025-09-29 10:25:00'): nan,\n",
       " Timestamp('2025-09-29 10:26:00'): nan,\n",
       " Timestamp('2025-09-29 10:27:00'): nan,\n",
       " Timestamp('2025-09-29 10:28:00'): nan,\n",
       " Timestamp('2025-09-29 10:29:00'): nan,\n",
       " Timestamp('2025-09-29 10:30:00'): nan,\n",
       " Timestamp('2025-09-29 10:31:00'): nan,\n",
       " Timestamp('2025-09-29 10:32:00'): nan,\n",
       " Timestamp('2025-09-29 10:33:00'): nan,\n",
       " Timestamp('2025-09-29 10:34:00'): nan,\n",
       " Timestamp('2025-09-29 10:35:00'): nan,\n",
       " Timestamp('2025-09-29 10:36:00'): nan,\n",
       " Timestamp('2025-09-29 10:37:00'): nan,\n",
       " Timestamp('2025-09-29 10:38:00'): nan,\n",
       " Timestamp('2025-09-29 10:39:00'): nan,\n",
       " Timestamp('2025-09-29 10:40:00'): nan,\n",
       " Timestamp('2025-09-29 10:41:00'): nan,\n",
       " Timestamp('2025-09-29 10:42:00'): nan,\n",
       " Timestamp('2025-09-29 10:43:00'): nan,\n",
       " Timestamp('2025-09-29 10:44:00'): nan,\n",
       " Timestamp('2025-09-29 10:45:00'): nan,\n",
       " Timestamp('2025-09-29 10:46:00'): nan,\n",
       " Timestamp('2025-09-29 10:47:00'): nan,\n",
       " Timestamp('2025-09-29 10:48:00'): nan,\n",
       " Timestamp('2025-09-29 10:49:00'): nan,\n",
       " Timestamp('2025-09-29 10:50:00'): nan,\n",
       " Timestamp('2025-09-29 10:51:00'): nan,\n",
       " Timestamp('2025-09-29 10:52:00'): nan,\n",
       " Timestamp('2025-09-29 10:53:00'): nan,\n",
       " Timestamp('2025-09-29 10:54:00'): nan,\n",
       " Timestamp('2025-09-29 10:55:00'): nan,\n",
       " Timestamp('2025-09-29 10:56:00'): nan,\n",
       " Timestamp('2025-09-29 10:57:00'): nan,\n",
       " Timestamp('2025-09-29 10:58:00'): nan,\n",
       " Timestamp('2025-09-29 10:59:00'): nan,\n",
       " Timestamp('2025-09-29 11:00:00'): nan,\n",
       " Timestamp('2025-09-29 11:01:00'): nan,\n",
       " Timestamp('2025-09-29 11:02:00'): nan,\n",
       " Timestamp('2025-09-29 11:03:00'): nan,\n",
       " Timestamp('2025-09-29 11:04:00'): nan,\n",
       " Timestamp('2025-09-29 11:05:00'): nan,\n",
       " Timestamp('2025-09-29 11:06:00'): nan,\n",
       " Timestamp('2025-09-29 11:07:00'): nan,\n",
       " Timestamp('2025-09-29 11:08:00'): nan,\n",
       " Timestamp('2025-09-29 11:09:00'): nan,\n",
       " Timestamp('2025-09-29 11:10:00'): nan,\n",
       " Timestamp('2025-09-29 11:11:00'): nan,\n",
       " Timestamp('2025-09-29 11:12:00'): nan,\n",
       " Timestamp('2025-09-29 11:13:00'): nan,\n",
       " Timestamp('2025-09-29 11:14:00'): nan,\n",
       " Timestamp('2025-09-29 11:15:00'): nan,\n",
       " Timestamp('2025-09-29 11:16:00'): nan,\n",
       " Timestamp('2025-09-29 11:17:00'): nan,\n",
       " Timestamp('2025-09-29 11:18:00'): nan,\n",
       " Timestamp('2025-09-29 11:19:00'): nan,\n",
       " Timestamp('2025-09-29 11:20:00'): nan,\n",
       " Timestamp('2025-09-29 11:21:00'): nan,\n",
       " Timestamp('2025-09-29 11:22:00'): nan,\n",
       " Timestamp('2025-09-29 11:23:00'): nan,\n",
       " Timestamp('2025-09-29 11:24:00'): nan,\n",
       " Timestamp('2025-09-29 11:25:00'): nan,\n",
       " Timestamp('2025-09-29 11:26:00'): nan,\n",
       " Timestamp('2025-09-29 11:27:00'): nan,\n",
       " Timestamp('2025-09-29 11:28:00'): nan,\n",
       " Timestamp('2025-09-29 11:29:00'): nan,\n",
       " Timestamp('2025-09-29 11:30:00'): nan,\n",
       " Timestamp('2025-09-29 11:31:00'): nan,\n",
       " Timestamp('2025-09-29 11:32:00'): nan,\n",
       " Timestamp('2025-09-29 11:33:00'): nan,\n",
       " Timestamp('2025-09-29 11:34:00'): nan,\n",
       " Timestamp('2025-09-29 11:35:00'): nan,\n",
       " Timestamp('2025-09-29 11:36:00'): nan,\n",
       " Timestamp('2025-09-29 11:37:00'): nan,\n",
       " Timestamp('2025-09-29 11:38:00'): nan,\n",
       " Timestamp('2025-09-29 11:39:00'): nan,\n",
       " Timestamp('2025-09-29 11:40:00'): nan,\n",
       " Timestamp('2025-09-29 11:41:00'): nan,\n",
       " Timestamp('2025-09-29 11:42:00'): nan,\n",
       " Timestamp('2025-09-29 11:43:00'): nan,\n",
       " Timestamp('2025-09-29 11:44:00'): nan,\n",
       " Timestamp('2025-09-29 11:45:00'): nan,\n",
       " Timestamp('2025-09-29 11:46:00'): nan,\n",
       " Timestamp('2025-09-29 11:47:00'): nan,\n",
       " Timestamp('2025-09-29 11:48:00'): nan,\n",
       " Timestamp('2025-09-29 11:49:00'): nan,\n",
       " Timestamp('2025-09-29 11:50:00'): nan,\n",
       " Timestamp('2025-09-29 11:51:00'): nan,\n",
       " Timestamp('2025-09-29 11:52:00'): nan,\n",
       " Timestamp('2025-09-29 11:53:00'): nan,\n",
       " Timestamp('2025-09-29 11:54:00'): nan,\n",
       " Timestamp('2025-09-29 11:55:00'): nan,\n",
       " Timestamp('2025-09-29 11:56:00'): nan,\n",
       " Timestamp('2025-09-29 11:57:00'): nan,\n",
       " Timestamp('2025-09-29 11:58:00'): nan,\n",
       " Timestamp('2025-09-29 11:59:00'): nan,\n",
       " Timestamp('2025-09-29 12:00:00'): nan,\n",
       " Timestamp('2025-09-29 12:01:00'): nan,\n",
       " Timestamp('2025-09-29 12:02:00'): nan,\n",
       " Timestamp('2025-09-29 12:03:00'): nan,\n",
       " Timestamp('2025-09-29 12:04:00'): nan,\n",
       " Timestamp('2025-09-29 12:05:00'): nan,\n",
       " Timestamp('2025-09-29 12:06:00'): nan,\n",
       " Timestamp('2025-09-29 12:07:00'): nan,\n",
       " Timestamp('2025-09-29 12:08:00'): nan,\n",
       " Timestamp('2025-09-29 12:09:00'): nan,\n",
       " Timestamp('2025-09-29 12:10:00'): nan,\n",
       " Timestamp('2025-09-29 12:11:00'): nan,\n",
       " Timestamp('2025-09-29 12:12:00'): nan,\n",
       " Timestamp('2025-09-29 12:13:00'): nan,\n",
       " Timestamp('2025-09-29 12:14:00'): nan,\n",
       " Timestamp('2025-09-29 12:15:00'): nan,\n",
       " Timestamp('2025-09-29 12:16:00'): nan,\n",
       " Timestamp('2025-09-29 12:17:00'): nan,\n",
       " Timestamp('2025-09-29 12:18:00'): nan,\n",
       " Timestamp('2025-09-29 12:19:00'): nan,\n",
       " Timestamp('2025-09-29 12:20:00'): nan,\n",
       " Timestamp('2025-09-29 12:21:00'): nan,\n",
       " Timestamp('2025-09-29 12:22:00'): nan,\n",
       " Timestamp('2025-09-29 12:23:00'): nan,\n",
       " Timestamp('2025-09-29 12:24:00'): nan,\n",
       " Timestamp('2025-09-29 12:25:00'): nan,\n",
       " Timestamp('2025-09-29 12:26:00'): nan,\n",
       " Timestamp('2025-09-29 12:27:00'): nan,\n",
       " Timestamp('2025-09-29 12:28:00'): nan,\n",
       " Timestamp('2025-09-29 12:29:00'): nan,\n",
       " Timestamp('2025-09-29 12:30:00'): nan,\n",
       " Timestamp('2025-09-29 12:31:00'): nan,\n",
       " Timestamp('2025-09-29 12:32:00'): nan,\n",
       " Timestamp('2025-09-29 12:33:00'): nan,\n",
       " Timestamp('2025-09-29 12:34:00'): nan,\n",
       " Timestamp('2025-09-29 12:35:00'): nan,\n",
       " Timestamp('2025-09-29 12:36:00'): nan,\n",
       " Timestamp('2025-09-29 12:37:00'): nan,\n",
       " Timestamp('2025-09-29 12:38:00'): nan,\n",
       " Timestamp('2025-09-29 12:39:00'): nan,\n",
       " Timestamp('2025-09-29 12:40:00'): nan,\n",
       " Timestamp('2025-09-29 12:41:00'): nan,\n",
       " Timestamp('2025-09-29 12:42:00'): nan,\n",
       " Timestamp('2025-09-29 12:43:00'): nan,\n",
       " Timestamp('2025-09-29 12:44:00'): nan,\n",
       " Timestamp('2025-09-29 12:45:00'): nan,\n",
       " Timestamp('2025-09-29 12:46:00'): nan,\n",
       " Timestamp('2025-09-29 12:47:00'): nan,\n",
       " Timestamp('2025-09-29 12:48:00'): nan,\n",
       " Timestamp('2025-09-29 12:49:00'): nan,\n",
       " Timestamp('2025-09-29 12:50:00'): nan,\n",
       " Timestamp('2025-09-29 12:51:00'): nan,\n",
       " Timestamp('2025-09-29 12:52:00'): nan,\n",
       " Timestamp('2025-09-29 12:53:00'): nan,\n",
       " Timestamp('2025-09-29 12:54:00'): nan,\n",
       " Timestamp('2025-09-29 12:55:00'): nan,\n",
       " Timestamp('2025-09-29 12:56:00'): nan,\n",
       " Timestamp('2025-09-29 12:57:00'): nan,\n",
       " Timestamp('2025-09-29 12:58:00'): nan,\n",
       " Timestamp('2025-09-29 12:59:00'): nan,\n",
       " Timestamp('2025-09-29 13:00:00'): nan,\n",
       " Timestamp('2025-09-29 13:01:00'): nan,\n",
       " Timestamp('2025-09-29 13:02:00'): nan,\n",
       " Timestamp('2025-09-29 13:03:00'): nan,\n",
       " Timestamp('2025-09-29 13:04:00'): nan,\n",
       " Timestamp('2025-09-29 13:05:00'): nan,\n",
       " Timestamp('2025-09-29 13:06:00'): nan,\n",
       " Timestamp('2025-09-29 13:07:00'): nan,\n",
       " Timestamp('2025-09-29 13:08:00'): nan,\n",
       " Timestamp('2025-09-29 13:09:00'): nan,\n",
       " Timestamp('2025-09-29 13:10:00'): nan,\n",
       " Timestamp('2025-09-29 13:11:00'): nan,\n",
       " Timestamp('2025-09-29 13:12:00'): nan,\n",
       " Timestamp('2025-09-29 13:13:00'): nan,\n",
       " Timestamp('2025-09-29 13:14:00'): nan,\n",
       " Timestamp('2025-09-29 13:15:00'): nan,\n",
       " Timestamp('2025-09-29 13:16:00'): nan,\n",
       " Timestamp('2025-09-29 13:17:00'): nan,\n",
       " Timestamp('2025-09-29 13:18:00'): nan,\n",
       " Timestamp('2025-09-29 13:19:00'): nan,\n",
       " Timestamp('2025-09-29 13:20:00'): nan,\n",
       " Timestamp('2025-09-29 13:21:00'): nan,\n",
       " Timestamp('2025-09-29 13:22:00'): nan,\n",
       " Timestamp('2025-09-29 13:23:00'): nan,\n",
       " Timestamp('2025-09-29 13:24:00'): nan,\n",
       " Timestamp('2025-09-29 13:25:00'): nan,\n",
       " Timestamp('2025-09-29 13:26:00'): nan,\n",
       " Timestamp('2025-09-29 13:27:00'): nan,\n",
       " Timestamp('2025-09-29 13:28:00'): nan,\n",
       " Timestamp('2025-09-29 13:29:00'): nan,\n",
       " Timestamp('2025-09-29 13:30:00'): nan,\n",
       " Timestamp('2025-09-29 13:31:00'): nan,\n",
       " Timestamp('2025-09-29 13:32:00'): nan,\n",
       " Timestamp('2025-09-29 13:33:00'): nan,\n",
       " Timestamp('2025-09-29 13:34:00'): nan,\n",
       " Timestamp('2025-09-29 13:35:00'): nan,\n",
       " Timestamp('2025-09-29 13:36:00'): nan,\n",
       " Timestamp('2025-09-29 13:37:00'): nan,\n",
       " Timestamp('2025-09-29 13:38:00'): nan,\n",
       " Timestamp('2025-09-29 13:39:00'): nan,\n",
       " Timestamp('2025-09-29 13:40:00'): nan,\n",
       " Timestamp('2025-09-29 13:41:00'): nan,\n",
       " Timestamp('2025-09-29 13:42:00'): nan,\n",
       " Timestamp('2025-09-29 13:43:00'): nan,\n",
       " Timestamp('2025-09-29 13:44:00'): nan,\n",
       " Timestamp('2025-09-29 13:45:00'): nan,\n",
       " Timestamp('2025-09-29 13:46:00'): nan,\n",
       " Timestamp('2025-09-29 13:47:00'): nan,\n",
       " Timestamp('2025-09-29 13:48:00'): nan,\n",
       " Timestamp('2025-09-29 13:49:00'): nan,\n",
       " Timestamp('2025-09-29 13:50:00'): nan,\n",
       " Timestamp('2025-09-29 13:51:00'): nan,\n",
       " Timestamp('2025-09-29 13:52:00'): nan,\n",
       " Timestamp('2025-09-29 13:53:00'): nan,\n",
       " Timestamp('2025-09-29 13:54:00'): nan,\n",
       " Timestamp('2025-09-29 13:55:00'): nan,\n",
       " Timestamp('2025-09-29 13:56:00'): nan,\n",
       " Timestamp('2025-09-29 13:57:00'): nan,\n",
       " Timestamp('2025-09-29 13:58:00'): nan,\n",
       " Timestamp('2025-09-29 13:59:00'): nan,\n",
       " Timestamp('2025-09-29 14:00:00'): nan,\n",
       " Timestamp('2025-09-29 14:01:00'): nan,\n",
       " Timestamp('2025-09-29 14:02:00'): nan,\n",
       " Timestamp('2025-09-29 14:03:00'): nan,\n",
       " Timestamp('2025-09-29 14:04:00'): nan,\n",
       " Timestamp('2025-09-29 14:05:00'): nan,\n",
       " Timestamp('2025-09-29 14:06:00'): nan,\n",
       " Timestamp('2025-09-29 14:07:00'): nan,\n",
       " Timestamp('2025-09-29 14:08:00'): nan,\n",
       " Timestamp('2025-09-29 14:09:00'): nan,\n",
       " Timestamp('2025-09-29 14:10:00'): nan,\n",
       " Timestamp('2025-09-29 14:11:00'): nan,\n",
       " Timestamp('2025-09-29 14:12:00'): nan,\n",
       " Timestamp('2025-09-29 14:13:00'): nan,\n",
       " Timestamp('2025-09-29 14:14:00'): nan,\n",
       " Timestamp('2025-09-29 14:15:00'): nan,\n",
       " Timestamp('2025-09-29 14:16:00'): nan,\n",
       " Timestamp('2025-09-29 14:17:00'): nan,\n",
       " Timestamp('2025-09-29 14:18:00'): nan,\n",
       " Timestamp('2025-09-29 14:19:00'): nan,\n",
       " Timestamp('2025-09-29 14:20:00'): nan,\n",
       " Timestamp('2025-09-29 14:21:00'): nan,\n",
       " Timestamp('2025-09-29 14:22:00'): nan,\n",
       " Timestamp('2025-09-29 14:23:00'): nan,\n",
       " Timestamp('2025-09-29 14:24:00'): nan,\n",
       " Timestamp('2025-09-29 14:25:00'): nan,\n",
       " Timestamp('2025-09-29 14:26:00'): nan,\n",
       " Timestamp('2025-09-29 14:27:00'): nan,\n",
       " Timestamp('2025-09-29 14:28:00'): nan,\n",
       " Timestamp('2025-09-29 14:29:00'): nan,\n",
       " Timestamp('2025-09-29 14:30:00'): nan,\n",
       " Timestamp('2025-09-29 14:31:00'): nan,\n",
       " Timestamp('2025-09-29 14:32:00'): nan,\n",
       " Timestamp('2025-09-29 14:33:00'): nan,\n",
       " Timestamp('2025-09-29 14:34:00'): nan,\n",
       " Timestamp('2025-09-29 14:35:00'): nan,\n",
       " Timestamp('2025-09-29 14:36:00'): nan,\n",
       " Timestamp('2025-09-29 14:37:00'): nan,\n",
       " Timestamp('2025-09-29 14:38:00'): nan,\n",
       " Timestamp('2025-09-29 14:39:00'): nan,\n",
       " Timestamp('2025-09-29 14:40:00'): nan,\n",
       " Timestamp('2025-09-29 14:41:00'): nan,\n",
       " Timestamp('2025-09-29 14:42:00'): nan,\n",
       " Timestamp('2025-09-29 14:43:00'): nan,\n",
       " Timestamp('2025-09-29 14:44:00'): nan,\n",
       " Timestamp('2025-09-29 14:45:00'): nan,\n",
       " Timestamp('2025-09-29 14:46:00'): nan,\n",
       " Timestamp('2025-09-29 14:47:00'): nan,\n",
       " Timestamp('2025-09-29 14:48:00'): nan,\n",
       " Timestamp('2025-09-29 14:49:00'): nan,\n",
       " Timestamp('2025-09-29 14:50:00'): nan,\n",
       " Timestamp('2025-09-29 14:51:00'): nan,\n",
       " Timestamp('2025-09-29 14:52:00'): nan,\n",
       " Timestamp('2025-09-29 14:53:00'): nan,\n",
       " Timestamp('2025-09-29 14:54:00'): nan,\n",
       " Timestamp('2025-09-29 14:55:00'): nan,\n",
       " Timestamp('2025-09-29 14:56:00'): nan,\n",
       " Timestamp('2025-09-29 14:57:00'): nan,\n",
       " Timestamp('2025-09-29 14:58:00'): nan,\n",
       " Timestamp('2025-09-29 14:59:00'): nan,\n",
       " Timestamp('2025-09-29 15:00:00'): nan,\n",
       " Timestamp('2025-09-29 15:01:00'): nan,\n",
       " Timestamp('2025-09-29 15:02:00'): nan,\n",
       " Timestamp('2025-09-29 15:03:00'): nan,\n",
       " Timestamp('2025-09-29 15:04:00'): nan,\n",
       " Timestamp('2025-09-29 15:05:00'): nan,\n",
       " Timestamp('2025-09-29 15:06:00'): nan,\n",
       " Timestamp('2025-09-29 15:07:00'): nan,\n",
       " Timestamp('2025-09-29 15:08:00'): nan,\n",
       " Timestamp('2025-09-29 15:09:00'): nan,\n",
       " Timestamp('2025-09-29 15:10:00'): nan,\n",
       " Timestamp('2025-09-29 15:11:00'): nan,\n",
       " Timestamp('2025-09-29 15:12:00'): nan,\n",
       " Timestamp('2025-09-29 15:13:00'): nan,\n",
       " Timestamp('2025-09-29 15:14:00'): nan,\n",
       " Timestamp('2025-09-29 15:15:00'): nan,\n",
       " Timestamp('2025-09-29 15:16:00'): nan,\n",
       " Timestamp('2025-09-29 15:17:00'): nan,\n",
       " Timestamp('2025-09-29 15:18:00'): nan,\n",
       " Timestamp('2025-09-29 15:19:00'): nan,\n",
       " Timestamp('2025-09-29 15:20:00'): nan,\n",
       " Timestamp('2025-09-29 15:21:00'): nan,\n",
       " Timestamp('2025-09-29 15:22:00'): nan,\n",
       " Timestamp('2025-09-29 15:23:00'): nan,\n",
       " Timestamp('2025-09-29 15:24:00'): nan,\n",
       " Timestamp('2025-09-29 15:25:00'): nan,\n",
       " Timestamp('2025-09-29 15:26:00'): nan,\n",
       " Timestamp('2025-09-29 15:27:00'): nan,\n",
       " Timestamp('2025-09-29 15:28:00'): nan,\n",
       " Timestamp('2025-09-29 15:29:00'): nan,\n",
       " Timestamp('2025-09-29 15:30:00'): nan,\n",
       " Timestamp('2025-09-29 15:31:00'): nan,\n",
       " Timestamp('2025-09-29 15:32:00'): nan,\n",
       " Timestamp('2025-09-29 15:33:00'): nan,\n",
       " Timestamp('2025-09-29 15:34:00'): nan,\n",
       " Timestamp('2025-09-29 15:35:00'): nan,\n",
       " Timestamp('2025-09-29 15:36:00'): nan,\n",
       " Timestamp('2025-09-29 15:37:00'): nan,\n",
       " Timestamp('2025-09-29 15:38:00'): nan,\n",
       " Timestamp('2025-09-29 15:39:00'): nan,\n",
       " Timestamp('2025-09-29 15:40:00'): nan,\n",
       " Timestamp('2025-09-29 15:41:00'): nan,\n",
       " Timestamp('2025-09-29 15:42:00'): nan,\n",
       " Timestamp('2025-09-29 15:43:00'): nan,\n",
       " Timestamp('2025-09-29 15:44:00'): nan,\n",
       " Timestamp('2025-09-29 15:45:00'): nan,\n",
       " Timestamp('2025-09-29 15:46:00'): nan,\n",
       " Timestamp('2025-09-29 15:47:00'): nan,\n",
       " Timestamp('2025-09-29 15:48:00'): nan,\n",
       " Timestamp('2025-09-29 15:49:00'): nan,\n",
       " Timestamp('2025-09-29 15:50:00'): nan,\n",
       " Timestamp('2025-09-29 15:51:00'): nan,\n",
       " Timestamp('2025-09-29 15:52:00'): nan,\n",
       " Timestamp('2025-09-29 15:53:00'): nan,\n",
       " Timestamp('2025-09-29 15:54:00'): nan,\n",
       " Timestamp('2025-09-29 15:55:00'): nan,\n",
       " Timestamp('2025-09-29 15:56:00'): nan,\n",
       " Timestamp('2025-09-29 15:57:00'): nan,\n",
       " Timestamp('2025-09-29 15:58:00'): nan,\n",
       " Timestamp('2025-09-29 15:59:00'): nan,\n",
       " Timestamp('2025-09-29 16:00:00'): nan,\n",
       " Timestamp('2025-09-29 16:01:00'): nan,\n",
       " Timestamp('2025-09-29 16:02:00'): nan,\n",
       " Timestamp('2025-09-29 16:03:00'): nan,\n",
       " Timestamp('2025-09-29 16:04:00'): nan,\n",
       " Timestamp('2025-09-29 16:05:00'): nan,\n",
       " Timestamp('2025-09-29 16:06:00'): nan,\n",
       " Timestamp('2025-09-29 16:07:00'): nan,\n",
       " Timestamp('2025-09-29 16:08:00'): nan,\n",
       " Timestamp('2025-09-29 16:09:00'): nan,\n",
       " Timestamp('2025-09-29 16:10:00'): nan,\n",
       " Timestamp('2025-09-29 16:11:00'): nan,\n",
       " Timestamp('2025-09-29 16:12:00'): nan,\n",
       " Timestamp('2025-09-29 16:13:00'): nan,\n",
       " Timestamp('2025-09-29 16:14:00'): nan,\n",
       " Timestamp('2025-09-29 16:15:00'): nan,\n",
       " Timestamp('2025-09-29 16:16:00'): nan,\n",
       " Timestamp('2025-09-29 16:17:00'): nan,\n",
       " Timestamp('2025-09-29 16:18:00'): nan,\n",
       " Timestamp('2025-09-29 16:19:00'): nan,\n",
       " Timestamp('2025-09-29 16:20:00'): nan,\n",
       " Timestamp('2025-09-29 16:21:00'): nan,\n",
       " Timestamp('2025-09-29 16:22:00'): nan,\n",
       " Timestamp('2025-09-29 16:23:00'): nan,\n",
       " Timestamp('2025-09-29 16:24:00'): nan,\n",
       " Timestamp('2025-09-29 16:25:00'): nan,\n",
       " Timestamp('2025-09-29 16:26:00'): nan,\n",
       " Timestamp('2025-09-29 16:27:00'): nan,\n",
       " Timestamp('2025-09-29 16:28:00'): nan,\n",
       " Timestamp('2025-09-29 16:29:00'): nan,\n",
       " Timestamp('2025-09-29 16:30:00'): nan,\n",
       " Timestamp('2025-09-29 16:31:00'): nan,\n",
       " Timestamp('2025-09-29 16:32:00'): nan,\n",
       " Timestamp('2025-09-29 16:33:00'): nan,\n",
       " Timestamp('2025-09-29 16:34:00'): nan,\n",
       " Timestamp('2025-09-29 16:35:00'): nan,\n",
       " Timestamp('2025-09-29 16:36:00'): nan,\n",
       " Timestamp('2025-09-29 16:37:00'): nan,\n",
       " Timestamp('2025-09-29 16:38:00'): nan,\n",
       " Timestamp('2025-09-29 16:39:00'): nan,\n",
       " Timestamp('2025-09-29 16:40:00'): nan,\n",
       " Timestamp('2025-09-29 16:41:00'): nan,\n",
       " Timestamp('2025-09-29 16:42:00'): nan,\n",
       " Timestamp('2025-09-29 16:43:00'): nan,\n",
       " Timestamp('2025-09-29 16:44:00'): nan,\n",
       " Timestamp('2025-09-29 16:45:00'): nan,\n",
       " Timestamp('2025-09-29 16:46:00'): nan,\n",
       " Timestamp('2025-09-29 16:47:00'): nan,\n",
       " Timestamp('2025-09-29 16:48:00'): nan,\n",
       " Timestamp('2025-09-29 16:49:00'): nan,\n",
       " Timestamp('2025-09-29 16:50:00'): nan,\n",
       " Timestamp('2025-09-29 16:51:00'): nan,\n",
       " Timestamp('2025-09-29 16:52:00'): nan,\n",
       " Timestamp('2025-09-29 16:53:00'): nan,\n",
       " Timestamp('2025-09-29 16:54:00'): nan,\n",
       " Timestamp('2025-09-29 16:55:00'): nan,\n",
       " Timestamp('2025-09-29 16:56:00'): nan,\n",
       " Timestamp('2025-09-29 16:57:00'): nan,\n",
       " Timestamp('2025-09-29 16:58:00'): nan,\n",
       " Timestamp('2025-09-29 16:59:00'): nan,\n",
       " Timestamp('2025-09-26 03:00:00'): nan,\n",
       " Timestamp('2025-09-26 03:01:00'): nan,\n",
       " Timestamp('2025-09-26 03:02:00'): nan,\n",
       " Timestamp('2025-09-26 03:03:00'): nan,\n",
       " Timestamp('2025-09-26 03:04:00'): nan,\n",
       " Timestamp('2025-09-26 03:05:00'): nan,\n",
       " Timestamp('2025-09-26 03:06:00'): nan,\n",
       " Timestamp('2025-09-26 03:07:00'): nan,\n",
       " Timestamp('2025-09-26 03:08:00'): nan,\n",
       " Timestamp('2025-09-26 03:09:00'): nan,\n",
       " Timestamp('2025-09-26 03:10:00'): nan,\n",
       " Timestamp('2025-09-26 03:11:00'): nan,\n",
       " Timestamp('2025-09-26 03:12:00'): nan,\n",
       " Timestamp('2025-09-26 03:13:00'): nan,\n",
       " Timestamp('2025-09-26 03:14:00'): nan,\n",
       " Timestamp('2025-09-26 03:15:00'): nan,\n",
       " Timestamp('2025-09-26 03:16:00'): nan,\n",
       " Timestamp('2025-09-26 03:17:00'): nan,\n",
       " Timestamp('2025-09-26 03:18:00'): nan,\n",
       " Timestamp('2025-09-26 03:19:00'): nan,\n",
       " Timestamp('2025-09-26 03:20:00'): nan,\n",
       " Timestamp('2025-09-26 03:21:00'): nan,\n",
       " Timestamp('2025-09-26 03:22:00'): nan,\n",
       " Timestamp('2025-09-26 03:23:00'): nan,\n",
       " Timestamp('2025-09-26 03:24:00'): nan,\n",
       " Timestamp('2025-09-26 03:25:00'): nan,\n",
       " Timestamp('2025-09-26 03:26:00'): nan,\n",
       " Timestamp('2025-09-26 03:27:00'): nan,\n",
       " Timestamp('2025-09-26 03:28:00'): nan,\n",
       " Timestamp('2025-09-26 03:29:00'): nan,\n",
       " Timestamp('2025-09-26 03:30:00'): nan,\n",
       " Timestamp('2025-09-26 03:31:00'): nan,\n",
       " Timestamp('2025-09-26 03:32:00'): nan,\n",
       " Timestamp('2025-09-26 03:33:00'): nan,\n",
       " Timestamp('2025-09-26 03:34:00'): nan,\n",
       " Timestamp('2025-09-26 03:35:00'): nan,\n",
       " Timestamp('2025-09-26 03:36:00'): nan,\n",
       " Timestamp('2025-09-26 03:37:00'): nan,\n",
       " Timestamp('2025-09-26 03:38:00'): nan,\n",
       " Timestamp('2025-09-26 03:39:00'): nan,\n",
       " Timestamp('2025-09-26 03:40:00'): nan,\n",
       " Timestamp('2025-09-26 03:41:00'): nan,\n",
       " Timestamp('2025-09-26 03:42:00'): nan,\n",
       " Timestamp('2025-09-26 03:43:00'): nan,\n",
       " Timestamp('2025-09-26 03:44:00'): nan,\n",
       " Timestamp('2025-09-26 03:45:00'): nan,\n",
       " Timestamp('2025-09-26 03:46:00'): nan,\n",
       " Timestamp('2025-09-26 03:47:00'): nan,\n",
       " Timestamp('2025-09-26 03:48:00'): nan,\n",
       " Timestamp('2025-09-26 03:49:00'): nan,\n",
       " Timestamp('2025-09-26 03:50:00'): nan,\n",
       " Timestamp('2025-09-26 03:51:00'): nan,\n",
       " Timestamp('2025-09-26 03:52:00'): nan,\n",
       " Timestamp('2025-09-26 03:53:00'): nan,\n",
       " Timestamp('2025-09-26 03:54:00'): nan,\n",
       " Timestamp('2025-09-26 03:55:00'): nan,\n",
       " Timestamp('2025-09-26 03:56:00'): nan,\n",
       " Timestamp('2025-09-26 03:57:00'): nan,\n",
       " Timestamp('2025-09-26 03:58:00'): nan,\n",
       " Timestamp('2025-09-26 03:59:00'): nan,\n",
       " Timestamp('2025-09-26 04:00:00'): nan,\n",
       " Timestamp('2025-09-26 04:01:00'): nan,\n",
       " Timestamp('2025-09-26 04:02:00'): nan,\n",
       " Timestamp('2025-09-26 04:03:00'): nan,\n",
       " Timestamp('2025-09-26 04:04:00'): nan,\n",
       " Timestamp('2025-09-26 04:05:00'): nan,\n",
       " Timestamp('2025-09-26 04:06:00'): nan,\n",
       " Timestamp('2025-09-26 04:07:00'): nan,\n",
       " Timestamp('2025-09-26 04:08:00'): nan,\n",
       " Timestamp('2025-09-26 04:09:00'): nan,\n",
       " Timestamp('2025-09-26 04:10:00'): nan,\n",
       " Timestamp('2025-09-26 04:11:00'): nan,\n",
       " Timestamp('2025-09-26 04:12:00'): nan,\n",
       " Timestamp('2025-09-26 04:13:00'): nan,\n",
       " Timestamp('2025-09-26 04:14:00'): nan,\n",
       " Timestamp('2025-09-26 04:15:00'): nan,\n",
       " Timestamp('2025-09-26 04:16:00'): nan,\n",
       " Timestamp('2025-09-26 04:17:00'): nan,\n",
       " Timestamp('2025-09-26 04:18:00'): nan,\n",
       " Timestamp('2025-09-26 04:19:00'): nan,\n",
       " Timestamp('2025-09-26 04:20:00'): nan,\n",
       " Timestamp('2025-09-26 04:21:00'): nan,\n",
       " Timestamp('2025-09-26 04:22:00'): nan,\n",
       " Timestamp('2025-09-26 04:23:00'): nan,\n",
       " Timestamp('2025-09-26 04:24:00'): nan,\n",
       " Timestamp('2025-09-26 04:25:00'): nan,\n",
       " Timestamp('2025-09-26 04:26:00'): nan,\n",
       " Timestamp('2025-09-26 04:27:00'): nan,\n",
       " Timestamp('2025-09-26 04:28:00'): nan,\n",
       " Timestamp('2025-09-26 04:29:00'): nan,\n",
       " Timestamp('2025-09-26 04:30:00'): nan,\n",
       " Timestamp('2025-09-26 04:31:00'): nan,\n",
       " Timestamp('2025-09-26 04:32:00'): nan,\n",
       " Timestamp('2025-09-26 04:33:00'): nan,\n",
       " Timestamp('2025-09-26 04:34:00'): nan,\n",
       " Timestamp('2025-09-26 04:35:00'): nan,\n",
       " Timestamp('2025-09-26 04:36:00'): nan,\n",
       " Timestamp('2025-09-26 04:37:00'): nan,\n",
       " Timestamp('2025-09-26 04:38:00'): nan,\n",
       " Timestamp('2025-09-26 04:39:00'): nan,\n",
       " Timestamp('2025-09-26 04:40:00'): nan,\n",
       " Timestamp('2025-09-26 04:41:00'): nan,\n",
       " Timestamp('2025-09-26 04:42:00'): nan,\n",
       " Timestamp('2025-09-26 04:43:00'): nan,\n",
       " Timestamp('2025-09-26 04:44:00'): nan,\n",
       " Timestamp('2025-09-26 04:45:00'): nan,\n",
       " Timestamp('2025-09-26 04:46:00'): nan,\n",
       " Timestamp('2025-09-26 04:47:00'): nan,\n",
       " Timestamp('2025-09-26 04:48:00'): nan,\n",
       " Timestamp('2025-09-26 04:49:00'): nan,\n",
       " Timestamp('2025-09-26 04:50:00'): nan,\n",
       " Timestamp('2025-09-26 04:51:00'): nan,\n",
       " Timestamp('2025-09-26 04:52:00'): nan,\n",
       " Timestamp('2025-09-26 04:53:00'): nan,\n",
       " Timestamp('2025-09-26 04:54:00'): nan,\n",
       " Timestamp('2025-09-26 04:55:00'): nan,\n",
       " Timestamp('2025-09-26 04:56:00'): nan,\n",
       " Timestamp('2025-09-26 04:57:00'): nan,\n",
       " Timestamp('2025-09-26 04:58:00'): nan,\n",
       " Timestamp('2025-09-26 04:59:00'): nan,\n",
       " Timestamp('2025-09-26 05:00:00'): nan,\n",
       " Timestamp('2025-09-26 05:01:00'): nan,\n",
       " Timestamp('2025-09-26 05:02:00'): nan,\n",
       " Timestamp('2025-09-26 05:03:00'): nan,\n",
       " Timestamp('2025-09-26 05:04:00'): nan,\n",
       " Timestamp('2025-09-26 05:05:00'): nan,\n",
       " Timestamp('2025-09-26 05:06:00'): nan,\n",
       " Timestamp('2025-09-26 05:07:00'): nan,\n",
       " Timestamp('2025-09-26 05:08:00'): nan,\n",
       " Timestamp('2025-09-26 05:09:00'): nan,\n",
       " Timestamp('2025-09-26 05:10:00'): nan,\n",
       " Timestamp('2025-09-26 05:11:00'): nan,\n",
       " Timestamp('2025-09-26 05:12:00'): nan,\n",
       " Timestamp('2025-09-26 05:13:00'): nan,\n",
       " Timestamp('2025-09-26 05:14:00'): nan,\n",
       " Timestamp('2025-09-26 05:15:00'): nan,\n",
       " Timestamp('2025-09-26 05:16:00'): nan,\n",
       " Timestamp('2025-09-26 05:17:00'): nan,\n",
       " Timestamp('2025-09-26 05:18:00'): nan,\n",
       " Timestamp('2025-09-26 05:19:00'): nan,\n",
       " Timestamp('2025-09-26 05:20:00'): nan,\n",
       " Timestamp('2025-09-26 05:21:00'): nan,\n",
       " Timestamp('2025-09-26 05:22:00'): nan,\n",
       " Timestamp('2025-09-26 05:23:00'): nan,\n",
       " Timestamp('2025-09-26 05:24:00'): nan,\n",
       " Timestamp('2025-09-26 05:25:00'): nan,\n",
       " Timestamp('2025-09-26 05:26:00'): nan,\n",
       " Timestamp('2025-09-26 05:27:00'): nan,\n",
       " Timestamp('2025-09-26 05:28:00'): nan,\n",
       " Timestamp('2025-09-26 05:29:00'): nan,\n",
       " Timestamp('2025-09-26 05:30:00'): nan,\n",
       " Timestamp('2025-09-26 05:31:00'): nan,\n",
       " Timestamp('2025-09-26 05:32:00'): nan,\n",
       " Timestamp('2025-09-26 05:33:00'): nan,\n",
       " Timestamp('2025-09-26 05:34:00'): nan,\n",
       " Timestamp('2025-09-26 05:35:00'): nan,\n",
       " Timestamp('2025-09-26 05:36:00'): nan,\n",
       " Timestamp('2025-09-26 05:37:00'): nan,\n",
       " Timestamp('2025-09-26 05:38:00'): nan,\n",
       " Timestamp('2025-09-26 05:39:00'): nan,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c0322-d18a-44e1-afab-059871d6552d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd0fe5-7474-487a-a555-d420f92a8720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e80441c-4e11-464a-9405-aa9306c5a75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e88dc18-fb8f-4b5c-942a-22eb26ce8d8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #ER Code\n",
    "# ####################################################\n",
    "\n",
    "# all_start_date = str((datetime.now()-timedelta(days=13*365+1)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "# labels = {\n",
    "#         \"LQD Funding Sprd\": \"DB(CDS,TRAC-X,EUROPEIBXTRS_USDLIG_3M,JPM_IMPLIEDFUNDING_MID)\",\n",
    "#         \"HYG Funding Sprd\": \"DB(CDS,TRAC-X,EUROPEIBXTRS_USDHY_3M,JPM_IMPLIEDFUNDING_MID)\",\n",
    "#         \"IEAC Funding Sprd\": \"DB(CDS,TRAC-X,EUROPEIBXTRS_EURIG_3M,JPM_IMPLIEDFUNDING_MID)\",\n",
    "#         \"IHYG Funding Sprd\": \"DB(CDS,TRAC-X,EUROPEIBXTRS_EURHY_3M,JPM_IMPLIEDFUNDING_MID)\",\n",
    "#         \"Fed Fund\": \"FF\",\n",
    "#         \"ER CDX IG 5Y\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_RETURN)\",\n",
    "#         \"ER CDX IG 10Y\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_RETURN)\",\n",
    "#         \"ER CDX HY 5Y\": \"DB(CDS,TRAC-X,NAHY100UNF05ONRUN,JPM_RETURN)\",\n",
    "#         \"ER CDX HY 10Y\": \"DB(CDS,TRAC-X,NAHY100UNF10ONRUN,JPM_RETURN)\",\n",
    "#         \"ER CDX EM 5Y\": \"DB(NEO-UK,credit/cds/index/cdxEM-onrun/5y/JPM_RETURN)\",\n",
    "#         \"ER ITRX MAIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "#         \"ER ITRX MAIN 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_UNFUNDED_INDEX)\",\n",
    "#         \"ER ITRX XOVER 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "#         \"ER ITRX SNRFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "#         \"ER ITRX SUBFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "# }\n",
    "\n",
    "# dq = DataQuery(\n",
    "# client_id='jbAIMF2Tkp0JO3sc',\n",
    "# client_secret='d7qfzgt55pddjs352sgxosFyI4t2eja07k7opbi6wg9oqjc1OjkdAksn1btmnugeMjchcx2vwTsJupw',\n",
    "# )\n",
    "\n",
    "# job = dq.create_job(expressions = list(labels.values()))\n",
    "# dq.start_date = all_start_date\n",
    "# var = job.execute()\n",
    "# df = job.to_pivot_table()\n",
    "# df = df.T\n",
    "# df.index = pd.to_datetime(df.index, format='%Y%m%d').date\n",
    "# df.index.name = 'Date'\n",
    "\n",
    "# df.rename(columns={v:k for k, v in labels.items()},inplace=True)\n",
    "# df.columns.name = None\n",
    "# clear_output(wait=False)\n",
    "# df = df.dropna(how='all')\n",
    "# df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# dq = df.copy()\n",
    "\n",
    "# end_date = dq.index[-1]\n",
    "# ####################################### BBG Data Acquisition\n",
    "\n",
    "# securities = ['HYG US Equity','EMB US Equity','LQD US Equity','VCIT US Equity',\n",
    "#               'IEAC LN Equity','IHYG LN EQUITY', 'BKLN US EQUITY', 'IBCN GR EQUITY',\n",
    "#               'IEI US Equity','IEF US Equity']\n",
    "\n",
    "# fields1 = ['YAS_MOD_DUR']\n",
    "# df = blp.bdh(tickers=securities, start_date = all_start_date, end_date = end_date, flds = fields1)\n",
    "# df.columns = [item.split(' ')[0] + ' DUR' for item in securities]\n",
    "# df1 = df.copy()\n",
    "\n",
    "# #################################### Fixing Bad Data Point in YAS of IEI\n",
    "# rolling_avg = df1['IEI DUR'].replace(0, np.nan).rolling(window=30, min_periods=1).mean()\n",
    "# df1['IEI DUR'] = df1.apply(\n",
    "#     lambda row: rolling_avg[row.name] if row['IEI DUR'] == 0.0 else row['IEI DUR'], axis=1\n",
    "# )\n",
    "# #################################### Fixing Bad Data Point in YAS of IEI\n",
    "\n",
    "# securities = ['LT03TRUU INDEX','LT09TRUU INDEX','QW3I INDEX', 'LT03MD INDEX','LT09MD INDEX']\n",
    "# fields = ['PX_LAST']\n",
    "# df = blp.bdh(tickers=securities, start_date = all_start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = ['TR ' + item.split(' ')[0] for item in securities[:3]] + [item.split(' ')[0] + ' DUR' for item in securities[:2]]\n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# securities = ['HYG US Equity','EMB US Equity','LQD US Equity','VCIT US Equity',\n",
    "#               'IEI US Equity','IEF US Equity', 'RSP US EQUITY', #'SPX INDEX',  'RTY INDEX',\n",
    "#               'IBCN GR EQUITY',\n",
    "#               'IEAC LN Equity','IHYG LN EQUITY', 'BKLN US EQUITY', 'IWM US EQUITY',\n",
    "#               'GSCBHYEQ Index', 'GSCBIGEQ Index', 'SPY US EQUITY', 'EEM US EQUITY','IJH US EQUITY'\n",
    "#              ]\n",
    "\n",
    "# fields = ['TOT_RETURN_INDEX_GROSS_DVDS']\n",
    "# df = blp.bdh(tickers=securities, start_date = all_start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = ['TR ' + item.split(' ')[0] for item in securities] \n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# securities = ['QW3I INDEX']\n",
    "# fields = ['MODIFIED_DURATION']\n",
    "# df = blp.bdh(tickers=securities, start_date = all_start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = [item.split(' ')[0] + ' DUR' for item in securities]\n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# # securities = ['SPXFP INDEX', 'RTYFPE INDEX','SX5EFSER Index']  ############## I want to calculate funding rate for spx, rty and sx5e separately\n",
    "# # fields = ['PX_LAST']\n",
    "# # df = blp.bdh(tickers=securities, start_date = all_start_date, end_date = end_date, flds = fields)\n",
    "# # df.columns = ['ER SPX','ER RTY','ER SX5E']\n",
    "# # df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# securities = ['EURR002W Index']\n",
    "# fields = ['PX_LAST']\n",
    "# df = blp.bdh(tickers=securities, start_date = all_start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = ['ECB Rate']\n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# bbg = df1.copy()\n",
    "# dq.index = pd.to_datetime(dq.index)\n",
    "# dq.index = dq.index.date\n",
    "# bbg.index = pd.to_datetime(bbg.index)\n",
    "# bbg.index = bbg.index.date\n",
    "\n",
    "# data = pd.concat([dq,bbg],axis=1)\n",
    "# data = data.sort_index()\n",
    "\n",
    "# df_funding = data[[col for col in data.columns if ('Funding Sprd' in col)]+['Fed Fund']+['ECB Rate']]\n",
    "\n",
    "# if np.isnan(df_funding.loc[df_funding.index[-1],'Fed Fund']):\n",
    "#     df_funding.loc[df_funding.index[-1],'Fed Fund'] = df_funding.loc[df_funding.index[-2],'Fed Fund']\n",
    "\n",
    "# for col in df_funding:\n",
    "#     if col.endswith('Sprd'):\n",
    "#         if col.split(' ')[0] in ['HYG','LQD']:\n",
    "#             df_funding[f'Net Long {col.replace(\" Sprd\",\"\")}'] = (df_funding['Fed Fund'] + df_funding[f'{col}']/100) + 0.25/100\n",
    "#             df_funding[f'Net Short {col.replace(\" Sprd\",\"\")}'] = (df_funding['Fed Fund'] + df_funding[f'{col}']/100) - 0.25/100\n",
    "#         if col.split(' ')[0] in ['IHYG','IEAC']:\n",
    "#             df_funding[f'Net Long {col.replace(\" Sprd\",\"\")}'] = (df_funding['ECB Rate'] + df_funding[f'{col}']/100) + 0.25/100\n",
    "#             df_funding[f'Net Short {col.replace(\" Sprd\",\"\")}'] = (df_funding['ECB Rate'] + df_funding[f'{col}']/100) - 0.25/100\n",
    "\n",
    "# df_funding['Net Long VCIT Funding'] = df_funding['Net Long LQD Funding']\n",
    "# df_funding['Net Short VCIT Funding'] = df_funding['Net Short LQD Funding']\n",
    "\n",
    "# for item in ['EMB','EEM']:\n",
    "#     df_funding[f'Net Long {item} Funding'] = df_funding['Fed Fund'] + 0.5\n",
    "#     df_funding[f'Net Short {item} Funding'] = df_funding['Fed Fund'] - 0.5\n",
    "\n",
    "# for item in ['IEI', 'IEF', 'RSP', 'BKLN', 'GSCBHYEQ', 'GSCBIGEQ', 'SPX', 'RTY', 'SPY', 'IJH','IWM']:\n",
    "#     df_funding[f'Net Long {item} Funding'] = df_funding['Fed Fund'] + 0.15\n",
    "#     df_funding[f'Net Short {item} Funding'] = df_funding['Fed Fund'] - 0.15\n",
    "\n",
    "# for item in ['IBCN','SX5E']:\n",
    "#     df_funding[f'Net Long {item} Funding'] = df_funding['ECB Rate'] + 0.15\n",
    "#     df_funding[f'Net Short {item} Funding'] = df_funding['ECB Rate'] - 0.15\n",
    "\n",
    "# df_funding = df_funding[[col for col in df_funding.columns if col.startswith(\"Net\")]]\n",
    "# df_funding.index = pd.to_datetime(df_funding.index)\n",
    "# df_funding = df_funding.resample('D').last()\n",
    "\n",
    "# original_er_data = data[[col for col in data.columns if col.startswith(\"ER \")]]\n",
    "# tr_data = data[[col for col in data.columns if col.startswith(\"TR \")]]\n",
    "# ust = tr_data[['TR LT09TRUU']] # for using corr later\n",
    "# tr_data = tr_data.iloc[:,:-3] #dropping LT03/09 and QW3I\n",
    "\n",
    "# tr_data.index = pd.to_datetime(tr_data.index).date\n",
    "# df_funding.index = pd.to_datetime(df_funding.index).date\n",
    "\n",
    "# er_tr_data = pd.concat([tr_data,df_funding],axis=1)\n",
    "# er_tr_data = er_tr_data.sort_index()\n",
    "# # er_tr_data = er_tr_data.dropna()\n",
    "\n",
    "# etfs = [col for col in er_tr_data.columns if col.startswith(\"TR \")]\n",
    "\n",
    "# for item in etfs:\n",
    "#     er_tr_data[item] = er_tr_data[item].diff()/er_tr_data[item].shift()\n",
    "\n",
    "# er_tr_data['Date'] = pd.to_datetime(er_tr_data.index)\n",
    "# er_tr_data['Days'] = (er_tr_data['Date'] - er_tr_data['Date'].shift()).dt.days\n",
    "# # er_tr_data = er_tr_data.dropna()\n",
    "# er_tr_data\n",
    "\n",
    "# ############################################################### Funding Sprds\n",
    "# funding = er_tr_data[[col for col in er_tr_data.columns if 'Funding' in col]].copy()\n",
    "# x = er_tr_data[[col for col in er_tr_data.columns if 'Funding' in col]].copy()\n",
    "# x = x.interpolate()\n",
    "# x.to_excel(\"Funding Rates.xlsx\")\n",
    "\n",
    "# y = x.copy()\n",
    "# y = round(y,2)\n",
    "# y.to_excel(\"Funding Rates 2.xlsx\")\n",
    "\n",
    "# ###############################################################\n",
    "# for item in etfs:\n",
    "#     name = item.split(' ')[1]\n",
    "#     er_tr_data[f'ER {name}'] = er_tr_data[item] - \\\n",
    "#                 (1/100)*(er_tr_data['Days']/360)*(0.5*(er_tr_data[f'Net Long {name} Funding'] + er_tr_data[f'Net Short {name} Funding']))\n",
    "\n",
    "\n",
    "# er_tr_data = er_tr_data[[col for col in er_tr_data.columns if col.startswith(\"ER \")]]\n",
    "# er_tr_data = (1+er_tr_data).cumprod()\n",
    "\n",
    "# tr_data.index = pd.to_datetime(tr_data.index).date\n",
    "# df_funding.index = pd.to_datetime(df_funding.index).date\n",
    "\n",
    "# er_tr_data = pd.concat([tr_data,df_funding],axis=1)\n",
    "# er_tr_data = er_tr_data.sort_index()\n",
    "# # er_tr_data = er_tr_data.dropna()\n",
    "\n",
    "# etfs = [col for col in er_tr_data.columns if col.startswith(\"TR \")]\n",
    "\n",
    "# for item in etfs:\n",
    "#     er_tr_data[item] = er_tr_data[item].diff()/er_tr_data[item].shift()\n",
    "\n",
    "# er_tr_data['Date'] = pd.to_datetime(er_tr_data.index)\n",
    "# er_tr_data['Days'] = (er_tr_data['Date'] - er_tr_data['Date'].shift()).dt.days\n",
    "# # er_tr_data = er_tr_data.dropna()\n",
    "\n",
    "# for item in etfs:\n",
    "#     name = item.split(' ')[1]\n",
    "#     er_tr_data[f'ER {name}'] = er_tr_data[item] - \\\n",
    "#                 (1/100)*(er_tr_data['Days']/360)*(0.5*(er_tr_data[f'Net Long {name} Funding'] + er_tr_data[f'Net Short {name} Funding']))\n",
    "\n",
    "# er_tr_data = er_tr_data[[col for col in er_tr_data.columns if col.startswith(\"ER \")]]\n",
    "# er_tr_data = (1+er_tr_data).cumprod()\n",
    "\n",
    "# er_data = pd.concat([original_er_data,er_tr_data],axis=1)\n",
    "# # er_data = er_data.dropna()\n",
    "# # er_data.columns = er_data.columns.str.replace(\"ER SPX\",\"ER ESA\").str.replace(\"ER RTY\",\"ER RTYA\").str.replace(\"ER SX5E\",\"ER VGA\")\n",
    "# er_data.columns = er_data.columns.str.replace(\"ER GSCBHYEQ\",\"ER HY Eqty\").str.replace(\"ER GSCBIGEQ\",\"ER IG Eqty\")\n",
    "\n",
    "# securities = ['SPXFP INDEX', 'RTYFPE INDEX','SX5EFSER Index']\n",
    "# fields = ['PX_LAST']\n",
    "# df = blp.bdh(tickers=securities, start_date = er_data.index[0], end_date = er_data.index[-1], flds = fields)\n",
    "# df.columns = ['ER SPX','ER RTY','ER SX5E']\n",
    "# er_data = pd.concat([er_data,df], axis=1)\n",
    "# er_data = er_data.sort_index()\n",
    "\n",
    "# er_data.to_csv(\"All ER.csv\")\n",
    "\n",
    "# ##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa8138fe-14a2-49f3-bc17-33df5ce96e49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# all_start_date = str((datetime.now()-timedelta(days=13*365+1)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "# labels = {\n",
    "#     \"CDX IG 5Y\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_DUR)\",\n",
    "#     \"CDX IG 10Y\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_DUR)\",\n",
    "#     \"ITRX MAIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_DUR)\",\n",
    "#     \"ITRX MAIN 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_DUR)\",\n",
    "#     \"ITRX SNRFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_DUR)\",\n",
    "#     \"ITRX SUBFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_DUR)\",\n",
    "#     \"ITRX XOVER 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_DUR)\",\n",
    "#     \"ITRX XOVER 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/10y/JPM_DUR)\"\n",
    "# }\n",
    "\n",
    "# dq = DataQuery(\n",
    "# client_id='jbAIMF2Tkp0JO3sc',\n",
    "# client_secret='d7qfzgt55pddjs352sgxosFyI4t2eja07k7opbi6wg9oqjc1OjkdAksn1btmnugeMjchcx2vwTsJupw',\n",
    "# )\n",
    "\n",
    "# job = dq.create_job(expressions = list(labels.values()))\n",
    "# dq.start_date = all_start_date\n",
    "# var = job.execute()\n",
    "# df = job.to_pivot_table()\n",
    "# df = df.T\n",
    "# df.index = pd.to_datetime(df.index, format='%Y%m%d').date\n",
    "# df.index.name = 'Date'\n",
    "\n",
    "# df.rename(columns={v:k for k, v in labels.items()},inplace=True)\n",
    "# df.columns.name = None\n",
    "# clear_output(wait=False)\n",
    "# df = df.dropna(how='all')\n",
    "# df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# dur = df.copy()\n",
    "# dur.to_excel(\"DQ Dur.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90ef8ca-31ba-44e6-9e58-c130be84d3b7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# all_start_date = str((datetime.now()-timedelta(days=13*365+1)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "# labels = {\n",
    "#     \"CDX IG 5Y\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_CDSSPREAD_MID)\",\n",
    "#     \"CDX IG 10Y\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_CDSSPREAD_MID)\",\n",
    "#     \"CDX HY 5Y\": \"DB(CDS,TRAC-X,NAHY100UNF05ONRUN,JPM_CLEAN_MID)\",\n",
    "#     \"ITRX MAIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "#     \"ITRX MAIN 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_CDSSPREAD_MID)\",\n",
    "#     \"ITRX SNRFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "#     \"ITRX SUBFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "#     \"ITRX XOVER 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "#     \"ITRX XOVER 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/10y/JPM_CDSSPREAD_MID)\",\n",
    "#     \"CDX EM 5Y\": \"DB(NEO-UK,credit/cds/index/cdxEM-onrun/5y/JPM_CLEAN_MID)\",\n",
    "# }\n",
    "\n",
    "# dq = DataQuery(\n",
    "# client_id='jbAIMF2Tkp0JO3sc',\n",
    "# client_secret='d7qfzgt55pddjs352sgxosFyI4t2eja07k7opbi6wg9oqjc1OjkdAksn1btmnugeMjchcx2vwTsJupw',\n",
    "# )\n",
    "\n",
    "# job = dq.create_job(expressions = list(labels.values()))\n",
    "# dq.start_date = all_start_date\n",
    "# var = job.execute()\n",
    "# df = job.to_pivot_table()\n",
    "# df = df.T\n",
    "# df.index = pd.to_datetime(df.index, format='%Y%m%d').date\n",
    "# df.index.name = 'Date'\n",
    "\n",
    "# df.rename(columns={v:k for k, v in labels.items()},inplace=True)\n",
    "# df.columns.name = None\n",
    "# clear_output(wait=False)\n",
    "# df = df.dropna(how='all')\n",
    "# df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# dur = df.copy()\n",
    "# dur.to_excel(\"DQ Ref Levels_PX_Sprd.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e3e16cf-ed41-4ff1-8d21-947a7368e1fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_backup = pd.read_parquet(\"Clean 1min data.parquet\")\n",
    "# df = pd.read_excel(\"IJH Data.xlsx\")\n",
    "# all_x = None\n",
    "# for i in range(int(len(df.columns)))[::2]:\n",
    "#     x = df.iloc[:,i:i+2]\n",
    "#     x.columns = [\"Date\",\"IJH\"]\n",
    "#     all_x = pd.concat([all_x, x])\n",
    "# all_x = all_x.dropna()\n",
    "# all_x = all_x.drop_duplicates()\n",
    "# all_x[\"Date\"] = pd.to_datetime(all_x[\"Date\"])\n",
    "# all_x = all_x.set_index(\"Date\")\n",
    "# all_x = all_x.resample(\"1min\").last().ffill().copy()\n",
    "# all_x = all_x[all_x.index.isin(df_backup.index)].copy()\n",
    "\n",
    "# df = pd.concat([df_backup, all_x],axis=1)\n",
    "# df = df[list(set(df.columns))].copy()\n",
    "# df.to_parquet(\"Clean 1min data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d463e11-9e5a-41ad-a11d-dd701918653a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dict_map = {\n",
    "    # product type, start time, end time, carry (%), trades on sprd, slippage (bps or $),\n",
    "    # fixed commission, notional (if selected as Y; moved to model up look up!), BBG ticker\n",
    "    'CDX IG 5Y': ['CDX', '07:45:00', '16:30:00', 1, 'Yes', 0.15, 300, \"CDX IG CDSI GEN 5Y CORP\"],\n",
    "    'CDX IG 10Y': ['CDX', '07:45:00', '16:30:00', 1, 'Yes', 0.3, 300, \"CDX IG CDSI GEN 10Y CORP\"],\n",
    "    'CDX HY 5Y': ['CDX', '07:45:00', '16:30:00', 5, 'No', 0.02, 300, \"CDX HY CDSI GEN 5Y CORP\"],\n",
    "    'SPX': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"SPX INDEX\"],\n",
    "    'SPY': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"SPY US EQUITY\"],\n",
    "    'RSP': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"RSP US INDEX\"],    \n",
    "    'RTY': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"RTY INDEX\"],\n",
    "    'IG Eqty': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"GSCBIGEQ Index\"],\n",
    "    'HY Eqty': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"GSCBHYEQ Index\"],\n",
    "    'ITRX MAIN 5Y': ['CDX', '03:30:00', '11:59:00', 1, 'Yes', 0.15, 300, \"ITRX EUR CDSI GEN 5Y CORP\"],\n",
    "    'ITRX XOVER 5Y': ['CDX', '03:30:00', '11:59:00', 5, 'Yes', 0.15, 300, \"ITRX XOVER CDSI GEN 5Y CORP\"],\n",
    "    \n",
    "    'VIX': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"VIX INDEX\"],\n",
    "    'V2X': ['Eq', '03:30:00', '11:59:00', 0, 'No', 0.01, 0, \"V2X INDEX\"],\n",
    "    'SX5E': ['Eq', '03:30:00', '11:59:00', 0, 'No', 0.01, 0, \"SX5E INDEX\"],\n",
    "    'ITRX SNRFIN 5Y': ['CDX', '03:30:00', '11:59:00', 1, 'Yes', 0.15, 300, \"SNRFIN CDSI GEN 5Y CORP\"],\n",
    "    'ITRX SUBFIN 5Y': ['CDX', '03:30:00', '11:59:00', 1, 'Yes', 0.15, 300, \"SUBFIN CDSI GEN 5Y CORP\"],\n",
    "    'CDX EM 5Y': ['CDX', '07:45:00', '16:30:00', 1, 'No', 0.02, 300, \"CDX EM CDSI GEN 5Y CORP\"],\n",
    "\n",
    "    'HYG': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"HYG US EQUITY\"],\n",
    "    'EMB': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"EMB US EQUITY\"],\n",
    "    'EEM': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"EEM US EQUITY\"],\n",
    "    'VCIT': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"VCIT US EQUITY\"],\n",
    "    'LQD': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"LQD US EQUITY\"],\n",
    "    'IEI': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"IEI US EQUITY\"],\n",
    "    'IEF': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"IEF US EQUITY\"],\n",
    "    'IWM': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"IWM US EQUITY\"],\n",
    "    'IJH': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"IJH US EQUITY\"],\n",
    "}\n",
    "\n",
    "# er_data = pd.read_csv(\"All ER.csv\", index_col=0, parse_dates=True)\n",
    "# er_data.columns = [item.split(\" \",1)[1] for item in er_data.columns]\n",
    "\n",
    "# df_backup = pd.read_parquet(\"Clean 1min data.parquet\")\n",
    "\n",
    "# ##################################################################\n",
    "\n",
    "# bbg_tickers = [dict_map[item][7] for item in dict_map.keys()]\n",
    "# reverse_dict = dict(zip(bbg_tickers, list(dict_map.keys())))\n",
    "# bbg_data = blp.bdh(tickers = bbg_tickers, flds='px_last', start_date='2017-01-01')\n",
    "# bbg_data.columns = bbg_tickers\n",
    "# bbg_data.index = pd.to_datetime(bbg_data.index)\n",
    "# bbg_data.columns = [reverse_dict[item] for item in bbg_data.columns]\n",
    "# ref = pd.read_excel(\"DQ Ref Levels_PX_Sprd.xlsx\", index_col=0, parse_dates=True)\n",
    "\n",
    "# for col in ref.columns:\n",
    "#     bbg_data[col] = ref[col]\n",
    "\n",
    "# bbg_data1 = bbg_data.resample(\"1min\").last().ffill().copy()\n",
    "# bbg_data1.columns = [item +'_bbg_px' for item in bbg_data1.columns]\n",
    "\n",
    "# bbg_data2 = bbg_data.shift().resample(\"1min\").last().ffill().copy()\n",
    "# bbg_data2.columns = [item +'_bbg_px_2' for item in bbg_data2.columns]\n",
    "\n",
    "# ##################################################################\n",
    "\n",
    "# dur = pd.read_excel(\"DQ Dur.xlsx\",index_col=0, parse_dates=True)\n",
    "# dur = dur.shift().resample(\"1min\").last().ffill().copy()  ############ yesterday's duration we take .. we have shifted it here\n",
    "# dur.columns = [item + '_dq_dur' for item in dur.columns]\n",
    "\n",
    "# df = df_backup.copy()\n",
    "# er = er_data.copy()\n",
    "# er.columns = [item + '_dq_ER' for item in er.columns]\n",
    "# er = er.resample(\"1min\").last().ffill().copy()\n",
    "\n",
    "# er2 = er_data.shift().copy()\n",
    "# er2.columns = [item + '_dq_ER_2' for item in er2.columns]\n",
    "# er2 = er2.resample(\"1min\").last().ffill().copy()\n",
    "\n",
    "# ##################################################################\n",
    "# #### V. V. Imp: the dq close is as of 5PM and with bbg_data only till 4PM we don't really 'see' the BBG ER series match the DQ series\n",
    "\n",
    "# intraday_tr_data = None\n",
    "\n",
    "# for col in df.columns:\n",
    "#     # col = \"IEI\"\n",
    "#     if dict_map[col][4] == 'Yes':\n",
    "#         x = pd.concat([df[[col]], er[[f'{col}_dq_ER']], er2[[f'{col}_dq_ER_2']], bbg_data1[[f'{col}_bbg_px']], bbg_data2[[f'{col}_bbg_px_2']],\n",
    "#             dur[[f'{col}_dq_dur']]], axis=1).sort_index().dropna().copy()\n",
    "#         x['TR Change'] = (x[f'{col}_dq_ER'] / x[f'{col}_dq_ER_2'] - 1)\n",
    "#         x['d-o-d sprd pnl'] = (-1) * (x[f'{col}_dq_dur']) * (x[f'{col}_bbg_px'] - x[f'{col}_bbg_px_2']) * 10**(-4)\n",
    "#         x['intraday sprd pnl'] = (-1) * (x[f'{col}_dq_dur']) * (x[col] - x[f'{col}_bbg_px_2']) * 10**(-4)\n",
    "#         x['Calculated TR Change'] = x['TR Change'] - x['d-o-d sprd pnl'] + x['intraday sprd pnl']\n",
    "#         x['Actual TR Series'] = (1 + x['Calculated TR Change']) * x[f'{col}_dq_ER_2']\n",
    "#         x = x[['Actual TR Series']].copy()\n",
    "#         x.columns = [col]\n",
    "#     else:\n",
    "#         x = pd.concat([df[[col]], er[[f'{col}_dq_ER']], er2[[f'{col}_dq_ER_2']], bbg_data1[[f'{col}_bbg_px']], bbg_data2[[f'{col}_bbg_px_2']],\n",
    "#             ], axis=1).sort_index().dropna().copy()\n",
    "#         x['TR Change'] = (x[f'{col}_dq_ER'] / x[f'{col}_dq_ER_2'] - 1)\n",
    "#         if col in [\"CDX HY 5Y\", \"CDX HY 10Y\", \"CDX EM 5Y\"]:\n",
    "#             x['d-o-d px pnl'] = (x[f'{col}_bbg_px'] - x[f'{col}_bbg_px_2']) * 10**(-2)\n",
    "#             x['intraday px pnl'] = (x[col] - x[f'{col}_bbg_px_2']) * 10**(-2)\n",
    "#         else:  ### it is an etf\n",
    "#             x['d-o-d px pnl'] = (x[f'{col}_bbg_px']/ x[f'{col}_bbg_px_2'] - 1)\n",
    "#             x['intraday px pnl'] = (x[col] / x[f'{col}_bbg_px_2'] - 1)\n",
    "#         x['Calculated TR Change'] = x['TR Change'] - x['d-o-d px pnl'] + x['intraday px pnl']\n",
    "#         x['Actual TR Series'] = (1 + x['Calculated TR Change']) * x[f'{col}_dq_ER_2']\n",
    "#         x = x[['Actual TR Series']].copy()\n",
    "#         x.columns = [col]\n",
    "#     intraday_tr_data = pd.concat([intraday_tr_data, x], axis=1)\n",
    "\n",
    "# intraday_tr_data.to_parquet(\"1min ER series.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "455d0189-5320-4a2a-b5ec-c8aa732c626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_method = \"Rtn\" #\"Rtn\"\n",
    "diff_period_list = [1]\n",
    "\n",
    "models_list = [ \n",
    "    # model_Y, model_X (specify as a list) ### We trade these\n",
    "    # zscore_Y, zscore_X (specify as a list) ### We use these only for generating the zscore; names are taken from BBG datafile\n",
    "\n",
    "    # ['CDX HY 5Y', ['HYG','IEI'], 'CDX HY 5Y', ['HYG','IEI'], 7*10**6],\n",
    "    # ['CDX EM 5Y', ['EMB','IEF'], 'CDX EM 5Y', ['EMB','IEF'], 9*10**6],\n",
    "    # ['CDX IG 5Y', ['VCIT','IEF'], 'CDX IG 5Y', ['VCIT','IEF'], 25*10**6],\n",
    "    \n",
    "    # ['CDX IG 10Y', ['LQD','IEF'], 'CDX IG 10Y', ['LQD','IEF'], 14*10**6],\n",
    "    ['CDX IG 5Y', ['SPX'],'CDX IG 5Y', ['SPX'],28*10**6],\n",
    "    # # ['CDX IG 5Y', ['RSP'],'CDX IG 5Y', ['RSP'],28*10**6],\n",
    "    # ['CDX IG 5Y', ['IWM'],'CDX IG 5Y', ['IWM'],28*10**6],\n",
    "    # ['CDX IG 5Y', ['IJH'],'CDX IG 5Y', ['IJH'],28*10**6],\n",
    "    # # ['CDX IG 5Y', ['IG Eqty'],'CDX IG 5Y', ['IG Eqty'],28*10**6],\n",
    "    \n",
    "    # ['CDX HY 5Y', ['SPX'],'CDX HY 5Y', ['SPX'],6*10**6],\n",
    "    # # ['CDX HY 5Y', ['RTY'],'CDX HY 5Y', ['RTY'],6*10**6],\n",
    "    # # ['CDX HY 5Y', ['RSP'],'CDX HY 5Y', ['RSP'],6*10**6],\n",
    "    # ['CDX HY 5Y', ['IWM'],'CDX HY 5Y', ['IWM'],6*10**6],\n",
    "    # ['CDX HY 5Y', ['IJH'],'CDX HY 5Y', ['IJH'],6*10**6],\n",
    "    # # ['CDX HY 5Y', ['HY Eqty'],'CDX HY 5Y', ['HY Eqty'],6*10**6],\n",
    "    \n",
    "    # # ['ITRX MAIN 5Y', ['SX5E'],'ITRX MAIN 5Y', ['SX5E'], 28*10**6],\n",
    "    # # ['ITRX XOVER 5Y', ['SX5E'],'ITRX XOVER 5Y', ['SX5E'], 6*10**6],\n",
    "    # # ['ITRX XOVER 5Y', ['SPX'],'ITRX XOVER 5Y', ['SPX'], 6*10**6],\n",
    "    # # ['CDX IG 5Y', ['ITRX MAIN 5Y'],'CDX IG 5Y', ['ITRX MAIN 5Y'], 50*10**6],\n",
    "    \n",
    "    # # ['CDX HY 5Y', ['ITRX XOVER 5Y'],'CDX HY 5Y', ['ITRX XOVER 5Y'], 8*10**6],\n",
    "    # # ['ITRX MAIN 5Y', ['ITRX XOVER 5Y'],'ITRX MAIN 5Y', ['ITRX XOVER 5Y'], 66*10**6],\n",
    "    # # ['CDX IG 5Y', ['CDX HY 5Y'],'CDX IG 5Y', ['CDX HY 5Y'], 74*10**6],\n",
    "    \n",
    "    # # ['CDX IG 5Y', ['CDX EM 5Y'],'CDX IG 5Y', ['CDX EM 5Y'], 22*10**6],\n",
    "    # # ['CDX EM 5Y', ['CDX HY 5Y'],'CDX EM 5Y', ['CDX HY 5Y'], 8*10**6],\n",
    "    # # ['ITRX MAIN 5Y', ['CDX EM 5Y'],'ITRX MAIN 5Y', ['CDX EM 5Y'], 22*10**6],\n",
    "    # # ['CDX EM 5Y', ['ITRX XOVER 5Y'],'CDX EM 5Y', ['ITRX XOVER 5Y'], 6*10**6],\n",
    "\n",
    "    # # ['CDX IG 5Y', ['SPY'],'CDX IG 5Y', ['SPY'],28*10**6],\n",
    "    # # ['CDX HY 5Y', ['SPY'],'CDX HY 5Y', ['SPY'],6*10**6],\n",
    "    \n",
    "    # # ['EEM', ['EMB'], 'EEM', ['EMB'], 1*10**6],\n",
    "    # # ['CDX EM 5Y', ['EEM'], 'CDX EM 5Y', ['EEM'], 6*10**6],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b472e43d-b30b-419e-9d87-58036c6990a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dict_models = {\n",
    "    1 : [\"Intraday\",252,252,'A (Intraday; 12M)'],\n",
    "    # 2 : [\"Intraday\",315,315,'B (Intraday; 15M)'],\n",
    "    # 3 : [\"Intraday\",378,378,'C (Intraday; 18M)'],\n",
    "    # 4 : [\"Intraday\",504,504,'D (Intraday; 24M)'],\n",
    "}\n",
    "\n",
    "sampling_freq = '10min'\n",
    "\n",
    "for global_model in models_list:\n",
    "    for model_num in list(dict_models.keys()):\n",
    "        for trade_btdf_direction in ['Long','Short','Long/Short']:\n",
    "            for info in ['$pnl','$pnl/trade','SR','Hit Ratio','trades','days/trade','max DD']:\n",
    "                globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_{info}'] = pd.DataFrame()\n",
    "                globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_{info}'].index.name = 'Entry'\n",
    "                globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_{info}'].columns.name = info\n",
    "\n",
    "fund_rates = pd.read_excel(\"Funding Rates 2.xlsx\")\n",
    "fund_rates.columns = ['Date'] + list(fund_rates.columns)[1:]\n",
    "fund_rates['Date'] = pd.to_datetime(fund_rates['Date'])\n",
    "fund_rates = fund_rates.set_index('Date')\n",
    "fund_rates.columns = fund_rates.columns.str.replace(\"GSCB\",\"\").str.replace(\"EQ \",\" Eqty \").str.replace(\" Funding\",\"\").str.replace(\"Net \",\"\")\n",
    "etfs = list(set([item.split(\" \",1)[1] for item in fund_rates.columns]))\n",
    "\n",
    "def apply_funding(df_funding_update, variable_item):\n",
    "    if f'Long {variable_item}' in fund_rates.columns:\n",
    "        rate = fund_rates[[f'Long {variable_item}',f'Short {variable_item}']].dropna().resample(\"1min\").last().bfill().copy()\n",
    "        df_funding_update = pd.concat([df_funding_update, rate],axis=1)\n",
    "        df_funding_update = df_funding_update.dropna()\n",
    "        df_funding_update.columns = list(df_funding_update.columns)[:-2] + ['Long_Funding','Short_Funding']\n",
    "        df_funding_update['T'] = df_funding_update.index.date\n",
    "        df_funding_update['T-1'] = [np.nan] + list(df_funding_update.index[:-1].date)\n",
    "        df_funding_update['Funding_Date'] = df_funding_update.apply(lambda row: 'New Day' if row['T'] != row['T-1'] else 'Same Day', axis=1)\n",
    "        df_funding_update['Long_Funding'] = df_funding_update.apply(lambda row: row[\"Long_Funding\"] if row[\"Funding_Date\"] == \"New Day\" else 0,axis=1)\n",
    "        df_funding_update['Short_Funding'] = df_funding_update.apply(lambda row: row[\"Short_Funding\"] if row[\"Funding_Date\"] == \"New Day\" else 0,axis=1)\n",
    "        df_funding_update = df_funding_update.drop(['Funding_Date'],axis=1)\n",
    "        # df_funding_update['Funding'] = 0.5*(df_funding_update['Long_Funding'] + df_funding_update['Short_Funding'])\n",
    "        df_funding_update['T-1'] = [item.date() for item in pd.to_datetime(df_funding_update['T-1'])]\n",
    "        df_funding_update['T-1'].iloc[0] = df_funding_update['T'].iloc[0]\n",
    "        df_funding_update['Long Funding P/L'] = [item.days for item in (df_funding_update['T'] - df_funding_update['T-1'])]\n",
    "        df_funding_update['Long Funding P/L'] *= (df_funding_update['Long_Funding'] / 100) * (1/360) * abs(df_funding_update['Notional'])\n",
    "        df_funding_update['Short Funding P/L'] = [item.days for item in (df_funding_update['T'] - df_funding_update['T-1'])]\n",
    "        df_funding_update['Short Funding P/L'] *= (df_funding_update['Short_Funding'] / 100) * (1/360) * abs(df_funding_update['Notional'])\n",
    "        df_funding_update = df_funding_update.drop(['T','T-1','Long_Funding','Short_Funding'],axis=1)\n",
    "    else:\n",
    "        df_funding_update['Long Funding P/L'] = [0.0] * len(df_funding_update)\n",
    "        df_funding_update['Short Funding P/L'] = [0.0] * len(df_funding_update)\n",
    "\n",
    "    return df_funding_update.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cdaefd-6710-4936-9fda-1658fc19990c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# bbg_px = pd.read_parquet(\"Clean 1min data.parquet\")\n",
    "bbg_px = pd.read_excel(\"10min data with EUv8.xlsx\", index_col=0, parse_dates=True)\n",
    "\n",
    "if z_score_method == \"PX\":\n",
    "    zscore_calc_df = bbg_px.copy()\n",
    "elif z_score_method == \"Rtn\":\n",
    "    zscore_calc_df = pd.read_parquet(\"1min ER series.parquet\")   \n",
    "    # zscore_calc_df = pd.read_parquet(\"Intraday 1min ER series.parquet\")\n",
    "\n",
    "for global_model in models_list:\n",
    "    for model_num in list(dict_models.keys()):  \n",
    "\n",
    "        # global_model = models_list[0]\n",
    "        # model_num = list(dict_models.keys())[0]\n",
    "        \n",
    "        model_Y = global_model[0]\n",
    "        model_X = global_model[1]\n",
    "        zscore_Y = global_model[2]\n",
    "        zscore_X = global_model[3]\n",
    "        backtest_start_date = pd.to_datetime('2017-03-01')\n",
    "        notional_to_use = global_model[4]\n",
    "        \n",
    "        zscore_vars = [model_Y, zscore_Y] + model_X + zscore_X\n",
    "        \n",
    "        zscore_vars = list(set(zscore_vars))\n",
    "        zscore_vars_start_time = max([dict_map[item][1] for item in zscore_vars])\n",
    "        zscore_vars_end_time = min([dict_map[item][2] for item in zscore_vars])\n",
    "        \n",
    "        ################################## Beta Calculation\n",
    "        if len(model_X) == 1:\n",
    "            er_Y = f'ER {model_Y}'\n",
    "            er_X = f'ER {model_X[0]}'\n",
    "            er_data = pd.read_csv(\"All ER.csv\")\n",
    "            er_data.columns = ['Date'] + list(er_data.columns)[1:]\n",
    "            er_data['Date'] = pd.to_datetime(er_data['Date'])\n",
    "            er_data = er_data.set_index('Date')\n",
    "            er_data = er_data.sort_index()\n",
    "            beta = er_data[[er_Y, er_X]].dropna()\n",
    "            beta = beta.resample('W').last()\n",
    "            beta = np.log(beta)\n",
    "            beta = beta.diff().dropna()\n",
    "            beta['Beta1'] = [np.nan] * len(beta)\n",
    "            beta['Beta2'] = [np.nan] * len(beta)\n",
    "            \n",
    "            for i in range(len(beta)-24+1):\n",
    "                reg_X = beta[er_X].iloc[i:i+24]\n",
    "                reg_Y = beta[er_Y].iloc[i:i+24]\n",
    "                model = sm.OLS(reg_Y, sm.add_constant(reg_X)).fit() \n",
    "                beta.iloc[i+23,2] = model.params.iloc[1]\n",
    "            \n",
    "                model = sm.OLS(reg_X, sm.add_constant(reg_Y)).fit() \n",
    "                beta.iloc[i+23,3] = model.params.iloc[1]\n",
    "            \n",
    "            beta['Beta1'] = beta['Beta1'].rolling(104).mean()\n",
    "            beta['Beta2'] = beta['Beta2'].rolling(104).mean()\n",
    "            beta['Beta'] = 0.5*(beta['Beta1'] + 1/ beta['Beta2'])\n",
    "            beta = beta[['Beta']].dropna()\n",
    "            # beta = pd.read_excel(\"igspx_beta_1ymodel.xlsx\", index_col=0, parse_dates=True)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            b1 = pd.read_csv(\"All Basis Trade Betas.csv\")\n",
    "            b1.columns = ['Date'] + list(b1.columns)[1:]\n",
    "            b1 = b1.set_index('Date')\n",
    "            beta = b1[[f'{model_Y}_{model_X[0]}_{model_X[1]}']]\n",
    "            beta.columns = ['Beta']\n",
    "            beta['Coef1'] = beta['Beta'].apply(lambda x: eval(x)[0])\n",
    "            beta['Coef2'] = beta['Beta'].apply(lambda x: eval(x)[1])\n",
    "            beta.index = pd.to_datetime(beta.index)\n",
    "            \n",
    "        beta = beta.resample(\"1min\").first().ffill()\n",
    "        \n",
    "        ################################## BBG DataFile Intraday\n",
    "        # df = pd.read_excel(\"10min data with EUv7.xlsx\")\n",
    "        # df['Date'] = pd.to_datetime(df['Date'])\n",
    "        # df = df.set_index('Date')\n",
    "        # df = df.sort_index()\n",
    "        df = zscore_calc_df.copy()\n",
    "        \n",
    "        zscore_df = df[zscore_vars].between_time(zscore_vars_start_time, zscore_vars_end_time).dropna().copy()\n",
    "        zscore_df = zscore_df.resample(\"10min\",offset=\"5min\").last().dropna().copy()\n",
    "        zscore_df = zscore_df.between_time(zscore_vars_start_time, zscore_vars_end_time).dropna().copy()\n",
    "        bbg_datafile = zscore_df.copy()\n",
    "        \n",
    "        sampling_multiplier = len(set(list(bbg_datafile.index.time)))\n",
    "        \n",
    "        ################################## ZScore Calculation Start : Convert Sprd to PX series\n",
    "        \n",
    "        df = pd.read_excel(\"All DQ Duration.xlsx\")\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.set_index('Date')\n",
    "        df = df.sort_index()\n",
    "        df.columns = df.columns.str.replace(\" Dur\",\"\")\n",
    "        df = df.resample(\"1min\").first().ffill().dropna()\n",
    "        dq_dur = df.copy()\n",
    "        \n",
    "        zscore_df1 = zscore_df.copy()\n",
    "\n",
    "        if z_score_method == \"PX\":\n",
    "            for col in zscore_df1.columns:\n",
    "                if col in dq_dur.columns:\n",
    "                    zscore_df1[f'{col} Dur'] = dq_dur[col]\n",
    "                    zscore_df1[f'{col} Dur'] = zscore_df1[f'{col} Dur'].shift(1)\n",
    "                    zscore_df1[f'Diff {col}'] = zscore_df1[col].diff()\n",
    "                    zscore_df1 = zscore_df1.dropna()\n",
    "                    zscore_df1[f'{col} Daily PX Change'] = -1 * zscore_df1[f'Diff {col}'] * zscore_df1[f'{col} Dur'] *10**(-4)\n",
    "                    zscore_df1[f'{col} Sum PX'] = zscore_df1[f'{col} Daily PX Change'].cumsum()\n",
    "                    zscore_df1[col] = zscore_df1[f'{col} Sum PX']\n",
    "                    zscore_df1 = zscore_df1[zscore_df.columns].copy()\n",
    "        \n",
    "        ################################## ZScore Calculation: Differencing and converting to ZScores\n",
    "        \n",
    "        zscore_df = zscore_df1[zscore_df1.index >= backtest_start_date].copy()\n",
    "        \n",
    "        col_list = zscore_df.columns\n",
    "        for period in diff_period_list:\n",
    "            for col in col_list:\n",
    "                zscore_df[f'{col}_{period}W'] = zscore_df[col].diff(sampling_multiplier*5*period)\n",
    "                # zscore_df[f'{col}_{period}W'] = zscore_df[col].diff(sampling_multiplier*period)  ##### Daily\n",
    "        \n",
    "        model_lookback = sampling_multiplier*dict_models[model_num][1]\n",
    "        model_lookback_res = sampling_multiplier*dict_models[model_num][2]\n",
    "        zscore_df = zscore_df.dropna().copy()\n",
    "        zscore_df2 = zscore_df.copy()\n",
    "\n",
    "        for period in diff_period_list:\n",
    "            for i in range(len(zscore_df) - model_lookback + 1):\n",
    "        \n",
    "                reg_Y = zscore_df[[f'{zscore_Y}_{period}W']].iloc[i:i+model_lookback]\n",
    "                reg_X = zscore_df[[item + f\"_{period}W\" for item in zscore_X]].iloc[i:i+model_lookback]\n",
    "\n",
    "                model = sm.OLS(reg_Y,sm.add_constant(reg_X)).fit()\n",
    "                x = (model.resid - model.resid.rolling(model_lookback_res).mean())/model.resid.rolling(model_lookback_res).std()\n",
    "                zscore_df.loc[zscore_df.index[i+model_lookback-1],f'{period}W_ZScore'] = x.iloc[-1]\n",
    "\n",
    "        zscore_df['Avg. ZScore'] = zscore_df[[col for col in zscore_df.columns if col.endswith(\"_ZScore\")]].mean(axis=1)\n",
    "        zscore_df = zscore_df[['Avg. ZScore']]\n",
    "        bt_df = pd.concat([bbg_datafile[[model_Y] + model_X],zscore_df],axis=1).dropna()   \n",
    "        \n",
    "        #############################################\n",
    "\n",
    "        if z_score_method == \"Rtn\":\n",
    "            bbg_data = bbg_px.copy()\n",
    "            bbg_data = bbg_data[[model_Y] + model_X].dropna().copy()\n",
    "            bbg_data = bbg_data.resample(\"10min\", offset=\"5min\").last().ffill().copy()\n",
    "            bbg_data = bbg_data[bbg_data.index.isin(bt_df.index)]\n",
    "            for col in bbg_data.columns:\n",
    "                bt_df[col] = bbg_data[col]\n",
    "            \n",
    "        ############################# If dur > 0 => trades on sprd ; if dur = 0 => cdx trades on px ; if dur = -1 => eq. product trades on px\n",
    "        for col in bt_df.columns:\n",
    "            if col != \"Avg. ZScore\":\n",
    "                if col in dq_dur.columns:\n",
    "                    bt_df[f'{col} Dur'] = dq_dur[dq_dur.index.isin(bt_df.index)][col]\n",
    "                elif dict_map[col][0] == 'CDX':\n",
    "                    bt_df[f'{col} Dur'] = [0.0] * len(bt_df)\n",
    "                elif dict_map[col][0] == 'Eq':\n",
    "                    bt_df[f'{col} Dur'] = [-1.0] * len(bt_df)\n",
    "        bt_df['volume'] = [0.0] * len(bt_df)\n",
    "        \n",
    "        bt_df = bt_df[bt_df.index >= pd.to_datetime(\"2021-07-01\")].dropna()\n",
    "        bt_df = bt_df[bt_df.index <= pd.to_datetime(\"2025-04-26\")].dropna()\n",
    "        \n",
    "        ############################# Backtrader dfs\n",
    "        bt_df = pd.concat([bt_df,beta],axis=1).dropna()\n",
    "        bt_df.columns = bt_df.columns.str.replace(\"Beta\",\"Notional\")\n",
    "        \n",
    "        if len(model_X) == 1:\n",
    "            dfy = bt_df[[model_Y,f'{model_Y} Dur','Avg. ZScore','Notional','volume']].dropna().copy()\n",
    "            dfy.columns = dfy.columns.str.replace(\"Avg. ZScore\",\"Signal\").str.replace(f'{model_Y} Dur','Dur').str.replace(model_Y,\"close\")\n",
    "            dfy['Notional'] = notional_to_use\n",
    "            \n",
    "            dfx = bt_df[[model_X[0],f'{model_X[0]} Dur','Avg. ZScore','Notional','volume']].dropna().copy()\n",
    "            dfx.columns = dfx.columns.str.replace(\"Avg. ZScore\",\"Signal\").str.replace(f'{model_X[0]} Dur','Dur').str.replace(model_X[0],\"close\")\n",
    "            dfx['Notional'] *= notional_to_use\n",
    "            dfx['Signal'] *= -1    \n",
    "        \n",
    "            dfy = apply_funding(dfy.copy(), model_Y)\n",
    "            dfx = apply_funding(dfx.copy(), model_X[0])\n",
    "        \n",
    "        if len(model_X) == 2:\n",
    "            \n",
    "            dfy = bt_df[[model_Y,f'{model_Y} Dur','Avg. ZScore','Notional','volume']].dropna().copy()\n",
    "            dfy.columns = dfy.columns.str.replace(\"Avg. ZScore\",\"Signal\").str.replace(f'{model_Y} Dur','Dur').str.replace(model_Y,\"close\")\n",
    "            dfy['Notional'] = notional_to_use\n",
    "            \n",
    "            dfx = bt_df[[model_X[0],f'{model_X[0]} Dur','Avg. ZScore','Notional','volume']].dropna().copy()\n",
    "            dfx.columns = dfx.columns.str.replace(\"Avg. ZScore\",\"Signal\").str.replace(f'{model_X[0]} Dur','Dur').str.replace(model_X[0],\"close\")\n",
    "            dfx['Notional'] = notional_to_use * bt_df['Coef1']\n",
    "            dfx['Signal'] *= -1   ############################ in basis only hyg px we sell when we buy the residuals so only this will be inverted\n",
    "            \n",
    "            dfx1 = bt_df[[model_X[1],f'{model_X[1]} Dur','Avg. ZScore','Notional','volume']].dropna().copy()\n",
    "            dfx1.columns = dfx1.columns.str.replace(\"Avg. ZScore\",\"Signal\").str.replace(f'{model_X[1]} Dur','Dur').str.replace(model_X[1],\"close\")\n",
    "            dfx1['Notional'] = notional_to_use * bt_df['Coef2']\n",
    "        \n",
    "            dfy = apply_funding(dfy.copy(), model_Y)\n",
    "            dfx = apply_funding(dfx.copy(), model_X[0])\n",
    "            dfx1 = apply_funding(dfx1.copy(), model_X[1])\n",
    "        \n",
    "        \n",
    "        class MyData(bt.feeds.PandasData):\n",
    "            lines = ('signal', 'notional', 'dur', 'long_funding', 'short_funding')\n",
    "            params = (('signal', 'Signal'), ('notional', 'Notional'), \n",
    "                      ('dur', 'Dur'), ('long_funding', 'Long Funding P/L'), ('short_funding', 'Short Funding P/L'))\n",
    "        \n",
    "        class FixedComm(bt.CommInfoBase):\n",
    "            params = (('commission', 0), ('stocklike', True), ('commtype', bt.CommInfoBase.COMM_FIXED),)\n",
    "            def _getcommission(self, size, price, pseudoexec):\n",
    "                return self.p.commission\n",
    "        \n",
    "        class TStrategy(bt.Strategy):\n",
    "            params = dict(cheat_on_close=True, entry_zscore = 0, exit_zscore = 0, carry = 0, product = 'Untitled')\n",
    "            \n",
    "            def __init__(self):\n",
    "                self.portfolio_values = []\n",
    "                self.entry_date = None\n",
    "                self.daily_pnl = []\n",
    "                self.roll_trades = []\n",
    "                self.carry = []\n",
    "                self.roll_carry = []\n",
    "                \n",
    "                self.funding = []\n",
    "                self.funding_carry = []\n",
    "                self.funding_direction = None\n",
    "                self.entry_bar_funding = None\n",
    "                \n",
    "                self.trade_direction = []\n",
    "                \n",
    "                self.scatter_plot_trade_pnl = []\n",
    "                self.scatter_plot_trade_init_len = None\n",
    "                \n",
    "                self.carry_direction = 0\n",
    "                self.logs = []\n",
    "        \n",
    "            def log(self, txt):\n",
    "                dt = self.datas[0].datetime.datetime(0)\n",
    "                print(f'{dt} - {txt}')\n",
    "                self.logs.append([dt.isoformat(), txt])\n",
    "        \n",
    "            def notify_trade(self, trade):            \n",
    "                if trade.isclosed:            \n",
    "                    current_date = self.datas[0].datetime.datetime(-1)\n",
    "                    \n",
    "                    self.trade_direction.append((current_date, self.funding_direction if self.p.product == model_Y else None)) \n",
    "                    #### Y determines the long/short direction\n",
    "                    \n",
    "                    \n",
    "                    ######################################## Calculating carry for all trades\n",
    "                    self.daily_pnl.append((current_date, trade.pnlcomm))\n",
    "        \n",
    "                    if self.carry_direction > 0 : \n",
    "                        self.carry.append((current_date, (current_date.date()-self.entry_date.\\\n",
    "                                                          date()).days*(1/360)*self.p.carry*(1/100)*self.datas[0].notional[0]))\n",
    "                    elif self.carry_direction < 0 :\n",
    "                        self.carry.append((current_date, -1*(current_date.date()-self.entry_date.\\\n",
    "                                                         date()).days*(1/360)*self.p.carry*(1/100)*self.datas[0].notional[0]))\n",
    "                    \n",
    "                    ######################################## Calculating funding for all trades\n",
    "                    bars_active = len(self) - self.entry_bar_funding - 1\n",
    "                    \n",
    "                    if self.funding_direction == \"Long\":\n",
    "                        values = [self.datas[0].long_funding[-i] for i in range(bars_active)]\n",
    "                        rolling_sum = -1*sum(values)  ############ We pay the long funding\n",
    "                    \n",
    "                    elif self.funding_direction == \"Short\":\n",
    "                        values = [self.datas[0].short_funding[-i] for i in range(bars_active)]\n",
    "                        rolling_sum = sum(values)  ############ We earn the short funding\n",
    "                        \n",
    "                    self.funding.append((current_date, rolling_sum))\n",
    "        \n",
    "                    \n",
    "                    p1 = [datetime(current_date.year, 3, 20), datetime(current_date.year, 3, 30)]\n",
    "                    p2 = [datetime(current_date.year, 9, 20), datetime(current_date.year, 9, 30)]\n",
    "                    p3 = [datetime(current_date.year+1, 3, 20), datetime(current_date.year+1, 3, 30)]\n",
    "                    p4 = [datetime(current_date.year-1, 9, 20), datetime(current_date.year-1, 9, 30)]\n",
    "                    periods = [p1, p2, p3, p4]\n",
    "                    \n",
    "                    ######################################## Calculating carry for roll trades separately\n",
    "                    \n",
    "                    for i in range(len(periods)):\n",
    "                        p = periods[i]\n",
    "                        if ((self.entry_date <= p[1]) and (p[0] <= current_date)):\n",
    "                            self.roll_trades.append((current_date, trade.pnlcomm))\n",
    "                            if self.carry_direction > 0:\n",
    "                                self.roll_carry.append((current_date, (current_date.date()-self.entry_date.\\\n",
    "                                                               date()).days*(1/360)*self.p.carry*(1/100)*self.datas[0].notional[0]))\n",
    "                            elif self.carry_direction < 0:\n",
    "                                self.roll_carry.append((current_date, -1*(current_date.date()-self.entry_date.\\\n",
    "                                                              date()).days*(1/360)*self.p.carry*(1/100)*self.datas[0].notional[0]))\n",
    "        \n",
    "                            self.funding_carry.append((current_date, rolling_sum))\n",
    "                            \n",
    "                            break\n",
    "                    \n",
    "                    ################################################# All trades\n",
    "                    \n",
    "                    # self.log(f'Gross P/L: {trade.pnl:.2f}, Net P/L: {trade.pnlcomm:.2f}, Funding P/L: {rolling_sum:.2f}')        \n",
    "                    self.carry_direction = 0\n",
    "        \n",
    "            # def notify_order(self, order):\n",
    "            #     if order.status in [order.Completed]:\n",
    "            #         if order.isbuy():\n",
    "            #             self.log(f'BUY EXECUTED, PX: {order.executed.price:.4f}, Qty: {order.executed.size:.2f}, Comm: {order.executed.comm}')\n",
    "            #         else:\n",
    "            #             self.log(f'SELL EXECUTED, PX: {order.executed.price:.4f}, Qty: {order.executed.size:.2f}, Comm: {order.executed.comm}')\n",
    "        \n",
    "            def next(self):\n",
    "                self.broker.set_coc(self.p.cheat_on_close)\n",
    "                # val = None\n",
    "                self.portfolio_values.append((self.datas[0].datetime.datetime(0), self.broker.getvalue()))\n",
    "                \n",
    "                z = self.datas[0].signal[0]\n",
    "                px = self.datas[0].close[0]        \n",
    "                pos = self.getposition(self.datas[0]).size\n",
    "        \n",
    "                # self.log(f'PX: {px}, ZScore: {z:.3f}, Notional: {self.datas[0].notional[0]:.0f}, Dur: {self.datas[0].dur[0]:.4f} '\n",
    "                #          f'Pos: {pos:.2f}, Long Funding $: {self.datas[0].long_funding[0]:.2f}, Short Funding $: {self.datas[0].short_funding[0]:.2f} '\n",
    "                #          f'Eqty (000): {(self.broker.getvalue()-1e12)*10**(-3):.2f}')\n",
    "                \n",
    "                if pos == 0:                \n",
    "                    if z < -self.p.entry_zscore:\n",
    "                        self.scatter_plot_trade_init_len = len(self)\n",
    "                        self.entry_date = self.datas[0].datetime.datetime(0)\n",
    "        \n",
    "                        if self.datas[0].dur[0] == -1:\n",
    "                            self.buy(data = self.data, size = self.datas[0].notional[0]/px)\n",
    "                        elif self.datas[0].dur[0] == 0.0:\n",
    "                            self.buy(data=self.datas[0], size = (self.datas[0].notional[0] / 100))\n",
    "                        elif self.datas[0].dur[0] > 0.0:\n",
    "                            self.sell(data=self.data, size = (self.datas[0].notional[0])*10**(-4)*self.datas[0].dur[0])\n",
    "        \n",
    "                        self.carry_direction = 1\n",
    "                        self.entry_bar_funding = len(self)\n",
    "                        self.funding_direction = \"Long\"\n",
    "        \n",
    "                    elif z > self.p.entry_zscore:\n",
    "                        self.scatter_plot_trade_init_len = len(self)\n",
    "                        self.entry_date = self.datas[0].datetime.datetime(0)\n",
    "        \n",
    "                        if self.datas[0].dur[0] == -1:\n",
    "                            self.sell(data = self.data, size = self.datas[0].notional[0]/px)\n",
    "                        elif self.datas[0].dur[0] == 0.0:\n",
    "                            self.sell(data=self.datas[0], size = (self.datas[0].notional[0] / 100))\n",
    "                        elif self.datas[0].dur[0] > 0.0:\n",
    "                            self.buy(data=self.data, size = (self.datas[0].notional[0])*10**(-4)*self.datas[0].dur[0])\n",
    "        \n",
    "                        self.carry_direction = -1\n",
    "                        self.entry_bar_funding = len(self)\n",
    "                        self.funding_direction = \"Short\"\n",
    "                        \n",
    "                else:\n",
    "                        if self.datas[0].dur[0] <= 0.0: ######## Equity & CDX HY\n",
    "                            if ((pos > 0 and z > -self.p.exit_zscore) or (pos < 0 and z < self.p.exit_zscore)):\n",
    "                                self.close(data=self.datas[0])\n",
    "                                self.scatter_plot_trade_pnl.append((self.datas[0].datetime.\\\n",
    "                                                    datetime(0), len(self) - self.scatter_plot_trade_init_len))\n",
    "                        \n",
    "                        elif self.datas[0].dur[0] > 0.0: ######## CDX IG\n",
    "                            if ((pos < 0 and z > -self.p.exit_zscore) or (pos > 0 and z < self.p.exit_zscore)):\n",
    "                                self.close(data=self.datas[0])\n",
    "                                self.scatter_plot_trade_pnl.append((self.datas[0].datetime.\\\n",
    "                                                    datetime(0), len(self) - self.scatter_plot_trade_init_len))\n",
    "        \n",
    "        \n",
    "            # def stop(self):\n",
    "            #     # with open(f'{self.p.product}.csv', 'w', newline='') as f:\n",
    "            #     with open(f'backtrader_log_{str(datetime.now().time()).replace(\":\",\"_\")}.csv', 'w', newline='') as f:\n",
    "            #         writer = csv.writer(f)\n",
    "            #         writer.writerow(['Date','Message'])\n",
    "            #         writer.writerows(self.logs)\n",
    "        \n",
    "        for strategy_zscore_exit in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "            for strategy_zscore_entry in [0, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5]:#, 1.75, 2.0, 2.5, 3.0]:\n",
    "                if strategy_zscore_entry > strategy_zscore_exit:\n",
    "        \n",
    "                    cerebro1 = bt.Cerebro()\n",
    "                    cerebro1.addstrategy(TStrategy, cheat_on_close=True, entry_zscore = strategy_zscore_entry, exit_zscore = strategy_zscore_exit,\\\n",
    "                                         carry = dict_map[model_Y][3], product = model_Y)\n",
    "                    \n",
    "                    cerebro1.broker.setcash(1e12)\n",
    "                    feed1 = MyData(dataname=dfy.copy(), timeframe=bt.TimeFrame.Minutes)\n",
    "                    cerebro1.adddata(feed1)\n",
    "                    cerebro1.broker.set_slippage_fixed(fixed=dict_map[model_Y][5], slip_open=True, slip_limit=True, slip_match=True, slip_out=True)\n",
    "                    cerebro1.broker.setcommission(margin=0.00001, mult=1)\n",
    "                    cerebro1.broker.addcommissioninfo(FixedComm(commission=dict_map[model_Y][6]))\n",
    "                    cerebro1.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trade_analyzer')\n",
    "                    results1 = cerebro1.run()\n",
    "                    \n",
    "                    cerebro2 = bt.Cerebro()\n",
    "                    cerebro2.addstrategy(TStrategy, cheat_on_close=True, entry_zscore = strategy_zscore_entry, exit_zscore = strategy_zscore_exit,\\\n",
    "                                         carry = dict_map[model_X[0]][3], product = model_X[0])\n",
    "                    cerebro2.broker.setcash(1e12)\n",
    "                    feed2 = MyData(dataname=dfx.copy(), timeframe=bt.TimeFrame.Minutes)\n",
    "                    cerebro2.adddata(feed2)\n",
    "                    cerebro2.broker.set_slippage_fixed(fixed=dict_map[model_X[0]][5], slip_open=True, slip_limit=True, slip_match=True, slip_out=True)\n",
    "                    cerebro2.broker.setcommission(margin=0.00001, mult=1)\n",
    "                    cerebro1.broker.addcommissioninfo(FixedComm(commission=dict_map[model_X[0]][6]))\n",
    "                    cerebro2.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trade_analyzer')\n",
    "                    results2 = cerebro2.run()\n",
    "        \n",
    "                    if len(model_X) == 2:\n",
    "                        cerebro3 = bt.Cerebro()\n",
    "                        cerebro3.addstrategy(TStrategy, cheat_on_close=True, entry_zscore = strategy_zscore_entry, \\\n",
    "                                             exit_zscore = strategy_zscore_exit,\\\n",
    "                                             carry = dict_map[model_X[1]][3], product = model_X[1])\n",
    "                        cerebro3.broker.setcash(1e12)\n",
    "                        feed3 = MyData(dataname=dfx1.copy(), timeframe=bt.TimeFrame.Minutes)\n",
    "                        cerebro3.adddata(feed3)\n",
    "                        cerebro3.broker.set_slippage_fixed(fixed=dict_map[model_X[1]][5], slip_open=True, slip_limit=True, \\\n",
    "                                                           slip_match=True, slip_out=True)\n",
    "                        cerebro3.broker.setcommission(margin=0.00001, mult=1)\n",
    "                        cerebro1.broker.addcommissioninfo(FixedComm(commission=dict_map[model_X[1]][6]))\n",
    "                        cerebro3.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trade_analyzer')\n",
    "                        results3 = cerebro3.run()\n",
    "        \n",
    "                    \n",
    "                    strat1 = results1[0]\n",
    "                    dates1, values1 = zip(*strat1.portfolio_values)\n",
    "                    \n",
    "                    strat2 = results2[0]\n",
    "                    dates2, values2 = zip(*strat2.portfolio_values)\n",
    "        \n",
    "                    if len(model_X) == 2:\n",
    "                        strat3 = results3[0]\n",
    "                        dates3, values3 = zip(*strat3.portfolio_values)\n",
    "                    \n",
    "                    # clear_output(wait=False)\n",
    "                    \n",
    "                    \n",
    "                    #############################################################################################################################\n",
    "                    \n",
    "                    ############################################### Basic PX based P/L\n",
    "                    try:\n",
    "                        dates3, values3 = zip(*strat1.daily_pnl)\n",
    "                        d3 = pd.DataFrame({ 'CDX':list(values3)}, index = list(dates3))\n",
    "                        dates3, values3 = zip(*strat2.daily_pnl)\n",
    "                        d4 = pd.DataFrame({ 'SPX':list(values3)}, index = list(dates3))\n",
    "            \n",
    "                        if len(model_X) == 1:\n",
    "                            d5 = pd.concat([d3,d4],axis=1).dropna().copy()\n",
    "                            d5['Daily P/L'] = d5['CDX'] + d5['SPX']\n",
    "                        \n",
    "                        elif len(model_X) == 2:\n",
    "                            dates3, values3 = zip(*strat3.daily_pnl)\n",
    "                            d41 = pd.DataFrame({ 'SPX2':list(values3)}, index = list(dates3))\n",
    "                            d5 = pd.concat([d3,d4,d41],axis=1).dropna().copy()\n",
    "                            d5['Daily P/L'] = d5['CDX'] + d5['SPX'] + d5['SPX2']\n",
    "            \n",
    "                        d5 = round(d5[['Daily P/L']].astype(float),2)\n",
    "                        daily_pnl = d5.copy()\n",
    "            \n",
    "                        ############################################### Removing Roll basic PX P/L\n",
    "                        try:\n",
    "                            dates3, values3 = zip(*strat1.roll_trades)\n",
    "                            d3 = pd.DataFrame({ 'CDX':list(values3)}, index = list(dates3))\n",
    "                            dates3, values3 = zip(*strat2.roll_trades)\n",
    "                            d4 = pd.DataFrame({ 'SPX':list(values3)}, index = list(dates3))\n",
    "            \n",
    "                            if len(model_X) == 1:\n",
    "                                d5 = pd.concat([d3,d4],axis=1).dropna().copy()\n",
    "                                d5['Roll P/L'] = d5['CDX'] + d5['SPX']\n",
    "                            \n",
    "                            elif len(model_X) == 2:\n",
    "                                dates3, values3 = zip(*strat3.roll_trades)\n",
    "                                d41 = pd.DataFrame({ 'SPX2':list(values3)}, index = list(dates3))\n",
    "                                d5 = pd.concat([d3,d4,d41],axis=1).dropna().copy()\n",
    "                                d5['Roll P/L'] = d5['CDX'] + d5['SPX'] + d5['SPX2']\n",
    "            \n",
    "                            d5 = -1*round(d5[['Roll P/L']].astype(float),2)\n",
    "                            roll_trades = d5.copy()\n",
    "                        except Exception as e:\n",
    "                            roll_trades = pd.DataFrame()\n",
    "            \n",
    "                        ############################################### Adding all trades carry\n",
    "                        try:\n",
    "                            dates3, values3 = zip(*strat1.carry)\n",
    "                            d3 = pd.DataFrame({ 'CDX':list(values3)}, index = list(dates3))\n",
    "                            dates3, values3 = zip(*strat2.carry)\n",
    "                            d4 = pd.DataFrame({ 'SPX':list(values3)}, index = list(dates3))\n",
    "            \n",
    "                            if len(model_X) == 1:\n",
    "                                d5 = pd.concat([d3,d4],axis=1).dropna().copy()\n",
    "                                d5['CDX Carry'] = d5['CDX'] + d5['SPX']\n",
    "                                \n",
    "                            elif len(model_X) == 2:\n",
    "                                dates3, values3 = zip(*strat3.carry)\n",
    "                                d41 = pd.DataFrame({ 'SPX2':list(values3)}, index = list(dates3))\n",
    "                                d5 = pd.concat([d3,d4,d41],axis=1).dropna().copy()\n",
    "                                d5['CDX Carry'] = d5['CDX'] + d5['SPX'] + d5['SPX2']\n",
    "            \n",
    "                            d5 = round(d5[['CDX Carry']].astype(float),2)\n",
    "                            carry = d5.copy()\n",
    "                        except Exception as e:\n",
    "                            carry = pd.DataFrame()\n",
    "                        \n",
    "                        ############################################### Remove carry of roll trades\n",
    "                        \n",
    "                        try:\n",
    "                            dates3, values3 = zip(*strat1.roll_carry)\n",
    "                            d3 = pd.DataFrame({ 'CDX':list(values3)}, index = list(dates3))\n",
    "                            dates3, values3 = zip(*strat2.roll_carry)\n",
    "                            d4 = pd.DataFrame({ 'SPX':list(values3)}, index = list(dates3))\n",
    "            \n",
    "                            if len(model_X) == 1:\n",
    "                                d5 = pd.concat([d3,d4],axis=1).dropna().copy()\n",
    "                                d5['CDX Roll Carry'] = d5['CDX'] + d5['SPX']\n",
    "            \n",
    "                            elif len(model_X) == 2:\n",
    "                                dates3, values3 = zip(*strat3.roll_carry)\n",
    "                                d41 = pd.DataFrame({ 'SPX2':list(values3)}, index = list(dates3))\n",
    "                                d5 = pd.concat([d3,d4,d41],axis=1).dropna().copy()\n",
    "                                d5['CDX Roll Carry'] = d5['CDX'] + d5['SPX'] + d5['SPX2']\n",
    "            \n",
    "                            d5 = -1*round(d5[['CDX Roll Carry']].astype(float),2)\n",
    "                            roll_carry = d5.copy()\n",
    "                        except Exception as e:\n",
    "                            roll_carry = pd.DataFrame()\n",
    "            \n",
    "                        ############################################### Add funding of all trades\n",
    "                        \n",
    "                        try:\n",
    "                            dates3, values3 = zip(*strat1.funding)\n",
    "                            d3 = pd.DataFrame({ 'CDX':list(values3)}, index = list(dates3))\n",
    "                            dates3, values3 = zip(*strat2.funding)\n",
    "                            d4 = pd.DataFrame({ 'SPX':list(values3)}, index = list(dates3))\n",
    "            \n",
    "                            if len(model_X) == 1:\n",
    "                                d5 = pd.concat([d3,d4],axis=1).dropna().copy()\n",
    "                                d5['Funding'] = d5['CDX'] + d5['SPX']\n",
    "            \n",
    "                            elif len(model_X) == 2:\n",
    "                                dates3, values3 = zip(*strat3.funding)\n",
    "                                d41 = pd.DataFrame({ 'SPX2':list(values3)}, index = list(dates3))\n",
    "                                d5 = pd.concat([d3,d4,d41],axis=1).dropna().copy()\n",
    "                                d5['Funding'] = d5['CDX'] + d5['SPX'] + d5['SPX2']\n",
    "            \n",
    "                            d5 = round(d5[['Funding']].astype(float),2)\n",
    "                            funding = d5.copy()\n",
    "                        except Exception as e:\n",
    "                            funding = pd.DataFrame()\n",
    "                            \n",
    "                        ############################################### Remove funding of carry trades\n",
    "                        \n",
    "                        try:\n",
    "                            dates3, values3 = zip(*strat1.funding_carry)\n",
    "                            d3 = pd.DataFrame({ 'CDX':list(values3)}, index = list(dates3))\n",
    "                            dates3, values3 = zip(*strat2.funding_carry)\n",
    "                            d4 = pd.DataFrame({ 'SPX':list(values3)}, index = list(dates3))\n",
    "            \n",
    "                            if len(model_X) == 1:\n",
    "                                d5 = pd.concat([d3,d4],axis=1).dropna().copy()\n",
    "                                d5['funding_carry'] = d5['CDX'] + d5['SPX']\n",
    "            \n",
    "                            elif len(model_X) == 2:\n",
    "                                dates3, values3 = zip(*strat3.funding_carry)\n",
    "                                d41 = pd.DataFrame({ 'SPX2':list(values3)}, index = list(dates3))\n",
    "                                d5 = pd.concat([d3,d4,d41],axis=1).dropna().copy()\n",
    "                                d5['funding_carry'] = d5['CDX'] + d5['SPX'] + d5['SPX2']\n",
    "            \n",
    "                            d5 = -1*round(d5[['funding_carry']].astype(float),2)\n",
    "                            funding_carry = d5.copy()\n",
    "                        except Exception as e:\n",
    "                            funding_carry = pd.DataFrame()\n",
    "            \n",
    "                        #############################################################################################################################\n",
    "            \n",
    "                        dates3, values3 = zip(*strat1.trade_direction)\n",
    "                        long_short_ind = pd.DataFrame({ 'Ind':list(values3)}, index = list(dates3))\n",
    "                        \n",
    "                        bt_df = pd.concat([dfy[['volume']], daily_pnl, roll_trades, carry, roll_carry, funding, funding_carry],axis=1)\n",
    "                        bt_df = bt_df.iloc[:,1:]\n",
    "                        bt_df = bt_df.fillna(0.0)\n",
    "\n",
    "\n",
    "                        ####### Keep commented to adjust for funding; if no funding then remove the hash    \n",
    "                        ####### these are when want to tally with old model outputs                        \n",
    "                        # bt_df['Funding'] = [0.0] * len(bt_df)\n",
    "                        # bt_df['funding_carry'] = [0.0] * len(bt_df)\n",
    "                        \n",
    "                        bt_df_backup = bt_df.copy()\n",
    "                        \n",
    "                        for trade_btdf_direction in ['Long/Short']: #'Long','Short',\n",
    "                            bt_df = bt_df_backup.copy()\n",
    "                            \n",
    "                            trade_check = None if trade_btdf_direction == 'Long/Short' else trade_btdf_direction\n",
    "                            \n",
    "                            bt_df['Sum'] = bt_df.sum(axis=1)\n",
    "                            x = pd.concat([bt_df, long_short_ind],axis=1).copy()\n",
    "                            x = x[x['Ind']!=trade_check].drop(\"Ind\",axis=1).copy()   ###### Use not equal operator\n",
    "                            x = pd.concat([dfy[['volume']],x],axis=1).copy()\n",
    "                            x = x.iloc[:,1:].fillna(0.0)\n",
    "                            bt_df = x.copy()\n",
    "                            trade_num = len(bt_df[bt_df['Sum']!=0])\n",
    "                            bt_df = bt_df.drop(\"Sum\",axis=1)\n",
    "            \n",
    "            \n",
    "                            ###########################################################################################################################\n",
    "            \n",
    "                            sr = bt_df.copy()\n",
    "                            sr['Sum'] = sr.sum(axis=1)\n",
    "                            sr = sr[['Sum']]\n",
    "                            sr = sr.cumsum().resample(\"D\").last().dropna().copy()\n",
    "                            sr += 10**7\n",
    "                            sr = sr.pct_change()\n",
    "                            sr = round((252**0.5*sr.mean()/sr.std()).iloc[0],3)\n",
    "                            \n",
    "                            globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_$pnl'].\\\n",
    "                            loc[strategy_zscore_entry,strategy_zscore_exit] = round(bt_df.sum().sum(),0)\n",
    "                            globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_$pnl/trade'].\\\n",
    "                            loc[strategy_zscore_entry,strategy_zscore_exit] = round(bt_df.sum().sum()/trade_num,0)\n",
    "                            globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_SR'].\\\n",
    "                            loc[strategy_zscore_entry,strategy_zscore_exit] = round(sr,2)\n",
    "                            globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_trades'].\\\n",
    "                            loc[strategy_zscore_entry,strategy_zscore_exit] = trade_num\n",
    "            \n",
    "                            bt_df['Sum'] = bt_df.sum(axis=1)\n",
    "                            pos = len(bt_df[bt_df['Sum']>0])\n",
    "                            neg = len(bt_df[bt_df['Sum']<0])\n",
    "                            try:\n",
    "                                hit = round((pos/(pos+neg))*100,0)\n",
    "                            except:\n",
    "                                hit = 0\n",
    "                            globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_Hit Ratio'].\\\n",
    "                            loc[strategy_zscore_entry,strategy_zscore_exit] = round(hit,0)\n",
    "                \n",
    "                            max_dd = bt_df[['Sum']].cumsum().copy()\n",
    "                            max_dd['Roll Max'] = max_dd[['Sum']].rolling(window=10000000, min_periods=1).max()\n",
    "                            max_dd['Diff'] = abs(max_dd['Roll Max'] - max_dd['Sum'])\n",
    "                            globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_max DD'].\\\n",
    "                            loc[strategy_zscore_entry,strategy_zscore_exit] = round(-1*max_dd['Diff'].max(),0)\n",
    "                            \n",
    "                            ############################### Plots\n",
    "                            \n",
    "                            dates3, values3 = zip(*strat1.scatter_plot_trade_pnl)\n",
    "                            d3 = pd.DataFrame({ 'CDX':list(values3)}, index = list(dates3))\n",
    "                            dates3, values3 = zip(*strat2.scatter_plot_trade_pnl)\n",
    "                            d4 = pd.DataFrame({ 'SPX':list(values3)}, index = list(dates3))\n",
    "                \n",
    "                            if len(model_X) == 1:\n",
    "                                d5 = pd.concat([d3,d4],axis=1).dropna().copy()\n",
    "                \n",
    "                            elif len(model_X) == 2:\n",
    "                                dates3, values3 = zip(*strat3.scatter_plot_trade_pnl)\n",
    "                                d41 = pd.DataFrame({ 'SPX2':list(values3)}, index = list(dates3))\n",
    "                                d5 = pd.concat([d3,d4,d41],axis=1).dropna().copy()\n",
    "                            \n",
    "                            scatter = pd.concat([bt_df[['Sum']],d5['CDX']],axis=1)\n",
    "                            # scatter = pd.concat([bt_df[['Sum']],d3['CDX']],axis=1)\n",
    "                            x = scatter[scatter['Sum']!=0.0]['CDX'].copy()\n",
    "                            bar_size = sampling_multiplier if dict_models[model_num][0] == 'Intraday' else np.nan\n",
    "                            \n",
    "                            globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_days/trade'].\\\n",
    "                            loc[strategy_zscore_entry,strategy_zscore_exit] = round(x.sum()/(bar_size*len(x)),1)\n",
    "                            \n",
    "                            title = f\"{global_model}; Model {dict_models[model_num][-1]}; {strategy_zscore_entry} entry; {trade_btdf_direction} direction\"\n",
    "                            title += f\" & {strategy_zscore_exit} exit; P/L: {bt_df[['Sum']].cumsum().iloc[-1,0]:.0f}; SR: {sr:.2f}\"\n",
    "                            title += f\" & Diff: {str(diff_period_list)}\"\n",
    "                            \n",
    "                            dates3, values3 = zip(*strat1.scatter_plot_trade_pnl)\n",
    "                            d3 = pd.DataFrame({ 'CDX':list(values3)}, index = list(dates3))\n",
    "                            dates3, values3 = zip(*strat2.scatter_plot_trade_pnl)\n",
    "                            d4 = pd.DataFrame({ 'SPX':list(values3)}, index = list(dates3))\n",
    "                            \n",
    "                            if (model_X) == 1:\n",
    "                                d5 = pd.concat([d3,d4],axis=1).dropna().copy()\n",
    "                \n",
    "                            elif len(model_X) == 2:\n",
    "                                dates3, values3 = zip(*strat3.scatter_plot_trade_pnl)\n",
    "                                d41 = pd.DataFrame({ 'SPX2':list(values3)}, index = list(dates3))\n",
    "                                d5 = pd.concat([d3,d4,d41],axis=1).dropna().copy()\n",
    "                            \n",
    "                            d5 = d5/bar_size\n",
    "                            scatter = pd.concat([bt_df[['Sum']],d5['CDX']],axis=1)\n",
    "                            scatter = scatter[scatter['Sum']!=0.0]\n",
    "                            plt.figure(figsize=(12,6))\n",
    "                            plt.scatter(scatter['CDX'], scatter['Sum'],label=\"Per Trade P/L\")\n",
    "                            plt.ylabel(\"Trade $P/L\")\n",
    "                            plt.xlabel(\"Trade Duration in Days\")\n",
    "                            plt.title(title)\n",
    "                            plt.legend()\n",
    "                            plt.savefig(f\"Plots/Scatter/{title.replace(\";\",\"_\").replace(\"/\",\"_\").replace(\"&\",\"_\").replace(\":\",\"_\")}.png\")\n",
    "                            # plt.show()\n",
    "                            plt.close()\n",
    "                             \n",
    "                            bt_df['Sum'].cumsum().plot(label=\"Cum. P/L\", figsize=(12,6))\n",
    "                            plt.title(title)\n",
    "                            plt.legend()\n",
    "                            plt.savefig(f\"Plots/PL/{title.replace(\";\",\"_\").replace(\"/\",\"_\").replace(\"&\",\"_\").replace(\":\",\"_\")}.png\")\n",
    "                            # plt.show()\n",
    "                            plt.close()\n",
    "                            ############################### Plots\n",
    "                    except:\n",
    "                        continue\n",
    "        \n",
    "        display(dfx.iloc[[0,-1],:])\n",
    "        \n",
    "        for trade_btdf_direction in ['Long/Short']: #'Long','Short',\n",
    "            print(f'global model is {global_model}')\n",
    "            print(f'model_num is {model_num}')\n",
    "            print(f'trade direction is {trade_btdf_direction}')\n",
    "            for info in ['$pnl','$pnl/trade','SR','Hit Ratio','trades','days/trade','max DD']:\n",
    "                display(globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_{info}'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
