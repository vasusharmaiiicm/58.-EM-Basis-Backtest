{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38e670e7-6fdc-4f86-888b-69ffbe02ca11",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "import backtrader as bt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pytz\n",
    "import time\n",
    "import os\n",
    "from xbbg import blp\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import interact, Dropdown, HBox, VBox, Button, Output, Text, widgets\n",
    "import sympy as sp\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "from IPython import get_ipython\n",
    "import matplotlib.dates as mdates\n",
    "from pydataquery import DataQuery\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "from scipy.optimize import minimize\n",
    "import scipy.stats as stats\n",
    "import itertools\n",
    "import warnings\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import yfinance as yf\n",
    "import csv\n",
    "import uuid\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import warnings\n",
    "from multiprocess import Pool\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e88dc18-fb8f-4b5c-942a-22eb26ce8d8e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #ER Code\n",
    "# ####################################################\n",
    "\n",
    "# all_start_date = str((datetime.now()-timedelta(days=13*365+1)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "# labels = {\n",
    "#         \"LQD Funding Sprd\": \"DB(CDS,TRAC-X,EUROPEIBXTRS_USDLIG_3M,JPM_IMPLIEDFUNDING_MID)\",\n",
    "#         \"HYG Funding Sprd\": \"DB(CDS,TRAC-X,EUROPEIBXTRS_USDHY_3M,JPM_IMPLIEDFUNDING_MID)\",\n",
    "#         \"IEAC Funding Sprd\": \"DB(CDS,TRAC-X,EUROPEIBXTRS_EURIG_3M,JPM_IMPLIEDFUNDING_MID)\",\n",
    "#         \"IHYG Funding Sprd\": \"DB(CDS,TRAC-X,EUROPEIBXTRS_EURHY_3M,JPM_IMPLIEDFUNDING_MID)\",\n",
    "#         \"Fed Fund\": \"FF\",\n",
    "#         \"ER CDX IG 5Y\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_RETURN)\",\n",
    "#         \"ER CDX IG 10Y\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_RETURN)\",\n",
    "#         \"ER CDX HY 5Y\": \"DB(CDS,TRAC-X,NAHY100UNF05ONRUN,JPM_RETURN)\",\n",
    "#         \"ER CDX HY 10Y\": \"DB(CDS,TRAC-X,NAHY100UNF10ONRUN,JPM_RETURN)\",\n",
    "#         \"ER CDX EM 5Y\": \"DB(NEO-UK,credit/cds/index/cdxEM-onrun/5y/JPM_RETURN)\",\n",
    "#         \"ER ITRX MAIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "#         \"ER ITRX MAIN 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_UNFUNDED_INDEX)\",\n",
    "#         \"ER ITRX XOVER 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "#         \"ER ITRX SNRFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "#         \"ER ITRX SUBFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "# }\n",
    "\n",
    "# dq = DataQuery(\n",
    "# client_id='jbAIMF2Tkp0JO3sc',\n",
    "# client_secret='d7qfzgt55pddjs352sgxosFyI4t2eja07k7opbi6wg9oqjc1OjkdAksn1btmnugeMjchcx2vwTsJupw',\n",
    "# )\n",
    "\n",
    "# job = dq.create_job(expressions = list(labels.values()))\n",
    "# dq.start_date = all_start_date\n",
    "# var = job.execute()\n",
    "# df = job.to_pivot_table()\n",
    "# df = df.T\n",
    "# df.index = pd.to_datetime(df.index, format='%Y%m%d').date\n",
    "# df.index.name = 'Date'\n",
    "\n",
    "# df.rename(columns={v:k for k, v in labels.items()},inplace=True)\n",
    "# df.columns.name = None\n",
    "# clear_output(wait=False)\n",
    "# df = df.dropna(how='all')\n",
    "# df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# dq = df.copy()\n",
    "\n",
    "# end_date = dq.index[-1]\n",
    "# ####################################### BBG Data Acquisition\n",
    "\n",
    "# securities = ['HYG US Equity','EMB US Equity','LQD US Equity','VCIT US Equity',\n",
    "#               'IEAC LN Equity','IHYG LN EQUITY', 'BKLN US EQUITY', 'IBCN GR EQUITY',\n",
    "#               'IEI US Equity','IEF US Equity']\n",
    "\n",
    "# fields1 = ['YAS_MOD_DUR']\n",
    "# df = blp.bdh(tickers=securities, start_date = all_start_date, end_date = end_date, flds = fields1)\n",
    "# df.columns = [item.split(' ')[0] + ' DUR' for item in securities]\n",
    "# df1 = df.copy()\n",
    "\n",
    "# #################################### Fixing Bad Data Point in YAS of IEI\n",
    "# rolling_avg = df1['IEI DUR'].replace(0, np.nan).rolling(window=30, min_periods=1).mean()\n",
    "# df1['IEI DUR'] = df1.apply(\n",
    "#     lambda row: rolling_avg[row.name] if row['IEI DUR'] == 0.0 else row['IEI DUR'], axis=1\n",
    "# )\n",
    "# #################################### Fixing Bad Data Point in YAS of IEI\n",
    "\n",
    "# securities = ['LT03TRUU INDEX','LT09TRUU INDEX','QW3I INDEX', 'LT03MD INDEX','LT09MD INDEX']\n",
    "# fields = ['PX_LAST']\n",
    "# df = blp.bdh(tickers=securities, start_date = all_start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = ['TR ' + item.split(' ')[0] for item in securities[:3]] + [item.split(' ')[0] + ' DUR' for item in securities[:2]]\n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# securities = ['HYG US Equity','EMB US Equity','LQD US Equity','VCIT US Equity',\n",
    "#               'IEI US Equity','IEF US Equity', 'RSP US EQUITY', #'SPX INDEX',  'RTY INDEX',\n",
    "#               'IBCN GR EQUITY',\n",
    "#               'IEAC LN Equity','IHYG LN EQUITY', 'BKLN US EQUITY', 'IWM US EQUITY',\n",
    "#               'GSCBHYEQ Index', 'GSCBIGEQ Index', 'SPY US EQUITY', 'EEM US EQUITY','IJH US EQUITY'\n",
    "#              ]\n",
    "\n",
    "# fields = ['TOT_RETURN_INDEX_GROSS_DVDS']\n",
    "# df = blp.bdh(tickers=securities, start_date = all_start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = ['TR ' + item.split(' ')[0] for item in securities] \n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# securities = ['QW3I INDEX']\n",
    "# fields = ['MODIFIED_DURATION']\n",
    "# df = blp.bdh(tickers=securities, start_date = all_start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = [item.split(' ')[0] + ' DUR' for item in securities]\n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# # securities = ['SPXFP INDEX', 'RTYFPE INDEX','SX5EFSER Index']  ############## I want to calculate funding rate for spx, rty and sx5e separately\n",
    "# # fields = ['PX_LAST']\n",
    "# # df = blp.bdh(tickers=securities, start_date = all_start_date, end_date = end_date, flds = fields)\n",
    "# # df.columns = ['ER SPX','ER RTY','ER SX5E']\n",
    "# # df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# securities = ['EURR002W Index']\n",
    "# fields = ['PX_LAST']\n",
    "# df = blp.bdh(tickers=securities, start_date = all_start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = ['ECB Rate']\n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# bbg = df1.copy()\n",
    "# dq.index = pd.to_datetime(dq.index)\n",
    "# dq.index = dq.index.date\n",
    "# bbg.index = pd.to_datetime(bbg.index)\n",
    "# bbg.index = bbg.index.date\n",
    "\n",
    "# data = pd.concat([dq,bbg],axis=1)\n",
    "# data = data.sort_index()\n",
    "\n",
    "# df_funding = data[[col for col in data.columns if ('Funding Sprd' in col)]+['Fed Fund']+['ECB Rate']]\n",
    "\n",
    "# if np.isnan(df_funding.loc[df_funding.index[-1],'Fed Fund']):\n",
    "#     df_funding.loc[df_funding.index[-1],'Fed Fund'] = df_funding.loc[df_funding.index[-2],'Fed Fund']\n",
    "\n",
    "# for col in df_funding:\n",
    "#     if col.endswith('Sprd'):\n",
    "#         if col.split(' ')[0] in ['HYG','LQD']:\n",
    "#             df_funding[f'Net Long {col.replace(\" Sprd\",\"\")}'] = (df_funding['Fed Fund'] + df_funding[f'{col}']/100) + 0.25/100\n",
    "#             df_funding[f'Net Short {col.replace(\" Sprd\",\"\")}'] = (df_funding['Fed Fund'] + df_funding[f'{col}']/100) - 0.25/100\n",
    "#         if col.split(' ')[0] in ['IHYG','IEAC']:\n",
    "#             df_funding[f'Net Long {col.replace(\" Sprd\",\"\")}'] = (df_funding['ECB Rate'] + df_funding[f'{col}']/100) + 0.25/100\n",
    "#             df_funding[f'Net Short {col.replace(\" Sprd\",\"\")}'] = (df_funding['ECB Rate'] + df_funding[f'{col}']/100) - 0.25/100\n",
    "\n",
    "# df_funding['Net Long VCIT Funding'] = df_funding['Net Long LQD Funding']\n",
    "# df_funding['Net Short VCIT Funding'] = df_funding['Net Short LQD Funding']\n",
    "\n",
    "# for item in ['EMB','EEM']:\n",
    "#     df_funding[f'Net Long {item} Funding'] = df_funding['Fed Fund'] + 0.5\n",
    "#     df_funding[f'Net Short {item} Funding'] = df_funding['Fed Fund'] - 0.5\n",
    "\n",
    "# for item in ['IEI', 'IEF', 'RSP', 'BKLN', 'GSCBHYEQ', 'GSCBIGEQ', 'SPX', 'RTY', 'SPY', 'IJH','IWM']:\n",
    "#     df_funding[f'Net Long {item} Funding'] = df_funding['Fed Fund'] + 0.15\n",
    "#     df_funding[f'Net Short {item} Funding'] = df_funding['Fed Fund'] - 0.15\n",
    "\n",
    "# for item in ['IBCN','SX5E']:\n",
    "#     df_funding[f'Net Long {item} Funding'] = df_funding['ECB Rate'] + 0.15\n",
    "#     df_funding[f'Net Short {item} Funding'] = df_funding['ECB Rate'] - 0.15\n",
    "\n",
    "# df_funding = df_funding[[col for col in df_funding.columns if col.startswith(\"Net\")]]\n",
    "# df_funding.index = pd.to_datetime(df_funding.index)\n",
    "# df_funding = df_funding.resample('D').last()\n",
    "\n",
    "# original_er_data = data[[col for col in data.columns if col.startswith(\"ER \")]]\n",
    "# tr_data = data[[col for col in data.columns if col.startswith(\"TR \")]]\n",
    "# ust = tr_data[['TR LT09TRUU']] # for using corr later\n",
    "# tr_data = tr_data.iloc[:,:-3] #dropping LT03/09 and QW3I\n",
    "\n",
    "# tr_data.index = pd.to_datetime(tr_data.index).date\n",
    "# df_funding.index = pd.to_datetime(df_funding.index).date\n",
    "\n",
    "# er_tr_data = pd.concat([tr_data,df_funding],axis=1)\n",
    "# er_tr_data = er_tr_data.sort_index()\n",
    "# # er_tr_data = er_tr_data.dropna()\n",
    "\n",
    "# etfs = [col for col in er_tr_data.columns if col.startswith(\"TR \")]\n",
    "\n",
    "# for item in etfs:\n",
    "#     er_tr_data[item] = er_tr_data[item].diff()/er_tr_data[item].shift()\n",
    "\n",
    "# er_tr_data['Date'] = pd.to_datetime(er_tr_data.index)\n",
    "# er_tr_data['Days'] = (er_tr_data['Date'] - er_tr_data['Date'].shift()).dt.days\n",
    "# # er_tr_data = er_tr_data.dropna()\n",
    "# er_tr_data\n",
    "\n",
    "# ############################################################### Funding Sprds\n",
    "# funding = er_tr_data[[col for col in er_tr_data.columns if 'Funding' in col]].copy()\n",
    "# x = er_tr_data[[col for col in er_tr_data.columns if 'Funding' in col]].copy()\n",
    "# x = x.interpolate()\n",
    "# x.to_excel(\"Funding Rates.xlsx\")\n",
    "\n",
    "# y = x.copy()\n",
    "# y = round(y,2)\n",
    "# y.to_excel(\"Funding Rates 2.xlsx\")\n",
    "\n",
    "# ###############################################################\n",
    "# for item in etfs:\n",
    "#     name = item.split(' ')[1]\n",
    "#     er_tr_data[f'ER {name}'] = er_tr_data[item] - \\\n",
    "#                 (1/100)*(er_tr_data['Days']/360)*(0.5*(er_tr_data[f'Net Long {name} Funding'] + er_tr_data[f'Net Short {name} Funding']))\n",
    "\n",
    "\n",
    "# er_tr_data = er_tr_data[[col for col in er_tr_data.columns if col.startswith(\"ER \")]]\n",
    "# er_tr_data = (1+er_tr_data).cumprod()\n",
    "\n",
    "# tr_data.index = pd.to_datetime(tr_data.index).date\n",
    "# df_funding.index = pd.to_datetime(df_funding.index).date\n",
    "\n",
    "# er_tr_data = pd.concat([tr_data,df_funding],axis=1)\n",
    "# er_tr_data = er_tr_data.sort_index()\n",
    "# # er_tr_data = er_tr_data.dropna()\n",
    "\n",
    "# etfs = [col for col in er_tr_data.columns if col.startswith(\"TR \")]\n",
    "\n",
    "# for item in etfs:\n",
    "#     er_tr_data[item] = er_tr_data[item].diff()/er_tr_data[item].shift()\n",
    "\n",
    "# er_tr_data['Date'] = pd.to_datetime(er_tr_data.index)\n",
    "# er_tr_data['Days'] = (er_tr_data['Date'] - er_tr_data['Date'].shift()).dt.days\n",
    "# # er_tr_data = er_tr_data.dropna()\n",
    "\n",
    "# for item in etfs:\n",
    "#     name = item.split(' ')[1]\n",
    "#     er_tr_data[f'ER {name}'] = er_tr_data[item] - \\\n",
    "#                 (1/100)*(er_tr_data['Days']/360)*(0.5*(er_tr_data[f'Net Long {name} Funding'] + er_tr_data[f'Net Short {name} Funding']))\n",
    "\n",
    "# er_tr_data = er_tr_data[[col for col in er_tr_data.columns if col.startswith(\"ER \")]]\n",
    "# er_tr_data = (1+er_tr_data).cumprod()\n",
    "\n",
    "# er_data = pd.concat([original_er_data,er_tr_data],axis=1)\n",
    "# # er_data = er_data.dropna()\n",
    "# # er_data.columns = er_data.columns.str.replace(\"ER SPX\",\"ER ESA\").str.replace(\"ER RTY\",\"ER RTYA\").str.replace(\"ER SX5E\",\"ER VGA\")\n",
    "# er_data.columns = er_data.columns.str.replace(\"ER GSCBHYEQ\",\"ER HY Eqty\").str.replace(\"ER GSCBIGEQ\",\"ER IG Eqty\")\n",
    "\n",
    "# securities = ['SPXFP INDEX', 'RTYFPE INDEX','SX5EFSER Index']\n",
    "# fields = ['PX_LAST']\n",
    "# df = blp.bdh(tickers=securities, start_date = er_data.index[0], end_date = er_data.index[-1], flds = fields)\n",
    "# df.columns = ['ER SPX','ER RTY','ER SX5E']\n",
    "# er_data = pd.concat([er_data,df], axis=1)\n",
    "# er_data = er_data.sort_index()\n",
    "\n",
    "# er_data.to_csv(\"All ER.csv\")\n",
    "\n",
    "# ##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa8138fe-14a2-49f3-bc17-33df5ce96e49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# all_start_date = str((datetime.now()-timedelta(days=13*365+1)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "# labels = {\n",
    "#     \"CDX IG 5Y\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_DUR)\",\n",
    "#     \"CDX IG 10Y\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_DUR)\",\n",
    "#     \"ITRX MAIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_DUR)\",\n",
    "#     \"ITRX MAIN 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_DUR)\",\n",
    "#     \"ITRX SNRFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_DUR)\",\n",
    "#     \"ITRX SUBFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_DUR)\",\n",
    "#     \"ITRX XOVER 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_DUR)\",\n",
    "#     \"ITRX XOVER 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/10y/JPM_DUR)\"\n",
    "# }\n",
    "\n",
    "# dq = DataQuery(\n",
    "# client_id='jbAIMF2Tkp0JO3sc',\n",
    "# client_secret='d7qfzgt55pddjs352sgxosFyI4t2eja07k7opbi6wg9oqjc1OjkdAksn1btmnugeMjchcx2vwTsJupw',\n",
    "# )\n",
    "\n",
    "# job = dq.create_job(expressions = list(labels.values()))\n",
    "# dq.start_date = all_start_date\n",
    "# var = job.execute()\n",
    "# df = job.to_pivot_table()\n",
    "# df = df.T\n",
    "# df.index = pd.to_datetime(df.index, format='%Y%m%d').date\n",
    "# df.index.name = 'Date'\n",
    "\n",
    "# df.rename(columns={v:k for k, v in labels.items()},inplace=True)\n",
    "# df.columns.name = None\n",
    "# clear_output(wait=False)\n",
    "# df = df.dropna(how='all')\n",
    "# df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# dur = df.copy()\n",
    "# dur.to_excel(\"DQ Dur.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90ef8ca-31ba-44e6-9e58-c130be84d3b7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# all_start_date = str((datetime.now()-timedelta(days=13*365+1)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "# labels = {\n",
    "#     \"CDX IG 5Y\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_CDSSPREAD_MID)\",\n",
    "#     \"CDX IG 10Y\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_CDSSPREAD_MID)\",\n",
    "#     \"CDX HY 5Y\": \"DB(CDS,TRAC-X,NAHY100UNF05ONRUN,JPM_CLEAN_MID)\",\n",
    "#     \"ITRX MAIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "#     \"ITRX MAIN 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_CDSSPREAD_MID)\",\n",
    "#     \"ITRX SNRFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "#     \"ITRX SUBFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "#     \"ITRX XOVER 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "#     \"ITRX XOVER 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/10y/JPM_CDSSPREAD_MID)\",\n",
    "#     \"CDX EM 5Y\": \"DB(NEO-UK,credit/cds/index/cdxEM-onrun/5y/JPM_CLEAN_MID)\",\n",
    "# }\n",
    "\n",
    "# dq = DataQuery(\n",
    "# client_id='jbAIMF2Tkp0JO3sc',\n",
    "# client_secret='d7qfzgt55pddjs352sgxosFyI4t2eja07k7opbi6wg9oqjc1OjkdAksn1btmnugeMjchcx2vwTsJupw',\n",
    "# )\n",
    "\n",
    "# job = dq.create_job(expressions = list(labels.values()))\n",
    "# dq.start_date = all_start_date\n",
    "# var = job.execute()\n",
    "# df = job.to_pivot_table()\n",
    "# df = df.T\n",
    "# df.index = pd.to_datetime(df.index, format='%Y%m%d').date\n",
    "# df.index.name = 'Date'\n",
    "\n",
    "# df.rename(columns={v:k for k, v in labels.items()},inplace=True)\n",
    "# df.columns.name = None\n",
    "# clear_output(wait=False)\n",
    "# df = df.dropna(how='all')\n",
    "# df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# dur = df.copy()\n",
    "# dur.to_excel(\"DQ Ref Levels_PX_Sprd.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e3e16cf-ed41-4ff1-8d21-947a7368e1fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_backup = pd.read_parquet(\"Clean 1min data.parquet\")\n",
    "# df = pd.read_excel(\"IJH Data.xlsx\")\n",
    "# all_x = None\n",
    "# for i in range(int(len(df.columns)))[::2]:\n",
    "#     x = df.iloc[:,i:i+2]\n",
    "#     x.columns = [\"Date\",\"IJH\"]\n",
    "#     all_x = pd.concat([all_x, x])\n",
    "# all_x = all_x.dropna()\n",
    "# all_x = all_x.drop_duplicates()\n",
    "# all_x[\"Date\"] = pd.to_datetime(all_x[\"Date\"])\n",
    "# all_x = all_x.set_index(\"Date\")\n",
    "# all_x = all_x.resample(\"1min\").last().ffill().copy()\n",
    "# all_x = all_x[all_x.index.isin(df_backup.index)].copy()\n",
    "\n",
    "# df = pd.concat([df_backup, all_x],axis=1)\n",
    "# df = df[list(set(df.columns))].copy()\n",
    "# df.to_parquet(\"Clean 1min data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d463e11-9e5a-41ad-a11d-dd701918653a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dict_map = {\n",
    "    # product type, start time, end time, carry (%), trades on sprd, slippage (bps or $),\n",
    "    # fixed commission, notional (if selected as Y; moved to model up look up!), BBG ticker\n",
    "    'CDX IG 5Y': ['CDX', '07:45:00', '16:30:00', 1, 'Yes', 0.15, 300, \"CDX IG CDSI GEN 5Y CORP\"],\n",
    "    'CDX IG 10Y': ['CDX', '07:45:00', '16:30:00', 1, 'Yes', 0.3, 300, \"CDX IG CDSI GEN 10Y CORP\"],\n",
    "    'CDX HY 5Y': ['CDX', '07:45:00', '16:30:00', 5, 'No', 0.02, 300, \"CDX HY CDSI GEN 5Y CORP\"],\n",
    "    'SPX': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"SPX INDEX\"],\n",
    "    'SPY': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"SPY US EQUITY\"],\n",
    "    'RSP': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"RSP US INDEX\"],    \n",
    "    'RTY': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"RTY INDEX\"],\n",
    "    'IG Eqty': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"GSCBIGEQ Index\"],\n",
    "    'HY Eqty': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"GSCBHYEQ Index\"],\n",
    "    'ITRX MAIN 5Y': ['CDX', '03:30:00', '11:59:00', 1, 'Yes', 0.15, 300, \"ITRX EUR CDSI GEN 5Y CORP\"],\n",
    "    'ITRX XOVER 5Y': ['CDX', '03:30:00', '11:59:00', 5, 'Yes', 0.15, 300, \"ITRX XOVER CDSI GEN 5Y CORP\"],\n",
    "    \n",
    "    'VIX': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"VIX INDEX\"],\n",
    "    'V2X': ['Eq', '03:30:00', '11:59:00', 0, 'No', 0.01, 0, \"V2X INDEX\"],\n",
    "    'SX5E': ['Eq', '03:30:00', '11:59:00', 0, 'No', 0.01, 0, \"SX5E INDEX\"],\n",
    "    'ITRX SNRFIN 5Y': ['CDX', '03:30:00', '11:59:00', 1, 'Yes', 0.15, 300, \"SNRFIN CDSI GEN 5Y CORP\"],\n",
    "    'ITRX SUBFIN 5Y': ['CDX', '03:30:00', '11:59:00', 1, 'Yes', 0.15, 300, \"SUBFIN CDSI GEN 5Y CORP\"],\n",
    "    'CDX EM 5Y': ['CDX', '07:45:00', '16:30:00', 1, 'No', 0.02, 300, \"CDX EM CDSI GEN 5Y CORP\"],\n",
    "\n",
    "    'HYG': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"HYG US EQUITY\"],\n",
    "    'EMB': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"EMB US EQUITY\"],\n",
    "    'EEM': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"EEM US EQUITY\"],\n",
    "    'VCIT': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"VCIT US EQUITY\"],\n",
    "    'LQD': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"LQD US EQUITY\"],\n",
    "    'IEI': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"IEI US EQUITY\"],\n",
    "    'IEF': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"IEF US EQUITY\"],\n",
    "    'IWM': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"IWM US EQUITY\"],\n",
    "    'IJH': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, \"IJH US EQUITY\"],\n",
    "}\n",
    "\n",
    "# er_data = pd.read_csv(\"All ER.csv\", index_col=0, parse_dates=True)\n",
    "# er_data.columns = [item.split(\" \",1)[1] for item in er_data.columns]\n",
    "\n",
    "# df_backup = pd.read_parquet(\"Clean 1min data.parquet\")\n",
    "\n",
    "# ##################################################################\n",
    "\n",
    "# bbg_tickers = [dict_map[item][7] for item in dict_map.keys()]\n",
    "# reverse_dict = dict(zip(bbg_tickers, list(dict_map.keys())))\n",
    "# bbg_data = blp.bdh(tickers = bbg_tickers, flds='px_last', start_date='2017-01-01')\n",
    "# bbg_data.columns = bbg_tickers\n",
    "# bbg_data.index = pd.to_datetime(bbg_data.index)\n",
    "# bbg_data.columns = [reverse_dict[item] for item in bbg_data.columns]\n",
    "# ref = pd.read_excel(\"DQ Ref Levels_PX_Sprd.xlsx\", index_col=0, parse_dates=True)\n",
    "\n",
    "# for col in ref.columns:\n",
    "#     bbg_data[col] = ref[col]\n",
    "\n",
    "# bbg_data1 = bbg_data.resample(\"1min\").last().ffill().copy()\n",
    "# bbg_data1.columns = [item +'_bbg_px' for item in bbg_data1.columns]\n",
    "\n",
    "# bbg_data2 = bbg_data.shift().resample(\"1min\").last().ffill().copy()\n",
    "# bbg_data2.columns = [item +'_bbg_px_2' for item in bbg_data2.columns]\n",
    "\n",
    "# ##################################################################\n",
    "\n",
    "# dur = pd.read_excel(\"DQ Dur.xlsx\",index_col=0, parse_dates=True)\n",
    "# dur = dur.shift().resample(\"1min\").last().ffill().copy()  ############ yesterday's duration we take .. we have shifted it here\n",
    "# dur.columns = [item + '_dq_dur' for item in dur.columns]\n",
    "\n",
    "# df = df_backup.copy()\n",
    "# er = er_data.copy()\n",
    "# er.columns = [item + '_dq_ER' for item in er.columns]\n",
    "# er = er.resample(\"1min\").last().ffill().copy()\n",
    "\n",
    "# er2 = er_data.shift().copy()\n",
    "# er2.columns = [item + '_dq_ER_2' for item in er2.columns]\n",
    "# er2 = er2.resample(\"1min\").last().ffill().copy()\n",
    "\n",
    "# ##################################################################\n",
    "# #### V. V. Imp: the dq close is as of 5PM and with bbg_data only till 4PM we don't really 'see' the BBG ER series match the DQ series\n",
    "\n",
    "# intraday_tr_data = None\n",
    "\n",
    "# for col in df.columns:\n",
    "#     # col = \"IEI\"\n",
    "#     if dict_map[col][4] == 'Yes':\n",
    "#         x = pd.concat([df[[col]], er[[f'{col}_dq_ER']], er2[[f'{col}_dq_ER_2']], bbg_data1[[f'{col}_bbg_px']], bbg_data2[[f'{col}_bbg_px_2']],\n",
    "#             dur[[f'{col}_dq_dur']]], axis=1).sort_index().dropna().copy()\n",
    "#         x['TR Change'] = (x[f'{col}_dq_ER'] / x[f'{col}_dq_ER_2'] - 1)\n",
    "#         x['d-o-d sprd pnl'] = (-1) * (x[f'{col}_dq_dur']) * (x[f'{col}_bbg_px'] - x[f'{col}_bbg_px_2']) * 10**(-4)\n",
    "#         x['intraday sprd pnl'] = (-1) * (x[f'{col}_dq_dur']) * (x[col] - x[f'{col}_bbg_px_2']) * 10**(-4)\n",
    "#         x['Calculated TR Change'] = x['TR Change'] - x['d-o-d sprd pnl'] + x['intraday sprd pnl']\n",
    "#         x['Actual TR Series'] = (1 + x['Calculated TR Change']) * x[f'{col}_dq_ER_2']\n",
    "#         x = x[['Actual TR Series']].copy()\n",
    "#         x.columns = [col]\n",
    "#     else:\n",
    "#         x = pd.concat([df[[col]], er[[f'{col}_dq_ER']], er2[[f'{col}_dq_ER_2']], bbg_data1[[f'{col}_bbg_px']], bbg_data2[[f'{col}_bbg_px_2']],\n",
    "#             ], axis=1).sort_index().dropna().copy()\n",
    "#         x['TR Change'] = (x[f'{col}_dq_ER'] / x[f'{col}_dq_ER_2'] - 1)\n",
    "#         if col in [\"CDX HY 5Y\", \"CDX HY 10Y\", \"CDX EM 5Y\"]:\n",
    "#             x['d-o-d px pnl'] = (x[f'{col}_bbg_px'] - x[f'{col}_bbg_px_2']) * 10**(-2)\n",
    "#             x['intraday px pnl'] = (x[col] - x[f'{col}_bbg_px_2']) * 10**(-2)\n",
    "#         else:  ### it is an etf\n",
    "#             x['d-o-d px pnl'] = (x[f'{col}_bbg_px']/ x[f'{col}_bbg_px_2'] - 1)\n",
    "#             x['intraday px pnl'] = (x[col] / x[f'{col}_bbg_px_2'] - 1)\n",
    "#         x['Calculated TR Change'] = x['TR Change'] - x['d-o-d px pnl'] + x['intraday px pnl']\n",
    "#         x['Actual TR Series'] = (1 + x['Calculated TR Change']) * x[f'{col}_dq_ER_2']\n",
    "#         x = x[['Actual TR Series']].copy()\n",
    "#         x.columns = [col]\n",
    "#     intraday_tr_data = pd.concat([intraday_tr_data, x], axis=1)\n",
    "\n",
    "# intraday_tr_data.to_parquet(\"1min ER series.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "455d0189-5320-4a2a-b5ec-c8aa732c626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_method = \"Rtn\" #\"Rtn\"\n",
    "diff_period_list = [1]\n",
    "\n",
    "models_list = [ \n",
    "    # model_Y, model_X (specify as a list) ### We trade these\n",
    "    # zscore_Y, zscore_X (specify as a list) ### We use these only for generating the zscore; names are taken from BBG datafile\n",
    "\n",
    "    # ['CDX HY 5Y', ['HYG','IEI'], 'CDX HY 5Y', ['HYG','IEI'], 7*10**6],\n",
    "    # ['CDX EM 5Y', ['EMB','IEF'], 'CDX EM 5Y', ['EMB','IEF'], 9*10**6],\n",
    "    # ['CDX IG 5Y', ['VCIT','IEF'], 'CDX IG 5Y', ['VCIT','IEF'], 25*10**6],\n",
    "    \n",
    "    # ['CDX IG 10Y', ['LQD','IEF'], 'CDX IG 10Y', ['LQD','IEF'], 14*10**6],\n",
    "    ['CDX IG 5Y', ['SPX'],'CDX IG 5Y', ['SPX'],28*10**6],\n",
    "    # # ['CDX IG 5Y', ['RSP'],'CDX IG 5Y', ['RSP'],28*10**6],\n",
    "    # ['CDX IG 5Y', ['IWM'],'CDX IG 5Y', ['IWM'],28*10**6],\n",
    "    # ['CDX IG 5Y', ['IJH'],'CDX IG 5Y', ['IJH'],28*10**6],\n",
    "    # # ['CDX IG 5Y', ['IG Eqty'],'CDX IG 5Y', ['IG Eqty'],28*10**6],\n",
    "    \n",
    "    # ['CDX HY 5Y', ['SPX'],'CDX HY 5Y', ['SPX'],6*10**6],\n",
    "    # # ['CDX HY 5Y', ['RTY'],'CDX HY 5Y', ['RTY'],6*10**6],\n",
    "    # # ['CDX HY 5Y', ['RSP'],'CDX HY 5Y', ['RSP'],6*10**6],\n",
    "    # ['CDX HY 5Y', ['IWM'],'CDX HY 5Y', ['IWM'],6*10**6],\n",
    "    # ['CDX HY 5Y', ['IJH'],'CDX HY 5Y', ['IJH'],6*10**6],\n",
    "    # # ['CDX HY 5Y', ['HY Eqty'],'CDX HY 5Y', ['HY Eqty'],6*10**6],\n",
    "    \n",
    "    # # ['ITRX MAIN 5Y', ['SX5E'],'ITRX MAIN 5Y', ['SX5E'], 28*10**6],\n",
    "    # # ['ITRX XOVER 5Y', ['SX5E'],'ITRX XOVER 5Y', ['SX5E'], 6*10**6],\n",
    "    # # ['ITRX XOVER 5Y', ['SPX'],'ITRX XOVER 5Y', ['SPX'], 6*10**6],\n",
    "    # # ['CDX IG 5Y', ['ITRX MAIN 5Y'],'CDX IG 5Y', ['ITRX MAIN 5Y'], 50*10**6],\n",
    "    \n",
    "    # # ['CDX HY 5Y', ['ITRX XOVER 5Y'],'CDX HY 5Y', ['ITRX XOVER 5Y'], 8*10**6],\n",
    "    # # ['ITRX MAIN 5Y', ['ITRX XOVER 5Y'],'ITRX MAIN 5Y', ['ITRX XOVER 5Y'], 66*10**6],\n",
    "    # # ['CDX IG 5Y', ['CDX HY 5Y'],'CDX IG 5Y', ['CDX HY 5Y'], 74*10**6],\n",
    "    \n",
    "    # # ['CDX IG 5Y', ['CDX EM 5Y'],'CDX IG 5Y', ['CDX EM 5Y'], 22*10**6],\n",
    "    # # ['CDX EM 5Y', ['CDX HY 5Y'],'CDX EM 5Y', ['CDX HY 5Y'], 8*10**6],\n",
    "    # # ['ITRX MAIN 5Y', ['CDX EM 5Y'],'ITRX MAIN 5Y', ['CDX EM 5Y'], 22*10**6],\n",
    "    # # ['CDX EM 5Y', ['ITRX XOVER 5Y'],'CDX EM 5Y', ['ITRX XOVER 5Y'], 6*10**6],\n",
    "\n",
    "    # # ['CDX IG 5Y', ['SPY'],'CDX IG 5Y', ['SPY'],28*10**6],\n",
    "    # # ['CDX HY 5Y', ['SPY'],'CDX HY 5Y', ['SPY'],6*10**6],\n",
    "    \n",
    "    # # ['EEM', ['EMB'], 'EEM', ['EMB'], 1*10**6],\n",
    "    # # ['CDX EM 5Y', ['EEM'], 'CDX EM 5Y', ['EEM'], 6*10**6],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b472e43d-b30b-419e-9d87-58036c6990a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dict_models = {\n",
    "    1 : [\"Intraday\",252,252,'A (Intraday; 12M)'],\n",
    "    # 2 : [\"Intraday\",315,315,'B (Intraday; 15M)'],\n",
    "    # 3 : [\"Intraday\",378,378,'C (Intraday; 18M)'],\n",
    "    # 4 : [\"Intraday\",504,504,'D (Intraday; 24M)'],\n",
    "}\n",
    "\n",
    "sampling_freq = '10min'\n",
    "\n",
    "for global_model in models_list:\n",
    "    for model_num in list(dict_models.keys()):\n",
    "        for trade_btdf_direction in ['Long','Short','Long/Short']:\n",
    "            for info in ['$pnl','$pnl/trade','SR','Hit Ratio','trades','days/trade','max DD']:\n",
    "                globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_{info}'] = pd.DataFrame()\n",
    "                globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_{info}'].index.name = 'Entry'\n",
    "                globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_{info}'].columns.name = info\n",
    "\n",
    "fund_rates = pd.read_excel(\"Funding Rates 2.xlsx\")\n",
    "fund_rates.columns = ['Date'] + list(fund_rates.columns)[1:]\n",
    "fund_rates['Date'] = pd.to_datetime(fund_rates['Date'])\n",
    "fund_rates = fund_rates.set_index('Date')\n",
    "fund_rates.columns = fund_rates.columns.str.replace(\"GSCB\",\"\").str.replace(\"EQ \",\" Eqty \").str.replace(\" Funding\",\"\").str.replace(\"Net \",\"\")\n",
    "etfs = list(set([item.split(\" \",1)[1] for item in fund_rates.columns]))\n",
    "\n",
    "def apply_funding(df_funding_update, variable_item):\n",
    "    if f'Long {variable_item}' in fund_rates.columns:\n",
    "        rate = fund_rates[[f'Long {variable_item}',f'Short {variable_item}']].dropna().resample(\"1min\").last().bfill().copy()\n",
    "        df_funding_update = pd.concat([df_funding_update, rate],axis=1)\n",
    "        df_funding_update = df_funding_update.dropna()\n",
    "        df_funding_update.columns = list(df_funding_update.columns)[:-2] + ['Long_Funding','Short_Funding']\n",
    "        df_funding_update['T'] = df_funding_update.index.date\n",
    "        df_funding_update['T-1'] = [np.nan] + list(df_funding_update.index[:-1].date)\n",
    "        df_funding_update['Funding_Date'] = df_funding_update.apply(lambda row: 'New Day' if row['T'] != row['T-1'] else 'Same Day', axis=1)\n",
    "        df_funding_update['Long_Funding'] = df_funding_update.apply(lambda row: row[\"Long_Funding\"] if row[\"Funding_Date\"] == \"New Day\" else 0,axis=1)\n",
    "        df_funding_update['Short_Funding'] = df_funding_update.apply(lambda row: row[\"Short_Funding\"] if row[\"Funding_Date\"] == \"New Day\" else 0,axis=1)\n",
    "        df_funding_update = df_funding_update.drop(['Funding_Date'],axis=1)\n",
    "        # df_funding_update['Funding'] = 0.5*(df_funding_update['Long_Funding'] + df_funding_update['Short_Funding'])\n",
    "        df_funding_update['T-1'] = [item.date() for item in pd.to_datetime(df_funding_update['T-1'])]\n",
    "        df_funding_update['T-1'].iloc[0] = df_funding_update['T'].iloc[0]\n",
    "        df_funding_update['Long Funding P/L'] = [item.days for item in (df_funding_update['T'] - df_funding_update['T-1'])]\n",
    "        df_funding_update['Long Funding P/L'] *= (df_funding_update['Long_Funding'] / 100) * (1/360) * abs(df_funding_update['Notional'])\n",
    "        df_funding_update['Short Funding P/L'] = [item.days for item in (df_funding_update['T'] - df_funding_update['T-1'])]\n",
    "        df_funding_update['Short Funding P/L'] *= (df_funding_update['Short_Funding'] / 100) * (1/360) * abs(df_funding_update['Notional'])\n",
    "        df_funding_update = df_funding_update.drop(['T','T-1','Long_Funding','Short_Funding'],axis=1)\n",
    "    else:\n",
    "        df_funding_update['Long Funding P/L'] = [0.0] * len(df_funding_update)\n",
    "        df_funding_update['Short Funding P/L'] = [0.0] * len(df_funding_update)\n",
    "\n",
    "    return df_funding_update.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cdaefd-6710-4936-9fda-1658fc19990c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# bbg_px = pd.read_parquet(\"Clean 1min data.parquet\")\n",
    "bbg_px = pd.read_excel(\"10min data with EUv8.xlsx\", index_col=0, parse_dates=True)\n",
    "\n",
    "if z_score_method == \"PX\":\n",
    "    zscore_calc_df = bbg_px.copy()\n",
    "elif z_score_method == \"Rtn\":\n",
    "    zscore_calc_df = pd.read_parquet(\"1min ER series.parquet\")   \n",
    "    # zscore_calc_df = pd.read_parquet(\"Intraday 1min ER series.parquet\")\n",
    "\n",
    "for global_model in models_list:\n",
    "    for model_num in list(dict_models.keys()):  \n",
    "\n",
    "        # global_model = models_list[0]\n",
    "        # model_num = list(dict_models.keys())[0]\n",
    "        \n",
    "        model_Y = global_model[0]\n",
    "        model_X = global_model[1]\n",
    "        zscore_Y = global_model[2]\n",
    "        zscore_X = global_model[3]\n",
    "        backtest_start_date = pd.to_datetime('2017-03-01')\n",
    "        notional_to_use = global_model[4]\n",
    "        \n",
    "        zscore_vars = [model_Y, zscore_Y] + model_X + zscore_X\n",
    "        \n",
    "        zscore_vars = list(set(zscore_vars))\n",
    "        zscore_vars_start_time = max([dict_map[item][1] for item in zscore_vars])\n",
    "        zscore_vars_end_time = min([dict_map[item][2] for item in zscore_vars])\n",
    "        \n",
    "        ################################## Beta Calculation\n",
    "        if len(model_X) == 1:\n",
    "            er_Y = f'ER {model_Y}'\n",
    "            er_X = f'ER {model_X[0]}'\n",
    "            er_data = pd.read_csv(\"All ER.csv\")\n",
    "            er_data.columns = ['Date'] + list(er_data.columns)[1:]\n",
    "            er_data['Date'] = pd.to_datetime(er_data['Date'])\n",
    "            er_data = er_data.set_index('Date')\n",
    "            er_data = er_data.sort_index()\n",
    "            beta = er_data[[er_Y, er_X]].dropna()\n",
    "            beta = beta.resample('W').last()\n",
    "            beta = np.log(beta)\n",
    "            beta = beta.diff().dropna()\n",
    "            beta['Beta1'] = [np.nan] * len(beta)\n",
    "            beta['Beta2'] = [np.nan] * len(beta)\n",
    "            \n",
    "            for i in range(len(beta)-24+1):\n",
    "                reg_X = beta[er_X].iloc[i:i+24]\n",
    "                reg_Y = beta[er_Y].iloc[i:i+24]\n",
    "                model = sm.OLS(reg_Y, sm.add_constant(reg_X)).fit() \n",
    "                beta.iloc[i+23,2] = model.params.iloc[1]\n",
    "            \n",
    "                model = sm.OLS(reg_X, sm.add_constant(reg_Y)).fit() \n",
    "                beta.iloc[i+23,3] = model.params.iloc[1]\n",
    "            \n",
    "            beta['Beta1'] = beta['Beta1'].rolling(104).mean()\n",
    "            beta['Beta2'] = beta['Beta2'].rolling(104).mean()\n",
    "            beta['Beta'] = 0.5*(beta['Beta1'] + 1/ beta['Beta2'])\n",
    "            beta = beta[['Beta']].dropna()\n",
    "            # beta = pd.read_excel(\"igspx_beta_1ymodel.xlsx\", index_col=0, parse_dates=True)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            b1 = pd.read_csv(\"All Basis Trade Betas.csv\")\n",
    "            b1.columns = ['Date'] + list(b1.columns)[1:]\n",
    "            b1 = b1.set_index('Date')\n",
    "            beta = b1[[f'{model_Y}_{model_X[0]}_{model_X[1]}']]\n",
    "            beta.columns = ['Beta']\n",
    "            beta['Coef1'] = beta['Beta'].apply(lambda x: eval(x)[0])\n",
    "            beta['Coef2'] = beta['Beta'].apply(lambda x: eval(x)[1])\n",
    "            beta.index = pd.to_datetime(beta.index)\n",
    "            \n",
    "        beta = beta.resample(\"1min\").first().ffill()\n",
    "        \n",
    "        ################################## BBG DataFile Intraday\n",
    "        # df = pd.read_excel(\"10min data with EUv7.xlsx\")\n",
    "        # df['Date'] = pd.to_datetime(df['Date'])\n",
    "        # df = df.set_index('Date')\n",
    "        # df = df.sort_index()\n",
    "        df = zscore_calc_df.copy()\n",
    "        \n",
    "        zscore_df = df[zscore_vars].between_time(zscore_vars_start_time, zscore_vars_end_time).dropna().copy()\n",
    "        zscore_df = zscore_df.resample(\"10min\",offset=\"5min\").last().dropna().copy()\n",
    "        zscore_df = zscore_df.between_time(zscore_vars_start_time, zscore_vars_end_time).dropna().copy()\n",
    "        bbg_datafile = zscore_df.copy()\n",
    "        \n",
    "        sampling_multiplier = len(set(list(bbg_datafile.index.time)))\n",
    "        \n",
    "        ################################## ZScore Calculation Start : Convert Sprd to PX series\n",
    "        \n",
    "        df = pd.read_excel(\"All DQ Duration.xlsx\")\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.set_index('Date')\n",
    "        df = df.sort_index()\n",
    "        df.columns = df.columns.str.replace(\" Dur\",\"\")\n",
    "        df = df.resample(\"1min\").first().ffill().dropna()\n",
    "        dq_dur = df.copy()\n",
    "        \n",
    "        zscore_df1 = zscore_df.copy()\n",
    "\n",
    "        if z_score_method == \"PX\":\n",
    "            for col in zscore_df1.columns:\n",
    "                if col in dq_dur.columns:\n",
    "                    zscore_df1[f'{col} Dur'] = dq_dur[col]\n",
    "                    zscore_df1[f'{col} Dur'] = zscore_df1[f'{col} Dur'].shift(1)\n",
    "                    zscore_df1[f'Diff {col}'] = zscore_df1[col].diff()\n",
    "                    zscore_df1 = zscore_df1.dropna()\n",
    "                    zscore_df1[f'{col} Daily PX Change'] = -1 * zscore_df1[f'Diff {col}'] * zscore_df1[f'{col} Dur'] *10**(-4)\n",
    "                    zscore_df1[f'{col} Sum PX'] = zscore_df1[f'{col} Daily PX Change'].cumsum()\n",
    "                    zscore_df1[col] = zscore_df1[f'{col} Sum PX']\n",
    "                    zscore_df1 = zscore_df1[zscore_df.columns].copy()\n",
    "        \n",
    "        ################################## ZScore Calculation: Differencing and converting to ZScores\n",
    "        \n",
    "        zscore_df = zscore_df1[zscore_df1.index >= backtest_start_date].copy()\n",
    "        \n",
    "        col_list = zscore_df.columns\n",
    "        for period in diff_period_list:\n",
    "            for col in col_list:\n",
    "                zscore_df[f'{col}_{period}W'] = zscore_df[col].diff(sampling_multiplier*5*period)\n",
    "                # zscore_df[f'{col}_{period}W'] = zscore_df[col].diff(sampling_multiplier*period)  ##### Daily\n",
    "        \n",
    "        model_lookback = sampling_multiplier*dict_models[model_num][1]\n",
    "        model_lookback_res = sampling_multiplier*dict_models[model_num][2]\n",
    "        zscore_df = zscore_df.dropna().copy()\n",
    "        zscore_df2 = zscore_df.copy()\n",
    "\n",
    "        for period in diff_period_list:\n",
    "            for i in range(len(zscore_df) - model_lookback + 1):\n",
    "        \n",
    "                reg_Y = zscore_df[[f'{zscore_Y}_{period}W']].iloc[i:i+model_lookback]\n",
    "                reg_X = zscore_df[[item + f\"_{period}W\" for item in zscore_X]].iloc[i:i+model_lookback]\n",
    "\n",
    "                model = sm.OLS(reg_Y,sm.add_constant(reg_X)).fit()\n",
    "                x = (model.resid - model.resid.rolling(model_lookback_res).mean())/model.resid.rolling(model_lookback_res).std()\n",
    "                zscore_df.loc[zscore_df.index[i+model_lookback-1],f'{period}W_ZScore'] = x.iloc[-1]\n",
    "\n",
    "        zscore_df['Avg. ZScore'] = zscore_df[[col for col in zscore_df.columns if col.endswith(\"_ZScore\")]].mean(axis=1)\n",
    "        zscore_df = zscore_df[['Avg. ZScore']]\n",
    "        bt_df = pd.concat([bbg_datafile[[model_Y] + model_X],zscore_df],axis=1).dropna()   \n",
    "        \n",
    "        #############################################\n",
    "\n",
    "        if z_score_method == \"Rtn\":\n",
    "            bbg_data = bbg_px.copy()\n",
    "            bbg_data = bbg_data[[model_Y] + model_X].dropna().copy()\n",
    "            bbg_data = bbg_data.resample(\"10min\", offset=\"5min\").last().ffill().copy()\n",
    "            bbg_data = bbg_data[bbg_data.index.isin(bt_df.index)]\n",
    "            for col in bbg_data.columns:\n",
    "                bt_df[col] = bbg_data[col]\n",
    "            \n",
    "        ############################# If dur > 0 => trades on sprd ; if dur = 0 => cdx trades on px ; if dur = -1 => eq. product trades on px\n",
    "        for col in bt_df.columns:\n",
    "            if col != \"Avg. ZScore\":\n",
    "                if col in dq_dur.columns:\n",
    "                    bt_df[f'{col} Dur'] = dq_dur[dq_dur.index.isin(bt_df.index)][col]\n",
    "                elif dict_map[col][0] == 'CDX':\n",
    "                    bt_df[f'{col} Dur'] = [0.0] * len(bt_df)\n",
    "                elif dict_map[col][0] == 'Eq':\n",
    "                    bt_df[f'{col} Dur'] = [-1.0] * len(bt_df)\n",
    "        bt_df['volume'] = [0.0] * len(bt_df)\n",
    "        \n",
    "        bt_df = bt_df[bt_df.index >= pd.to_datetime(\"2021-07-01\")].dropna()\n",
    "        bt_df = bt_df[bt_df.index <= pd.to_datetime(\"2025-04-26\")].dropna()\n",
    "        \n",
    "        ############################# Backtrader dfs\n",
    "        bt_df = pd.concat([bt_df,beta],axis=1).dropna()\n",
    "        bt_df.columns = bt_df.columns.str.replace(\"Beta\",\"Notional\")\n",
    "        \n",
    "        if len(model_X) == 1:\n",
    "            dfy = bt_df[[model_Y,f'{model_Y} Dur','Avg. ZScore','Notional','volume']].dropna().copy()\n",
    "            dfy.columns = dfy.columns.str.replace(\"Avg. ZScore\",\"Signal\").str.replace(f'{model_Y} Dur','Dur').str.replace(model_Y,\"close\")\n",
    "            dfy['Notional'] = notional_to_use\n",
    "            \n",
    "            dfx = bt_df[[model_X[0],f'{model_X[0]} Dur','Avg. ZScore','Notional','volume']].dropna().copy()\n",
    "            dfx.columns = dfx.columns.str.replace(\"Avg. ZScore\",\"Signal\").str.replace(f'{model_X[0]} Dur','Dur').str.replace(model_X[0],\"close\")\n",
    "            dfx['Notional'] *= notional_to_use\n",
    "            dfx['Signal'] *= -1    \n",
    "        \n",
    "            dfy = apply_funding(dfy.copy(), model_Y)\n",
    "            dfx = apply_funding(dfx.copy(), model_X[0])\n",
    "        \n",
    "        if len(model_X) == 2:\n",
    "            \n",
    "            dfy = bt_df[[model_Y,f'{model_Y} Dur','Avg. ZScore','Notional','volume']].dropna().copy()\n",
    "            dfy.columns = dfy.columns.str.replace(\"Avg. ZScore\",\"Signal\").str.replace(f'{model_Y} Dur','Dur').str.replace(model_Y,\"close\")\n",
    "            dfy['Notional'] = notional_to_use\n",
    "            \n",
    "            dfx = bt_df[[model_X[0],f'{model_X[0]} Dur','Avg. ZScore','Notional','volume']].dropna().copy()\n",
    "            dfx.columns = dfx.columns.str.replace(\"Avg. ZScore\",\"Signal\").str.replace(f'{model_X[0]} Dur','Dur').str.replace(model_X[0],\"close\")\n",
    "            dfx['Notional'] = notional_to_use * bt_df['Coef1']\n",
    "            dfx['Signal'] *= -1   ############################ in basis only hyg px we sell when we buy the residuals so only this will be inverted\n",
    "            \n",
    "            dfx1 = bt_df[[model_X[1],f'{model_X[1]} Dur','Avg. ZScore','Notional','volume']].dropna().copy()\n",
    "            dfx1.columns = dfx1.columns.str.replace(\"Avg. ZScore\",\"Signal\").str.replace(f'{model_X[1]} Dur','Dur').str.replace(model_X[1],\"close\")\n",
    "            dfx1['Notional'] = notional_to_use * bt_df['Coef2']\n",
    "        \n",
    "            dfy = apply_funding(dfy.copy(), model_Y)\n",
    "            dfx = apply_funding(dfx.copy(), model_X[0])\n",
    "            dfx1 = apply_funding(dfx1.copy(), model_X[1])\n",
    "        \n",
    "        \n",
    "        class MyData(bt.feeds.PandasData):\n",
    "            lines = ('signal', 'notional', 'dur', 'long_funding', 'short_funding')\n",
    "            params = (('signal', 'Signal'), ('notional', 'Notional'), \n",
    "                      ('dur', 'Dur'), ('long_funding', 'Long Funding P/L'), ('short_funding', 'Short Funding P/L'))\n",
    "        \n",
    "        class FixedComm(bt.CommInfoBase):\n",
    "            params = (('commission', 0), ('stocklike', True), ('commtype', bt.CommInfoBase.COMM_FIXED),)\n",
    "            def _getcommission(self, size, price, pseudoexec):\n",
    "                return self.p.commission\n",
    "        \n",
    "        class TStrategy(bt.Strategy):\n",
    "            params = dict(cheat_on_close=True, entry_zscore = 0, exit_zscore = 0, carry = 0, product = 'Untitled')\n",
    "            \n",
    "            def __init__(self):\n",
    "                self.portfolio_values = []\n",
    "                self.entry_date = None\n",
    "                self.daily_pnl = []\n",
    "                self.roll_trades = []\n",
    "                self.carry = []\n",
    "                self.roll_carry = []\n",
    "                \n",
    "                self.funding = []\n",
    "                self.funding_carry = []\n",
    "                self.funding_direction = None\n",
    "                self.entry_bar_funding = None\n",
    "                \n",
    "                self.trade_direction = []\n",
    "                \n",
    "                self.scatter_plot_trade_pnl = []\n",
    "                self.scatter_plot_trade_init_len = None\n",
    "                \n",
    "                self.carry_direction = 0\n",
    "                self.logs = []\n",
    "        \n",
    "            def log(self, txt):\n",
    "                dt = self.datas[0].datetime.datetime(0)\n",
    "                print(f'{dt} - {txt}')\n",
    "                self.logs.append([dt.isoformat(), txt])\n",
    "        \n",
    "            def notify_trade(self, trade):            \n",
    "                if trade.isclosed:            \n",
    "                    current_date = self.datas[0].datetime.datetime(-1)\n",
    "                    \n",
    "                    self.trade_direction.append((current_date, self.funding_direction if self.p.product == model_Y else None)) \n",
    "                    #### Y determines the long/short direction\n",
    "                    \n",
    "                    \n",
    "                    ######################################## Calculating carry for all trades\n",
    "                    self.daily_pnl.append((current_date, trade.pnlcomm))\n",
    "        \n",
    "                    if self.carry_direction > 0 : \n",
    "                        self.carry.append((current_date, (current_date.date()-self.entry_date.\\\n",
    "                                                          date()).days*(1/360)*self.p.carry*(1/100)*self.datas[0].notional[0]))\n",
    "                    elif self.carry_direction < 0 :\n",
    "                        self.carry.append((current_date, -1*(current_date.date()-self.entry_date.\\\n",
    "                                                         date()).days*(1/360)*self.p.carry*(1/100)*self.datas[0].notional[0]))\n",
    "                    \n",
    "                    ######################################## Calculating funding for all trades\n",
    "                    bars_active = len(self) - self.entry_bar_funding - 1\n",
    "                    \n",
    "                    if self.funding_direction == \"Long\":\n",
    "                        values = [self.datas[0].long_funding[-i] for i in range(bars_active)]\n",
    "                        rolling_sum = -1*sum(values)  ############ We pay the long funding\n",
    "                    \n",
    "                    elif self.funding_direction == \"Short\":\n",
    "                        values = [self.datas[0].short_funding[-i] for i in range(bars_active)]\n",
    "                        rolling_sum = sum(values)  ############ We earn the short funding\n",
    "                        \n",
    "                    self.funding.append((current_date, rolling_sum))\n",
    "        \n",
    "                    \n",
    "                    p1 = [datetime(current_date.year, 3, 20), datetime(current_date.year, 3, 30)]\n",
    "                    p2 = [datetime(current_date.year, 9, 20), datetime(current_date.year, 9, 30)]\n",
    "                    p3 = [datetime(current_date.year+1, 3, 20), datetime(current_date.year+1, 3, 30)]\n",
    "                    p4 = [datetime(current_date.year-1, 9, 20), datetime(current_date.year-1, 9, 30)]\n",
    "                    periods = [p1, p2, p3, p4]\n",
    "                    \n",
    "                    ######################################## Calculating carry for roll trades separately\n",
    "                    \n",
    "                    for i in range(len(periods)):\n",
    "                        p = periods[i]\n",
    "                        if ((self.entry_date <= p[1]) and (p[0] <= current_date)):\n",
    "                            self.roll_trades.append((current_date, trade.pnlcomm))\n",
    "                            if self.carry_direction > 0:\n",
    "                                self.roll_carry.append((current_date, (current_date.date()-self.entry_date.\\\n",
    "                                                               date()).days*(1/360)*self.p.carry*(1/100)*self.datas[0].notional[0]))\n",
    "                            elif self.carry_direction < 0:\n",
    "                                self.roll_carry.append((current_date, -1*(current_date.date()-self.entry_date.\\\n",
    "                                                              date()).days*(1/360)*self.p.carry*(1/100)*self.datas[0].notional[0]))\n",
    "        \n",
    "                            self.funding_carry.append((current_date, rolling_sum))\n",
    "                            \n",
    "                            break\n",
    "                    \n",
    "                    ################################################# All trades\n",
    "                    \n",
    "                    # self.log(f'Gross P/L: {trade.pnl:.2f}, Net P/L: {trade.pnlcomm:.2f}, Funding P/L: {rolling_sum:.2f}')        \n",
    "                    self.carry_direction = 0\n",
    "        \n",
    "            # def notify_order(self, order):\n",
    "            #     if order.status in [order.Completed]:\n",
    "            #         if order.isbuy():\n",
    "            #             self.log(f'BUY EXECUTED, PX: {order.executed.price:.4f}, Qty: {order.executed.size:.2f}, Comm: {order.executed.comm}')\n",
    "            #         else:\n",
    "            #             self.log(f'SELL EXECUTED, PX: {order.executed.price:.4f}, Qty: {order.executed.size:.2f}, Comm: {order.executed.comm}')\n",
    "        \n",
    "            def next(self):\n",
    "                self.broker.set_coc(self.p.cheat_on_close)\n",
    "                # val = None\n",
    "                self.portfolio_values.append((self.datas[0].datetime.datetime(0), self.broker.getvalue()))\n",
    "                \n",
    "                z = self.datas[0].signal[0]\n",
    "                px = self.datas[0].close[0]        \n",
    "                pos = self.getposition(self.datas[0]).size\n",
    "        \n",
    "                # self.log(f'PX: {px}, ZScore: {z:.3f}, Notional: {self.datas[0].notional[0]:.0f}, Dur: {self.datas[0].dur[0]:.4f} '\n",
    "                #          f'Pos: {pos:.2f}, Long Funding $: {self.datas[0].long_funding[0]:.2f}, Short Funding $: {self.datas[0].short_funding[0]:.2f} '\n",
    "                #          f'Eqty (000): {(self.broker.getvalue()-1e12)*10**(-3):.2f}')\n",
    "                \n",
    "                if pos == 0:                \n",
    "                    if z < -self.p.entry_zscore:\n",
    "                        self.scatter_plot_trade_init_len = len(self)\n",
    "                        self.entry_date = self.datas[0].datetime.datetime(0)\n",
    "        \n",
    "                        if self.datas[0].dur[0] == -1:\n",
    "                            self.buy(data = self.data, size = self.datas[0].notional[0]/px)\n",
    "                        elif self.datas[0].dur[0] == 0.0:\n",
    "                            self.buy(data=self.datas[0], size = (self.datas[0].notional[0] / 100))\n",
    "                        elif self.datas[0].dur[0] > 0.0:\n",
    "                            self.sell(data=self.data, size = (self.datas[0].notional[0])*10**(-4)*self.datas[0].dur[0])\n",
    "        \n",
    "                        self.carry_direction = 1\n",
    "                        self.entry_bar_funding = len(self)\n",
    "                        self.funding_direction = \"Long\"\n",
    "        \n",
    "                    elif z > self.p.entry_zscore:\n",
    "                        self.scatter_plot_trade_init_len = len(self)\n",
    "                        self.entry_date = self.datas[0].datetime.datetime(0)\n",
    "        \n",
    "                        if self.datas[0].dur[0] == -1:\n",
    "                            self.sell(data = self.data, size = self.datas[0].notional[0]/px)\n",
    "                        elif self.datas[0].dur[0] == 0.0:\n",
    "                            self.sell(data=self.datas[0], size = (self.datas[0].notional[0] / 100))\n",
    "                        elif self.datas[0].dur[0] > 0.0:\n",
    "                            self.buy(data=self.data, size = (self.datas[0].notional[0])*10**(-4)*self.datas[0].dur[0])\n",
    "        \n",
    "                        self.carry_direction = -1\n",
    "                        self.entry_bar_funding = len(self)\n",
    "                        self.funding_direction = \"Short\"\n",
    "                        \n",
    "                else:\n",
    "                        if self.datas[0].dur[0] <= 0.0: ######## Equity & CDX HY\n",
    "                            if ((pos > 0 and z > -self.p.exit_zscore) or (pos < 0 and z < self.p.exit_zscore)):\n",
    "                                self.close(data=self.datas[0])\n",
    "                                self.scatter_plot_trade_pnl.append((self.datas[0].datetime.\\\n",
    "                                                    datetime(0), len(self) - self.scatter_plot_trade_init_len))\n",
    "                        \n",
    "                        elif self.datas[0].dur[0] > 0.0: ######## CDX IG\n",
    "                            if ((pos < 0 and z > -self.p.exit_zscore) or (pos > 0 and z < self.p.exit_zscore)):\n",
    "                                self.close(data=self.datas[0])\n",
    "                                self.scatter_plot_trade_pnl.append((self.datas[0].datetime.\\\n",
    "                                                    datetime(0), len(self) - self.scatter_plot_trade_init_len))\n",
    "        \n",
    "        \n",
    "            # def stop(self):\n",
    "            #     # with open(f'{self.p.product}.csv', 'w', newline='') as f:\n",
    "            #     with open(f'backtrader_log_{str(datetime.now().time()).replace(\":\",\"_\")}.csv', 'w', newline='') as f:\n",
    "            #         writer = csv.writer(f)\n",
    "            #         writer.writerow(['Date','Message'])\n",
    "            #         writer.writerows(self.logs)\n",
    "        \n",
    "        for strategy_zscore_exit in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "            for strategy_zscore_entry in [0, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5]:#, 1.75, 2.0, 2.5, 3.0]:\n",
    "                if strategy_zscore_entry > strategy_zscore_exit:\n",
    "        \n",
    "                    cerebro1 = bt.Cerebro()\n",
    "                    cerebro1.addstrategy(TStrategy, cheat_on_close=True, entry_zscore = strategy_zscore_entry, exit_zscore = strategy_zscore_exit,\\\n",
    "                                         carry = dict_map[model_Y][3], product = model_Y)\n",
    "                    \n",
    "                    cerebro1.broker.setcash(1e12)\n",
    "                    feed1 = MyData(dataname=dfy.copy(), timeframe=bt.TimeFrame.Minutes)\n",
    "                    cerebro1.adddata(feed1)\n",
    "                    cerebro1.broker.set_slippage_fixed(fixed=dict_map[model_Y][5], slip_open=True, slip_limit=True, slip_match=True, slip_out=True)\n",
    "                    cerebro1.broker.setcommission(margin=0.00001, mult=1)\n",
    "                    cerebro1.broker.addcommissioninfo(FixedComm(commission=dict_map[model_Y][6]))\n",
    "                    cerebro1.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trade_analyzer')\n",
    "                    results1 = cerebro1.run()\n",
    "                    \n",
    "                    cerebro2 = bt.Cerebro()\n",
    "                    cerebro2.addstrategy(TStrategy, cheat_on_close=True, entry_zscore = strategy_zscore_entry, exit_zscore = strategy_zscore_exit,\\\n",
    "                                         carry = dict_map[model_X[0]][3], product = model_X[0])\n",
    "                    cerebro2.broker.setcash(1e12)\n",
    "                    feed2 = MyData(dataname=dfx.copy(), timeframe=bt.TimeFrame.Minutes)\n",
    "                    cerebro2.adddata(feed2)\n",
    "                    cerebro2.broker.set_slippage_fixed(fixed=dict_map[model_X[0]][5], slip_open=True, slip_limit=True, slip_match=True, slip_out=True)\n",
    "                    cerebro2.broker.setcommission(margin=0.00001, mult=1)\n",
    "                    cerebro1.broker.addcommissioninfo(FixedComm(commission=dict_map[model_X[0]][6]))\n",
    "                    cerebro2.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trade_analyzer')\n",
    "                    results2 = cerebro2.run()\n",
    "        \n",
    "                    if len(model_X) == 2:\n",
    "                        cerebro3 = bt.Cerebro()\n",
    "                        cerebro3.addstrategy(TStrategy, cheat_on_close=True, entry_zscore = strategy_zscore_entry, \\\n",
    "                                             exit_zscore = strategy_zscore_exit,\\\n",
    "                                             carry = dict_map[model_X[1]][3], product = model_X[1])\n",
    "                        cerebro3.broker.setcash(1e12)\n",
    "                        feed3 = MyData(dataname=dfx1.copy(), timeframe=bt.TimeFrame.Minutes)\n",
    "                        cerebro3.adddata(feed3)\n",
    "                        cerebro3.broker.set_slippage_fixed(fixed=dict_map[model_X[1]][5], slip_open=True, slip_limit=True, \\\n",
    "                                                           slip_match=True, slip_out=True)\n",
    "                        cerebro3.broker.setcommission(margin=0.00001, mult=1)\n",
    "                        cerebro1.broker.addcommissioninfo(FixedComm(commission=dict_map[model_X[1]][6]))\n",
    "                        cerebro3.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trade_analyzer')\n",
    "                        results3 = cerebro3.run()\n",
    "        \n",
    "                    \n",
    "                    strat1 = results1[0]\n",
    "                    dates1, values1 = zip(*strat1.portfolio_values)\n",
    "                    \n",
    "                    strat2 = results2[0]\n",
    "                    dates2, values2 = zip(*strat2.portfolio_values)\n",
    "        \n",
    "                    if len(model_X) == 2:\n",
    "                        strat3 = results3[0]\n",
    "                        dates3, values3 = zip(*strat3.portfolio_values)\n",
    "                    \n",
    "                    # clear_output(wait=False)\n",
    "                    \n",
    "                    \n",
    "                    #############################################################################################################################\n",
    "                    \n",
    "                    ############################################### Basic PX based P/L\n",
    "                    try:\n",
    "                        dates3, values3 = zip(*strat1.daily_pnl)\n",
    "                        d3 = pd.DataFrame({ 'CDX':list(values3)}, index = list(dates3))\n",
    "                        dates3, values3 = zip(*strat2.daily_pnl)\n",
    "                        d4 = pd.DataFrame({ 'SPX':list(values3)}, index = list(dates3))\n",
    "            \n",
    "                        if len(model_X) == 1:\n",
    "                            d5 = pd.concat([d3,d4],axis=1).dropna().copy()\n",
    "                            d5['Daily P/L'] = d5['CDX'] + d5['SPX']\n",
    "                        \n",
    "                        elif len(model_X) == 2:\n",
    "                            dates3, values3 = zip(*strat3.daily_pnl)\n",
    "                            d41 = pd.DataFrame({ 'SPX2':list(values3)}, index = list(dates3))\n",
    "                            d5 = pd.concat([d3,d4,d41],axis=1).dropna().copy()\n",
    "                            d5['Daily P/L'] = d5['CDX'] + d5['SPX'] + d5['SPX2']\n",
    "            \n",
    "                        d5 = round(d5[['Daily P/L']].astype(float),2)\n",
    "                        daily_pnl = d5.copy()\n",
    "            \n",
    "                        ############################################### Removing Roll basic PX P/L\n",
    "                        try:\n",
    "                            dates3, values3 = zip(*strat1.roll_trades)\n",
    "                            d3 = pd.DataFrame({ 'CDX':list(values3)}, index = list(dates3))\n",
    "                            dates3, values3 = zip(*strat2.roll_trades)\n",
    "                            d4 = pd.DataFrame({ 'SPX':list(values3)}, index = list(dates3))\n",
    "            \n",
    "                            if len(model_X) == 1:\n",
    "                                d5 = pd.concat([d3,d4],axis=1).dropna().copy()\n",
    "                                d5['Roll P/L'] = d5['CDX'] + d5['SPX']\n",
    "                            \n",
    "                            elif len(model_X) == 2:\n",
    "                                dates3, values3 = zip(*strat3.roll_trades)\n",
    "                                d41 = pd.DataFrame({ 'SPX2':list(values3)}, index = list(dates3))\n",
    "                                d5 = pd.concat([d3,d4,d41],axis=1).dropna().copy()\n",
    "                                d5['Roll P/L'] = d5['CDX'] + d5['SPX'] + d5['SPX2']\n",
    "            \n",
    "                            d5 = -1*round(d5[['Roll P/L']].astype(float),2)\n",
    "                            roll_trades = d5.copy()\n",
    "                        except Exception as e:\n",
    "                            roll_trades = pd.DataFrame()\n",
    "            \n",
    "                        ############################################### Adding all trades carry\n",
    "                        try:\n",
    "                            dates3, values3 = zip(*strat1.carry)\n",
    "                            d3 = pd.DataFrame({ 'CDX':list(values3)}, index = list(dates3))\n",
    "                            dates3, values3 = zip(*strat2.carry)\n",
    "                            d4 = pd.DataFrame({ 'SPX':list(values3)}, index = list(dates3))\n",
    "            \n",
    "                            if len(model_X) == 1:\n",
    "                                d5 = pd.concat([d3,d4],axis=1).dropna().copy()\n",
    "                                d5['CDX Carry'] = d5['CDX'] + d5['SPX']\n",
    "                                \n",
    "                            elif len(model_X) == 2:\n",
    "                                dates3, values3 = zip(*strat3.carry)\n",
    "                                d41 = pd.DataFrame({ 'SPX2':list(values3)}, index = list(dates3))\n",
    "                                d5 = pd.concat([d3,d4,d41],axis=1).dropna().copy()\n",
    "                                d5['CDX Carry'] = d5['CDX'] + d5['SPX'] + d5['SPX2']\n",
    "            \n",
    "                            d5 = round(d5[['CDX Carry']].astype(float),2)\n",
    "                            carry = d5.copy()\n",
    "                        except Exception as e:\n",
    "                            carry = pd.DataFrame()\n",
    "                        \n",
    "                        ############################################### Remove carry of roll trades\n",
    "                        \n",
    "                        try:\n",
    "                            dates3, values3 = zip(*strat1.roll_carry)\n",
    "                            d3 = pd.DataFrame({ 'CDX':list(values3)}, index = list(dates3))\n",
    "                            dates3, values3 = zip(*strat2.roll_carry)\n",
    "                            d4 = pd.DataFrame({ 'SPX':list(values3)}, index = list(dates3))\n",
    "            \n",
    "                            if len(model_X) == 1:\n",
    "                                d5 = pd.concat([d3,d4],axis=1).dropna().copy()\n",
    "                                d5['CDX Roll Carry'] = d5['CDX'] + d5['SPX']\n",
    "            \n",
    "                            elif len(model_X) == 2:\n",
    "                                dates3, values3 = zip(*strat3.roll_carry)\n",
    "                                d41 = pd.DataFrame({ 'SPX2':list(values3)}, index = list(dates3))\n",
    "                                d5 = pd.concat([d3,d4,d41],axis=1).dropna().copy()\n",
    "                                d5['CDX Roll Carry'] = d5['CDX'] + d5['SPX'] + d5['SPX2']\n",
    "            \n",
    "                            d5 = -1*round(d5[['CDX Roll Carry']].astype(float),2)\n",
    "                            roll_carry = d5.copy()\n",
    "                        except Exception as e:\n",
    "                            roll_carry = pd.DataFrame()\n",
    "            \n",
    "                        ############################################### Add funding of all trades\n",
    "                        \n",
    "                        try:\n",
    "                            dates3, values3 = zip(*strat1.funding)\n",
    "                            d3 = pd.DataFrame({ 'CDX':list(values3)}, index = list(dates3))\n",
    "                            dates3, values3 = zip(*strat2.funding)\n",
    "                            d4 = pd.DataFrame({ 'SPX':list(values3)}, index = list(dates3))\n",
    "            \n",
    "                            if len(model_X) == 1:\n",
    "                                d5 = pd.concat([d3,d4],axis=1).dropna().copy()\n",
    "                                d5['Funding'] = d5['CDX'] + d5['SPX']\n",
    "            \n",
    "                            elif len(model_X) == 2:\n",
    "                                dates3, values3 = zip(*strat3.funding)\n",
    "                                d41 = pd.DataFrame({ 'SPX2':list(values3)}, index = list(dates3))\n",
    "                                d5 = pd.concat([d3,d4,d41],axis=1).dropna().copy()\n",
    "                                d5['Funding'] = d5['CDX'] + d5['SPX'] + d5['SPX2']\n",
    "            \n",
    "                            d5 = round(d5[['Funding']].astype(float),2)\n",
    "                            funding = d5.copy()\n",
    "                        except Exception as e:\n",
    "                            funding = pd.DataFrame()\n",
    "                            \n",
    "                        ############################################### Remove funding of carry trades\n",
    "                        \n",
    "                        try:\n",
    "                            dates3, values3 = zip(*strat1.funding_carry)\n",
    "                            d3 = pd.DataFrame({ 'CDX':list(values3)}, index = list(dates3))\n",
    "                            dates3, values3 = zip(*strat2.funding_carry)\n",
    "                            d4 = pd.DataFrame({ 'SPX':list(values3)}, index = list(dates3))\n",
    "            \n",
    "                            if len(model_X) == 1:\n",
    "                                d5 = pd.concat([d3,d4],axis=1).dropna().copy()\n",
    "                                d5['funding_carry'] = d5['CDX'] + d5['SPX']\n",
    "            \n",
    "                            elif len(model_X) == 2:\n",
    "                                dates3, values3 = zip(*strat3.funding_carry)\n",
    "                                d41 = pd.DataFrame({ 'SPX2':list(values3)}, index = list(dates3))\n",
    "                                d5 = pd.concat([d3,d4,d41],axis=1).dropna().copy()\n",
    "                                d5['funding_carry'] = d5['CDX'] + d5['SPX'] + d5['SPX2']\n",
    "            \n",
    "                            d5 = -1*round(d5[['funding_carry']].astype(float),2)\n",
    "                            funding_carry = d5.copy()\n",
    "                        except Exception as e:\n",
    "                            funding_carry = pd.DataFrame()\n",
    "            \n",
    "                        #############################################################################################################################\n",
    "            \n",
    "                        dates3, values3 = zip(*strat1.trade_direction)\n",
    "                        long_short_ind = pd.DataFrame({ 'Ind':list(values3)}, index = list(dates3))\n",
    "                        \n",
    "                        bt_df = pd.concat([dfy[['volume']], daily_pnl, roll_trades, carry, roll_carry, funding, funding_carry],axis=1)\n",
    "                        bt_df = bt_df.iloc[:,1:]\n",
    "                        bt_df = bt_df.fillna(0.0)\n",
    "\n",
    "\n",
    "                        ####### Keep commented to adjust for funding; if no funding then remove the hash    \n",
    "                        ####### these are when want to tally with old model outputs                        \n",
    "                        # bt_df['Funding'] = [0.0] * len(bt_df)\n",
    "                        # bt_df['funding_carry'] = [0.0] * len(bt_df)\n",
    "                        \n",
    "                        bt_df_backup = bt_df.copy()\n",
    "                        \n",
    "                        for trade_btdf_direction in ['Long/Short']: #'Long','Short',\n",
    "                            bt_df = bt_df_backup.copy()\n",
    "                            \n",
    "                            trade_check = None if trade_btdf_direction == 'Long/Short' else trade_btdf_direction\n",
    "                            \n",
    "                            bt_df['Sum'] = bt_df.sum(axis=1)\n",
    "                            x = pd.concat([bt_df, long_short_ind],axis=1).copy()\n",
    "                            x = x[x['Ind']!=trade_check].drop(\"Ind\",axis=1).copy()   ###### Use not equal operator\n",
    "                            x = pd.concat([dfy[['volume']],x],axis=1).copy()\n",
    "                            x = x.iloc[:,1:].fillna(0.0)\n",
    "                            bt_df = x.copy()\n",
    "                            trade_num = len(bt_df[bt_df['Sum']!=0])\n",
    "                            bt_df = bt_df.drop(\"Sum\",axis=1)\n",
    "            \n",
    "            \n",
    "                            ###########################################################################################################################\n",
    "            \n",
    "                            sr = bt_df.copy()\n",
    "                            sr['Sum'] = sr.sum(axis=1)\n",
    "                            sr = sr[['Sum']]\n",
    "                            sr = sr.cumsum().resample(\"D\").last().dropna().copy()\n",
    "                            sr += 10**7\n",
    "                            sr = sr.pct_change()\n",
    "                            sr = round((252**0.5*sr.mean()/sr.std()).iloc[0],3)\n",
    "                            \n",
    "                            globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_$pnl'].\\\n",
    "                            loc[strategy_zscore_entry,strategy_zscore_exit] = round(bt_df.sum().sum(),0)\n",
    "                            globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_$pnl/trade'].\\\n",
    "                            loc[strategy_zscore_entry,strategy_zscore_exit] = round(bt_df.sum().sum()/trade_num,0)\n",
    "                            globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_SR'].\\\n",
    "                            loc[strategy_zscore_entry,strategy_zscore_exit] = round(sr,2)\n",
    "                            globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_trades'].\\\n",
    "                            loc[strategy_zscore_entry,strategy_zscore_exit] = trade_num\n",
    "            \n",
    "                            bt_df['Sum'] = bt_df.sum(axis=1)\n",
    "                            pos = len(bt_df[bt_df['Sum']>0])\n",
    "                            neg = len(bt_df[bt_df['Sum']<0])\n",
    "                            try:\n",
    "                                hit = round((pos/(pos+neg))*100,0)\n",
    "                            except:\n",
    "                                hit = 0\n",
    "                            globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_Hit Ratio'].\\\n",
    "                            loc[strategy_zscore_entry,strategy_zscore_exit] = round(hit,0)\n",
    "                \n",
    "                            max_dd = bt_df[['Sum']].cumsum().copy()\n",
    "                            max_dd['Roll Max'] = max_dd[['Sum']].rolling(window=10000000, min_periods=1).max()\n",
    "                            max_dd['Diff'] = abs(max_dd['Roll Max'] - max_dd['Sum'])\n",
    "                            globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_max DD'].\\\n",
    "                            loc[strategy_zscore_entry,strategy_zscore_exit] = round(-1*max_dd['Diff'].max(),0)\n",
    "                            \n",
    "                            ############################### Plots\n",
    "                            \n",
    "                            dates3, values3 = zip(*strat1.scatter_plot_trade_pnl)\n",
    "                            d3 = pd.DataFrame({ 'CDX':list(values3)}, index = list(dates3))\n",
    "                            dates3, values3 = zip(*strat2.scatter_plot_trade_pnl)\n",
    "                            d4 = pd.DataFrame({ 'SPX':list(values3)}, index = list(dates3))\n",
    "                \n",
    "                            if len(model_X) == 1:\n",
    "                                d5 = pd.concat([d3,d4],axis=1).dropna().copy()\n",
    "                \n",
    "                            elif len(model_X) == 2:\n",
    "                                dates3, values3 = zip(*strat3.scatter_plot_trade_pnl)\n",
    "                                d41 = pd.DataFrame({ 'SPX2':list(values3)}, index = list(dates3))\n",
    "                                d5 = pd.concat([d3,d4,d41],axis=1).dropna().copy()\n",
    "                            \n",
    "                            scatter = pd.concat([bt_df[['Sum']],d5['CDX']],axis=1)\n",
    "                            # scatter = pd.concat([bt_df[['Sum']],d3['CDX']],axis=1)\n",
    "                            x = scatter[scatter['Sum']!=0.0]['CDX'].copy()\n",
    "                            bar_size = sampling_multiplier if dict_models[model_num][0] == 'Intraday' else np.nan\n",
    "                            \n",
    "                            globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_days/trade'].\\\n",
    "                            loc[strategy_zscore_entry,strategy_zscore_exit] = round(x.sum()/(bar_size*len(x)),1)\n",
    "                            \n",
    "                            title = f\"{global_model}; Model {dict_models[model_num][-1]}; {strategy_zscore_entry} entry; {trade_btdf_direction} direction\"\n",
    "                            title += f\" & {strategy_zscore_exit} exit; P/L: {bt_df[['Sum']].cumsum().iloc[-1,0]:.0f}; SR: {sr:.2f}\"\n",
    "                            title += f\" & Diff: {str(diff_period_list)}\"\n",
    "                            \n",
    "                            dates3, values3 = zip(*strat1.scatter_plot_trade_pnl)\n",
    "                            d3 = pd.DataFrame({ 'CDX':list(values3)}, index = list(dates3))\n",
    "                            dates3, values3 = zip(*strat2.scatter_plot_trade_pnl)\n",
    "                            d4 = pd.DataFrame({ 'SPX':list(values3)}, index = list(dates3))\n",
    "                            \n",
    "                            if (model_X) == 1:\n",
    "                                d5 = pd.concat([d3,d4],axis=1).dropna().copy()\n",
    "                \n",
    "                            elif len(model_X) == 2:\n",
    "                                dates3, values3 = zip(*strat3.scatter_plot_trade_pnl)\n",
    "                                d41 = pd.DataFrame({ 'SPX2':list(values3)}, index = list(dates3))\n",
    "                                d5 = pd.concat([d3,d4,d41],axis=1).dropna().copy()\n",
    "                            \n",
    "                            d5 = d5/bar_size\n",
    "                            scatter = pd.concat([bt_df[['Sum']],d5['CDX']],axis=1)\n",
    "                            scatter = scatter[scatter['Sum']!=0.0]\n",
    "                            plt.figure(figsize=(12,6))\n",
    "                            plt.scatter(scatter['CDX'], scatter['Sum'],label=\"Per Trade P/L\")\n",
    "                            plt.ylabel(\"Trade $P/L\")\n",
    "                            plt.xlabel(\"Trade Duration in Days\")\n",
    "                            plt.title(title)\n",
    "                            plt.legend()\n",
    "                            plt.savefig(f\"Plots/Scatter/{title.replace(\";\",\"_\").replace(\"/\",\"_\").replace(\"&\",\"_\").replace(\":\",\"_\")}.png\")\n",
    "                            # plt.show()\n",
    "                            plt.close()\n",
    "                             \n",
    "                            bt_df['Sum'].cumsum().plot(label=\"Cum. P/L\", figsize=(12,6))\n",
    "                            plt.title(title)\n",
    "                            plt.legend()\n",
    "                            plt.savefig(f\"Plots/PL/{title.replace(\";\",\"_\").replace(\"/\",\"_\").replace(\"&\",\"_\").replace(\":\",\"_\")}.png\")\n",
    "                            # plt.show()\n",
    "                            plt.close()\n",
    "                            ############################### Plots\n",
    "                    except:\n",
    "                        continue\n",
    "        \n",
    "        display(dfx.iloc[[0,-1],:])\n",
    "        \n",
    "        for trade_btdf_direction in ['Long/Short']: #'Long','Short',\n",
    "            print(f'global model is {global_model}')\n",
    "            print(f'model_num is {model_num}')\n",
    "            print(f'trade direction is {trade_btdf_direction}')\n",
    "            for info in ['$pnl','$pnl/trade','SR','Hit Ratio','trades','days/trade','max DD']:\n",
    "                display(globals()[f'{global_model}_{model_num}_{trade_btdf_direction}_{info}'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
